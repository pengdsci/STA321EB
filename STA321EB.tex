% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={STA 321 E-pack: Advanced Statistics},
  pdfauthor={Cheng Peng},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{STA 321 E-pack: Advanced Statistics}
\author{Cheng Peng}
\date{West Chester University}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{)}
\CommentTok{\# or the development version}
\CommentTok{\# devtools::install\_github("rstudio/bookdown")}
\end{Highlighting}
\end{Shaded}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

This \emph{E-coursepack} (a.k.a. \textbf{E-Pack}) is a self-contained homegrown Ebook that contains all topics covered in current STA321 at WCU.

\hypertarget{why-this-e-pack}{%
\section{Why This E-Pack?}\label{why-this-e-pack}}

Since this is an advanced-topic course that covers three major topics in linear regression modeling, generalized linear regression modeling, and time series modeling. These topics are typically covered in three different textbook. This E-pack contains all topics and are delivered using parametric and non-parametric methods.

\hypertarget{components-statistical-reports}{%
\section{Components Statistical Reports}\label{components-statistical-reports}}

Since is a project-based modeling class. The assignments are building-blocks of about 3 projects that cover linear regression, generalized linear regression and time series. Every will use data sets that are real-world or close to the real-world data for all projects. All statistical reports must have the following key components.

\hypertarget{a.-introduction}{%
\subsection*{A. Introduction}\label{a.-introduction}}
\addcontentsline{toc}{subsection}{A. Introduction}

Provide some background on the problem. This includes the motivations and objectives of the analysis.

\hypertarget{b.-materials}{%
\subsection*{B. Materials}\label{b.-materials}}
\addcontentsline{toc}{subsection}{B. Materials}

Some information about the data should be described here. For example, methods of data collection, variable names, and definitions, potential data challenges, etc. You could use subsections to organize your work.

\hypertarget{c.-methodology}{%
\subsection*{C. Methodology}\label{c.-methodology}}
\addcontentsline{toc}{subsection}{C. Methodology}

Describe all the methods (including justifications for using the methods) you used to gather and analyze the data here. You need to provide extensive details so that anyone can replicate your results.

\hypertarget{d.-results-and-conclusions}{%
\subsection*{D. Results and Conclusions}\label{d.-results-and-conclusions}}
\addcontentsline{toc}{subsection}{D. Results and Conclusions}

Show your audience all your results and conclusions with justifications. Write this section in a way that enables a non-statistician to understand the content. Be very specific.

\hypertarget{e.-general-discussion}{%
\subsection*{E. General Discussion}\label{e.-general-discussion}}
\addcontentsline{toc}{subsection}{E. General Discussion}

Talk about results (with justifications) and link them to real-world implications. Pay attention to whether the research questions were well addressed.

\hypertarget{f.-references-if-any}{%
\subsection*{F. References (if any)}\label{f.-references-if-any}}
\addcontentsline{toc}{subsection}{F. References (if any)}

Everything you used in the analysis including notes and blogs from the internet, textbook, journal articles, etc.

\hypertarget{g.-appendices-if-any}{%
\subsection*{G. Appendices (if any)}\label{g.-appendices-if-any}}
\addcontentsline{toc}{subsection}{G. Appendices (if any)}

Additional output tables and graphics that important but not fundamental to the report should be placed in this section.

\hypertarget{basics-of-rstudio}{%
\section{Basics of RStudio}\label{basics-of-rstudio}}

This brief note will introduce the basics of Rstudio, R Markdown, and R.

\begin{itemize}
\item
  \textbf{RStudio} is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging, and workspace management.
\item
  \textbf{R Markdown} is a file format for making dynamic documents with R. An R Markdown document is written in markdown (an easy-to-write plain text format) and contains chunks of embedded R code and the output generated from the R code. This note is written in R Markdown. This is also a tutorial showing how to use R Markdown to write an R Markdown report. -- RStudio documentation.
\item
  \textbf{R} is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (formerly AT\&T, now Lucent Technologies) by John Chambers and colleagues. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.
\end{itemize}

\hypertarget{rstudio-gui}{%
\subsection{RStudio GUI}\label{rstudio-gui}}

The RStudio interface consists of several windows. I insert an image of a regular RStudio GUI.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img00/01-rstudio} 

}

\caption{The GUI of RStudio}\label{fig:unnamed-chunk-3}
\end{figure}

\hypertarget{console}{%
\subsubsection{Console}\label{console}}

We can type commands directly into the console, or write in a text file, and then send the command to the console. It is convenient to use the console if your task involves one line of code. Otherwise, we should always use an editor to write code and then run the code in the Console.

\hypertarget{source-editor}{%
\subsubsection{Source Editor}\label{source-editor}}

Generally, we will want to write programs longer than a few lines. The Source Editor can help you open, edit and execute these programs.

\hypertarget{environment-window}{%
\subsubsection{Environment Window}\label{environment-window}}

The Environment window shows the objects (i.e., data frames, arrays, values, and functions) in the environment (workspace). We can see the descriptive information such as types as the dimension of the objects in your environment. We also choose a data source from the environment to view in the source window like a spreadsheet.

\hypertarget{system-and-graphic-files}{%
\subsubsection{System and Graphic files}\label{system-and-graphic-files}}

The Files tab has a navigable file manager, just like the file system on your operating system. The Plot tab is where the graphics you create will appear. The Packages tab shows you the packages that are installed and those that can be installed (more on this just now). The Help tab allows you to search the R documentation for help and is where the help appears when you ask for it from the Console.

\hypertarget{rmarkdown}{%
\subsection{RMarkdown}\label{rmarkdown}}

An R Markdown document is a text-based file format that allows you to include both descriptive text, code blocks, and code output. It can be converted to other types of files such as PDF, HTML, and WORD that can include code, plots, outputs generated from the code chunks.

\hypertarget{code-chunk}{%
\subsubsection{Code Chunk}\label{code-chunk}}

In R Markdown, we can embed R code in the code chunk defined by the symbol \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{\}} and closed by \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}}. The symbol \texttt{} `, also called \textbf{backquote} or \textbf{backtick}, can be found on the top left corner of the standard keyboard as shown in the following.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img00/Key4CodeChunk} 

}

\caption{The location of backquote on the standard keyboard}\label{fig:unnamed-chunk-4}
\end{figure}

There are two code chunks: executable and non-executable chunks. The following code chunk is non-executable since is no argument specified in the \texttt{\{\}}.

\begin{figure}

{\centering \includegraphics[width=12.54in]{img00/Non-executable-code-chunk} 

}

\caption{Non-executable code chunk.}\label{fig:unnamed-chunk-5}
\end{figure}

\begin{verbatim}
This is a code chunk
\end{verbatim}

To write a code chunk that will be executed, we can simply put the letter \texttt{r} inside the curly bracket. If the code chunk is executable, you will the green arrow on the top-right corner of the chunk.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img00/Executable-code-chunk} 

}

\caption{Executable code chunk.}\label{fig:unnamed-chunk-6}
\end{figure}

We can define R objects with and without any outputs. In the above R code chunk, we define an R object under the name \texttt{x} and assign value 5 to \texttt{x} (the first line of the code). We also request an output that prints the value of \texttt{x}. The above executable code chunk gives output \texttt{{[}1{]}\ 5} in the Markdown document. The same output in the knit output files is in a box with a transparent background in the form \texttt{\#\#\ {[}1{]}\ 5}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \DecValTok{5}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

We can also use an argument in the code chunk to control the output. For example, the following code chunk will be evaluated when kitting to other formats of files. But we can still click the green arrow inside the code chunk to evaluate the code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \DecValTok{5}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\hypertarget{graphics-generated-from-r-code-chunks}{%
\subsubsection{Graphics Generated from R Code Chunks}\label{graphics-generated-from-r-code-chunks}}

In the previous sub-sections, we include images from external image files. In fact, can use the R function to generate graphics (other than interacting with plots, etc.) in the markdown file \& knit. For instance, we can generate the following image from R and include it in the Markdown document and the knitter output files.

Unlike the way of including an external image in to the R code chunk in which we use chunk option \textbf{out.width=``80\%''} or \textbf{out.height = ``60\%'', out.width=``80\%''} to specify the dimension of the displayed image, The graphics generated from R need a different option to specify the dimension. The dimension of the following graph is specified by \texttt{\{r,\ fig.align="center",\ fig.height=5,\ fig.width=5,\ fig.cap=\ "R\ Generated\ Graph"\}}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(iris)}
\FunctionTok{plot}\NormalTok{(iris[,}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-10-1} 

}

\caption{R Generated Graph}\label{fig:unnamed-chunk-10}
\end{figure}

\hypertarget{collaborative-platforms}{%
\section{Collaborative Platforms}\label{collaborative-platforms}}

There are many platforms and technologies available for applied statisticians and data scientists. We will use RPubs (\url{https://rpubs.com/}) and GitHub Repository (\url{https://github.com/}) in this class.

\hypertarget{rpubs}{%
\subsection{RPubs}\label{rpubs}}

\textbf{RPubs} is a free web server provided by RStudio (recently changed to \textbf{Posit}) that you can use it to publish you analytic reports and code and share with your peers and friends worldwide.

To use this resource, you need to sign up an account with RPubs first. Onece you set up your RPubs account, you can then create reports via RMarkdown and publish them on RPubs in the HTML format. You can share your work with people by providing the hyperlink to them.

In this class, all preject reports are required to be published on RPubs so I can read your work directly from RPubs. You need to submit the links to you reports via D2L dropbox.

\hypertarget{github-repository}{%
\subsection{GitHub Repository}\label{github-repository}}

GitHub is an online software development platform. It's used for storing, tracking, and collaborating on software projects.

To use it, you need to create an account. After you set your GitHub account, you can upload your files (text, code, photos, videos, etc) to the repository. You can also use GitHub to host your personal web page (static).

In this class, all data sets you are going to use in your assignments and project are required to uploaded to your specific repository so you can read your data sets directly from GitHub repository.

\hypertarget{github}{%
\section{Github}\label{github}}

\hypertarget{what-is-github}{%
\subsection{What is Github?}\label{what-is-github}}

GitHub is a social networking site for programmers to share their code. Many companies and organizations use it to facilitate project management and collaboration. It is the most prominent source code host, with over 60 million new repositories.

Most importantly, it is free. We can also use this resource to host web pages. Many images and data sets that I used are stored on GitHub. You need to register a GitHub account (\url{https://github.com/login}) to use create GitHub repositories and download and install Git for version control (version control is not required for this course, but is is extremely important in practice). The following Figure 1.5 shows the GitHub front page.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img00/github} 

}

\caption{GitHub front page.}\label{fig:unnamed-chunk-11}
\end{figure}

\hypertarget{getting-started-with-github}{%
\subsection{Getting Started with GitHub}\label{getting-started-with-github}}

We will use screenshots to demonstrate how to create a repository, folders, and files.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  After you logged into your account, you click the ``continue for free'' button located at the bottom of the following page (screenshot, Figure 1.6)
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img00/FreeGithub} 

}

\caption{The first page after logging-on.}\label{fig:unnamed-chunk-12}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Now you see your Github front page. Click the green button ``create repository'' on the left panel. Our first repository is called ``sta553'' (Figure 1.7)
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img00/GithubFrontPage} 

}

\caption{Starting creating repository.}\label{fig:unnamed-chunk-13}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  To organize files in the repository \texttt{sta553}, We want folders for different files. To create a folder under \texttt{sta553}, click the hyperlink `creating a new file (Figure 1.8)
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img00/Create1stFolder} 

}

\caption{Creating new folders to organize your files.}\label{fig:unnamed-chunk-14}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The first folder to create is called the \texttt{data} folder which will be used to store data files. After typing ``data/'', a new box appears under the ``data'' folder, type the first file name - readme, and the content of the file (see the screenshot). In the end, click the green button ``Commit new file'' to complete the creation of the first folder in the repository \texttt{data} (Figure 1.9).
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img00/Create1stFileIn1stFolder} 

}

\caption{Creating new files in a folder created earlier.}\label{fig:unnamed-chunk-15}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  To load the data file to the \texttt{data} folder, we click the drop-down menu on the top right corner and select \texttt{upload\ files} (Figure 1.10)
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img00/CreateNextFolder} 

}

\caption{Creating another new folder.}\label{fig:unnamed-chunk-16}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  To create other folders under \texttt{sta553}, we click \texttt{Creating\ New\ File}, and we can create a new folder \texttt{image} similarly (Figure 1.11).
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img00/CreateNewImageFolder} 

}

\caption{Creating new folders for specialized files such as image files.}\label{fig:unnamed-chunk-17}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  To create a new repository, Click the drop-down menu on the top right corner and select \texttt{New\ repository} to create a new repository (Figure 1.12).
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img00/CreateNewRepos} 

}

\caption{Creating new repositories for different projects}\label{fig:unnamed-chunk-18}
\end{figure}

\hypertarget{data-input-and-output-in-r}{%
\chapter{Data Input and Output in R}\label{data-input-and-output-in-r}}

Loading data into R can be quite frustrating since there so many different ways available for different types data files. We compile a short note to list some of these functions that commonly used to read the most commonly used formats of data in practice. We may also want to use some data sets built in some libraries (packages) in R for the purpose of illustration and practice.

\hypertarget{built-in-data}{%
\section{Built-in Data}\label{built-in-data}}

Build-in data are also ready in R format. To get the list of available data sets in \textbf{base R}, we can use data(). However, if we want to get the list of data sets available \textbf{in a package}, we first need to \textbf{load that package} then \textbf{data()} command shows the available data sets in that package.

For example, the library \texttt{datasets} in base R has a number of built-in data sets, we can use command \texttt{data()} or \texttt{ls("package:datasets")} to list all of these built-in data sets. When opening the R session, all built-in data sets in library \texttt{datasets} are all automatically loaded to the current working directory. We can simply use any of these data set The well-known \texttt{iris} is one of these built-in data sets. Next, we make a pair-wise scatter plot for all numerical variables in the iris data set in the following

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(iris[,}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{])  }\CommentTok{\# the 5th column is species of iris flower.}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-19-1} 

}

\caption{Pair-wise scatter plot of iris data}\label{fig:unnamed-chunk-19}
\end{figure}

To use a built-in data set that is not in the library \texttt{datasets} in base R, we need to load the library first. After the library is loaded, we can use command to list all data sets in that library. For example, \texttt{PimaIndiansDiabetes2} is a frequently used data set in logistic regression modeling. It is a built-in data set in the library \texttt{mlbench}. If we don't load library \texttt{mlbench} (Machine Learning Benchmark Problems), we will not be able to access that data set.

\begin{verbatim}
summary(PimaIndiansDiabetes2)
\end{verbatim}

The message returned from the above code is \texttt{Error\ in\ summary(PimaIndiansDiabetes2):\ object\ \textquotesingle{}PimaIndiansDiabetes2\textquotesingle{}\ not\ found}. However, if we load the library first, then generate the summary table, we will have

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"mlbench"}\NormalTok{)) \{      }\CommentTok{\# check whether *mlbench* is installed in the machine }
   \FunctionTok{install.packages}\NormalTok{(}\StringTok{"mlbench"}\NormalTok{)  }\CommentTok{\# if not, install it}
   \FunctionTok{library}\NormalTok{(mlbench)             }\CommentTok{\# then load the package!}
\NormalTok{\}}
\FunctionTok{data}\NormalTok{(PimaIndiansDiabetes2)      }\CommentTok{\# load the specific data set to the working directory}
\FunctionTok{summary}\NormalTok{(PimaIndiansDiabetes2)   }\CommentTok{\# use the data set for analysis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     pregnant         glucose         pressure         triceps     
##  Min.   : 0.000   Min.   : 44.0   Min.   : 24.00   Min.   : 7.00  
##  1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 64.00   1st Qu.:22.00  
##  Median : 3.000   Median :117.0   Median : 72.00   Median :29.00  
##  Mean   : 3.845   Mean   :121.7   Mean   : 72.41   Mean   :29.15  
##  3rd Qu.: 6.000   3rd Qu.:141.0   3rd Qu.: 80.00   3rd Qu.:36.00  
##  Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  
##                   NA's   :5       NA's   :35       NA's   :227    
##     insulin            mass          pedigree           age        diabetes 
##  Min.   : 14.00   Min.   :18.20   Min.   :0.0780   Min.   :21.00   neg:500  
##  1st Qu.: 76.25   1st Qu.:27.50   1st Qu.:0.2437   1st Qu.:24.00   pos:268  
##  Median :125.00   Median :32.30   Median :0.3725   Median :29.00            
##  Mean   :155.55   Mean   :32.46   Mean   :0.4719   Mean   :33.24            
##  3rd Qu.:190.00   3rd Qu.:36.60   3rd Qu.:0.6262   3rd Qu.:41.00            
##  Max.   :846.00   Max.   :67.10   Max.   :2.4200   Max.   :81.00            
##  NA's   :374      NA's   :11
\end{verbatim}

We can also use \texttt{data()} to list all buit-in data set in the package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mlbench)             }\CommentTok{\# load the package available in the machine}
\FunctionTok{data}\NormalTok{(}\AttributeTok{package =} \StringTok{\textquotesingle{}mlbench\textquotesingle{}}\NormalTok{)    }\CommentTok{\# list all data sets available in the package}
\end{Highlighting}
\end{Shaded}

\textbf{Note}: if the package name is not specified in the argument of \texttt{data()}, \textbf{all} built-in data sets in \textbf{all loaded packages} will be listed!

\hypertarget{loading-external-data-sets}{%
\section{Loading External Data Sets}\label{loading-external-data-sets}}

Loading an external data set to can be a challenge depending on the format and the structure of the data.

\begin{quote}
Before moving on and discover how to load data into R, it might be useful to go over the following checklist that will make it easier to import the data correctly into R:
\end{quote}

\begin{itemize}
\item
  If working with spreadsheets, the first row is usually reserved for the header, while the first column is used to identify the sampling unit;
\item
  Avoid names, values, or fields with blank spaces. Otherwise, each word will be interpreted as a separate variable, resulting in errors that are related to the number of elements per line in your data set;
\item
  If we want to concatenate words, inserting a \texttt{.} in between two words instead of a space;
  Short names are preferred over longer names;
\item
  Try to avoid using names that contain symbols, such as ?, \$,\%, \^{}, \&, *, (, ),-,\#, ?,,,\textless,\textgreater, /, \textbar, , {[} ,{]} ,\{, and \};
\item
  Delete any comments in the Excel file to avoid extra columns or NA's to be added to the file; and
\item
  Make sure that any missing values in the data set are indicated with NA.
\end{itemize}

\hypertarget{preparing-r-workspace}{%
\subsection{Preparing R Workspace}\label{preparing-r-workspace}}

It is a good practice to empty all R objects defined in other R sessions to avoid miss-use (unintentional-use) of R objects with simple easy names that were defined in the other sessions.

We might want to start an environment that is \textbf{NOT} filled with data and values from other sessions. The simplest way is to delete all existing R objects before starting new analysis using the following line of code

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list=}\FunctionTok{ls}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

The \texttt{rm()} function removes objects from a specified environment. We can then use \texttt{ls()} to check whether all previously defined R objects were removed.

\hypertarget{setting-up-working-directory}{%
\subsection{Setting-up Working Directory}\label{setting-up-working-directory}}

It is dependent on whether you are writing R script or RMarkdown. If writing an R script, it is suggested to set-up a working directory for the specific analysis task using the following R function

\begin{verbatim}
setwd("<location of your dataset>")
\end{verbatim}

If writing RMarkdown document, by the default, the folder we save the RMarkdown document is the automatically set as working directory (also called document directory). \texttt{setwd()} in the code chunk does not work in RMarkdown!

\hypertarget{read-common-files-into-r}{%
\section{Read Common Files into R}\label{read-common-files-into-r}}

The following basic R functions focus on getting spreadsheets into R, rather than Excel or other type of files. There are ways of importing other files into R using various R functions in different packages.

\hypertarget{read-txt-files-with-read.table}{%
\subsection{Read TXT files with read.table()}\label{read-txt-files-with-read.table}}

If you have a \texttt{.txt} or a \texttt{tab-delimited} text file, we can easily import it with the basic R function read.table().

\begin{itemize}
\tightlist
\item
  \textbf{Data with No Column Names}
\end{itemize}

In other words, the contents of the file will look similar to this

\begin{verbatim}
1   6   a
2   7   b
3   8   c
4   9   d
5   10  e
\end{verbatim}

If the above data file is saved in a local folder, say, \texttt{C:\textbackslash{}peng\textbackslash{}eBooks\textbackslash{}STA321\textbackslash{}fakeDat.txt}, we use the following code to load this data to R

\begin{verbatim}
myFakeData <- read.table("C:\\peng\\eBooks\\STA321\\fakeDat.txt")
\end{verbatim}

If the above data file is on a web server, say, \texttt{http://www.someserver.com/STA321/fakeDat.txt}, we use the following code to load this data to R

\begin{verbatim}
myFakeData <- read.table("http://www.someserver.com/STA321/fakeDat.txt")
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \textbf{Data with Good Column Names}
\end{itemize}

The above fake data file does not have column names, the loaded R data set will be automatically assigned names \texttt{V1}, \texttt{V2}, and \texttt{V3} to the three corresponding columns. If the data file has column names and we want to use it in the R data frame, we need to use the \texttt{header} argument.

\begin{verbatim}
ID num  char
1   6   a
2   7   b
3   8   c
4   9   d
5   10  e
\end{verbatim}

We can use the following code to load the data correctly in R.

\begin{verbatim}
myFakeData <- read.table("C:\\peng\\eBooks\\STA321\\fakeDat.txt", header = TRUE)
\end{verbatim}

or

\begin{verbatim}
myFakeData <- read.table("http://www.someserver.com/STA321/fakeDat.txt", header = TRUE)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \textbf{Data With Bad Column Names}
\end{itemize}

Sometimes, we may have a data file with messy or unwanted column names, some comments about the data set, we can use \texttt{skip} argument to skip certain number of rows when read the data file to R then rename the columns with appropriate variable names. For example, if the data file has comments like

\begin{verbatim}
This data set was collected by someone in the cloud.
the column names also violates the naming convention.
ID num@3  char#
1   6   a
2   7   b
3   8   c
4   9   d
5   10  e
\end{verbatim}

We can read the above data in R and rename the columns using the following code

\begin{verbatim}
myFakeData <- read.table("C:\\peng\\eBooks\\STA321\\fakeDat.txt", skip = 3)
names(myFakeData) = c("var1", "var2", "var3")
\end{verbatim}

\hypertarget{data-values-separated-by-special-symbol}{%
\section{Data Values Separated By Special Symbol}\label{data-values-separated-by-special-symbol}}

If the data file has a special separator, we need to use argument \texttt{sep=} to specify the separator.

\begin{verbatim}
This data set was collected by someone in the cloud.
the column names also violates the naming convention.
ID @ num @  char
1  @  6  @  a
2  @  7  @  b
3  @  8  @  c
4  @  9  @  d
5  @ 10  @  e
\end{verbatim}

The following code will accomplish the task.

\begin{verbatim}
myFakeData <- read.table("C:\\peng\\eBooks\\STA321\\fakeDat.txt", header = TRUE, skip = 2, sep = "@")
\end{verbatim}

\hypertarget{importing-a-csv-into-r}{%
\subsection{Importing a CSV into R}\label{importing-a-csv-into-r}}

If the values were separated with a \texttt{,} or \texttt{;}, we usually are working with a \texttt{.csv} file. Its contents will look similar to this:

\begin{verbatim}
Col1,Col2,Col3
1,    2,   a
4,    5,   b
7,    8,   d
\end{verbatim}

To successfully load this file into R, we can also use the \texttt{read.table()} function in which we specify the separator character, or we can use the \texttt{read.csv()} or \texttt{read.csv2()} functions. The former function is used if the separator is a \texttt{,}, the latter if \texttt{;} is used to separate the values in your data file.

Remember that the \texttt{read.csv()} as well as the \texttt{read.csv2()} function are almost identical to the \texttt{read.table()} function, with the sole difference that they have the header and fill arguments set as TRUE by default.

\hypertarget{import-sas-spss-and-other-data-sets-into-r}{%
\section{Import SAS, SPSS, and Other Data Sets into R}\label{import-sas-spss-and-other-data-sets-into-r}}

R is a programming language and software environment for statistical computing. sometimes we need to import data from advanced statistical software programs such as SAS and SPSS. We need to install specialized packages to achieve this task. The well-known package \texttt{foreign} has several R function to read different data sets generated from different software programs.

\hypertarget{import-spss-files-into-r}{%
\subsection{Import SPSS Files into R}\label{import-spss-files-into-r}}

The R package \texttt{foreign} has a function \texttt{read.spss()} to read SPSS data set (with extension \texttt{.sav}). The code is something like

\begin{verbatim}
# Activate the `foreign` library
library(foreign)

# Read the SPSS data
mySPSSData <- read.spss("example.sav")
\end{verbatim}

If we want the result to be displayed in a data frame, we can set the \texttt{to.data.frame} argument of the \texttt{read.spss()} function to \texttt{TRUE}. Furthermore, if we \textbf{do NOT} want the variables with value labels to be converted into R factors with corresponding levels, we should set the \texttt{use.value.labels} argument to \texttt{FALSE}:

\begin{verbatim}
# Activate the `foreign` library
library(foreign)

# Read the SPSS data
mySPSSData <- read.spss("example.sav",
                       to.data.frame=TRUE,
                       use.value.labels=FALSE)
\end{verbatim}

Remember that factors are variables that can only contain a limited number of different values. As such, they are often called ``categorical variables''. The different values of factors can be labeled and are therefore often called ``value labels''

\hypertarget{mport-stata-files-into-r}{%
\subsection{mport Stata Files into R}\label{mport-stata-files-into-r}}

To import Stata files, we use the \texttt{read.dta()} function in package \texttt{foreign} to read data into R:

\begin{verbatim}
# Activate the `foreign` library
library(foreign)

# Read Stata data into R
mydata <- read.dta("<Path to file>") 
\end{verbatim}

\hypertarget{import-systat-files-into-r}{%
\subsection{Import Systat Files into R}\label{import-systat-files-into-r}}

If you want to get Systat files into R, we also want to use the foreign package, just like shown below:

\begin{verbatim}
# Activate the `foreign` library
library(foreign)

# Read Systat data
mydata <- read.systat("<Path to file>") 
\end{verbatim}

\hypertarget{import-sas-files-into-r}{%
\subsection{Import SAS Files into R}\label{import-sas-files-into-r}}

We need to install the \texttt{sas7bdat} package and load it, and then invoke the \texttt{read.sas7bdat()} function contained within the package.

\begin{verbatim}
# Activate the `sas7bdat` library
library(sas7bdat)

# Read in the SAS data
mySASData <- read.sas7bdat("example.sas7bdat")
\end{verbatim}

\textbf{Note that} we can also use the foreign library to load in SAS data in R. In such cases, we'll start from a SAS Permanent Dataset or a \texttt{SAS\ XPORT} Format Library with the \texttt{read.ssd()} and \texttt{read.xport()} functions, respectively. But this is relatively inconvenient.

\hypertarget{base-r-graphical-functions}{%
\chapter{Base R Graphical Functions}\label{base-r-graphical-functions}}

This note introduces the graphical capabilities of base R graphical functions. Base R graphical system contains a set of \textbf{high-level plotting functions} such as \texttt{plot()}, \texttt{hist()}, \texttt{barplot()}, etc., and also a set of \textbf{low-level functions} that are used jointly with the high-level plotting functions such as \texttt{points()}, \texttt{lines()}, \texttt{text()}, \texttt{segments()}, etc. to make a flexible graphical system.

\hypertarget{r-graphic-devices}{%
\section{R Graphic Devices}\label{r-graphic-devices}}

R is able to output graphics to the screen or save them directly to a file (e.g.~postscript, pdf, svg, png, jpeg, etc.). The different functions for producing graphical output are known as \textbf{Graphic Devices}. For example, \texttt{pdf()} would invoke the \texttt{pdf\ device}, while \texttt{png()} would invoke the \texttt{png\ device}. Type ?Devices into the R console to see a list of graphical devices that are available to R on your system.

By default, graphical output is sent to the screen. As R is cross-platform, the graphics device for producing **screen*``** graphics differs by the system. The available fonts may also differ by the system and graphical device.

\hypertarget{plotting-with-base-graphics-plot}{%
\section{\texorpdfstring{Plotting with Base Graphics: \texttt{plot()}}{Plotting with Base Graphics: plot()}}\label{plotting-with-base-graphics-plot}}

\texttt{plot()} is the most important high-level graphic function. When using \texttt{plot()}, the output is sent to the \textbf{graphics device}. This creates a ``plot region'' on the graphics device. The visible area of the graphics device is also known as the ``device region''. The plot region on the other hand is the area of the plot (usually bounded by a box) and does not include the plot axes, labels, or margins.

The plot region, plus the axes, labels, and margins are known as the ``figure region''. Often, the device region and figure region can be the same size - but they are not the same thing.

\hypertarget{outer-and-inner-margins}{%
\subsection{Outer and Inner Margins}\label{outer-and-inner-margins}}

\texttt{par(mar=c(x,\ x,\ x,\ x),\ oma\ =\ c(x,\ x,\ x,\ x))}

\textbf{\color{red}Almost all kinds of plots, charts, and graphs can be produced using base graphics}, These plots can be fully customized using \texttt{par()} (graphical parameter). The following figure explains the graphical parameters we can use to customize the graphical layout.

\begin{center}\includegraphics[width=0.75\linewidth]{img02/w02-RGraphicMargins} \end{center}

\textbf{Example 1}: We make a scatter plot of the two numerical variable in the well-known data set \texttt{iris}. To make nearly overlapped data points distinguishable, we are going to use R function \texttt{alpha()} in the library \texttt{scales}.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"scales"}\NormalTok{)) \{}
   \FunctionTok{install.packages}\NormalTok{(}\StringTok{"scales"}\NormalTok{)}
   \FunctionTok{library}\NormalTok{(scales)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We read the data from the GitHub repository.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{library}\NormalTok{(scales)}
\NormalTok{iris }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww02/w02{-}iris.txt"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{oma=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalLength, iris}\SpecialCharTok{$}\NormalTok{PetalLength, }\AttributeTok{pch =} \DecValTok{16}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"Sepal Length vs Petal Length (Outer)"}\NormalTok{, }
      \AttributeTok{outer=}\ConstantTok{TRUE}\NormalTok{, }
      \AttributeTok{col.main =} \StringTok{"red"}\NormalTok{, }
      \AttributeTok{cex.main=}\DecValTok{1}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"Sepal Length vs Petal Length (Inner)"}\NormalTok{, }
      \AttributeTok{outer=}\ConstantTok{FALSE}\NormalTok{,}
      \AttributeTok{col.main =} \StringTok{"blue"}\NormalTok{,}
      \AttributeTok{cex.main=}\FloatTok{0.8}\NormalTok{)}
\DocumentationTok{\#\# Coloring points with the transparency level}
\DocumentationTok{\#\# species ID}
\NormalTok{setosa }\OtherTok{=} \FunctionTok{which}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Classification }\SpecialCharTok{==} \StringTok{"Iris{-}setosa"}\NormalTok{)}
\NormalTok{versicolor }\OtherTok{=} \FunctionTok{which}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Classification }\SpecialCharTok{==} \StringTok{"Iris{-}versicolor"}\NormalTok{)}
\NormalTok{virginica }\OtherTok{=} \FunctionTok{which}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Classification }\SpecialCharTok{==} \StringTok{"Iris{-}virginica"}\NormalTok{)}
\DocumentationTok{\#\# adding points}
\FunctionTok{points}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalLength[setosa], iris}\SpecialCharTok{$}\NormalTok{PetalLength[setosa], }
       \AttributeTok{pch=}\DecValTok{16}\NormalTok{, }
       \AttributeTok{col =} \FunctionTok{alpha}\NormalTok{(}\StringTok{"purple"}\NormalTok{, }\FloatTok{0.5}\NormalTok{),    }\CommentTok{\# add a transparency level}
       \AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalLength[versicolor], iris}\SpecialCharTok{$}\NormalTok{PetalLength[versicolor], }
       \AttributeTok{pch=}\DecValTok{16}\NormalTok{, }
       \AttributeTok{col =} \FunctionTok{alpha}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
       \AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalLength[virginica], iris}\SpecialCharTok{$}\NormalTok{PetalLength[virginica], }
       \AttributeTok{pch=}\DecValTok{16}\NormalTok{, }
       \AttributeTok{col =} \FunctionTok{alpha}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
       \AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"setosa"}\NormalTok{, }\StringTok{"versicolor"}\NormalTok{, }\StringTok{"virginica"}\NormalTok{),}
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"purple"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }
       \AttributeTok{pch=}\FunctionTok{rep}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{3}\NormalTok{), }\AttributeTok{cex=}\FunctionTok{rep}\NormalTok{(}\FloatTok{1.5}\NormalTok{,}\DecValTok{3}\NormalTok{), }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-25-1} \end{center}

\hypertarget{simple-multi-panel-plots}{%
\subsection{Simple multi-panel Plots}\label{simple-multi-panel-plots}}

\texttt{layout()} or \texttt{par(mfrow\ =\ c())}

\textbf{Example 2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww02/w02{-}iris.txt"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\DocumentationTok{\#\# species ID}
\NormalTok{setosa }\OtherTok{=} \FunctionTok{which}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Classification }\SpecialCharTok{==} \StringTok{"Iris{-}setosa"}\NormalTok{)}
\NormalTok{versicolor }\OtherTok{=} \FunctionTok{which}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Classification }\SpecialCharTok{==} \StringTok{"Iris{-}versicolor"}\NormalTok{)}
\NormalTok{virginica }\OtherTok{=} \FunctionTok{which}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Classification }\SpecialCharTok{==} \StringTok{"Iris{-}virginica"}\NormalTok{)}
\DocumentationTok{\#\#\#}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{oma=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalLength, iris}\SpecialCharTok{$}\NormalTok{PetalLength, }\AttributeTok{pch =} \DecValTok{16}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{,}
            \AttributeTok{xlab =} \StringTok{"Sepal Width"}\NormalTok{,}
            \AttributeTok{ylab =} \StringTok{"Petal Width"}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"Sepal Length vs Petal Length"}\NormalTok{, }
      \AttributeTok{outer=}\ConstantTok{FALSE}\NormalTok{,}
      \AttributeTok{col.main =} \StringTok{"blue"}\NormalTok{,}
      \AttributeTok{cex.main=}\FloatTok{0.8}\NormalTok{)}
\DocumentationTok{\#\# adding points}
\FunctionTok{points}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalLength[setosa], iris}\SpecialCharTok{$}\NormalTok{PetalLength[setosa], }
       \AttributeTok{pch=}\DecValTok{16}\NormalTok{, }
       \AttributeTok{col =} \FunctionTok{alpha}\NormalTok{(}\StringTok{"purple"}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
       \AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalLength[versicolor], iris}\SpecialCharTok{$}\NormalTok{PetalLength[versicolor], }
       \AttributeTok{pch=}\DecValTok{16}\NormalTok{, }
       \AttributeTok{col =} \FunctionTok{alpha}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
       \AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalLength[virginica], iris}\SpecialCharTok{$}\NormalTok{PetalLength[virginica], }
       \AttributeTok{pch=}\DecValTok{16}\NormalTok{, }
       \AttributeTok{col =} \FunctionTok{alpha}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
       \AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"setosa"}\NormalTok{, }\StringTok{"versicolor"}\NormalTok{, }\StringTok{"virginica"}\NormalTok{),}
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"purple"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }
       \AttributeTok{pch=}\FunctionTok{rep}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{3}\NormalTok{), }\AttributeTok{cex=}\FloatTok{0.8}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{plot}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalWidth, iris}\SpecialCharTok{$}\NormalTok{PetalWidth, }\AttributeTok{pch =} \DecValTok{16}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{,}
            \AttributeTok{xlab =} \StringTok{"Sepal Width"}\NormalTok{,}
            \AttributeTok{ylab =} \StringTok{"Petal Width"}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"Sepal Length vs Petal Length"}\NormalTok{, }
      \AttributeTok{outer=}\ConstantTok{FALSE}\NormalTok{,}
      \AttributeTok{col.main =} \StringTok{"blue"}\NormalTok{,}
      \AttributeTok{cex.main=}\FloatTok{0.8}\NormalTok{)}
\DocumentationTok{\#\# adding points}
\FunctionTok{points}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalWidth[setosa], iris}\SpecialCharTok{$}\NormalTok{PetalWidth[setosa], }
       \AttributeTok{pch=}\DecValTok{16}\NormalTok{, }
       \AttributeTok{col =} \FunctionTok{alpha}\NormalTok{(}\StringTok{"purple"}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
       \AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalWidth[versicolor], iris}\SpecialCharTok{$}\NormalTok{PetalWidth[versicolor], }
       \AttributeTok{pch=}\DecValTok{16}\NormalTok{, }
       \AttributeTok{col =} \FunctionTok{alpha}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
       \AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalWidth[virginica], iris}\SpecialCharTok{$}\NormalTok{PetalWidth[virginica], }
       \AttributeTok{pch=}\DecValTok{16}\NormalTok{, }
       \AttributeTok{col =} \FunctionTok{alpha}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
       \AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"setosa"}\NormalTok{, }\StringTok{"versicolor"}\NormalTok{, }\StringTok{"virginica"}\NormalTok{),}
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"purple"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }
       \AttributeTok{pch=}\FunctionTok{rep}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{3}\NormalTok{), }\AttributeTok{cex=}\FloatTok{0.8}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\DocumentationTok{\#\# Overall Title}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"Comaring Sepal and Petal Sizes"}\NormalTok{, }
      \AttributeTok{outer=}\ConstantTok{TRUE}\NormalTok{,}
      \AttributeTok{col.main =} \StringTok{"darkred"}\NormalTok{,}
      \AttributeTok{cex.main=}\FloatTok{1.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-26-1} \end{center}

\hypertarget{making-our-own-colors}{%
\section{Making Our Own Colors}\label{making-our-own-colors}}

We can make transparent colors using R and the \texttt{rgb()\ command}. These colors can be useful for charts and graphics with overlapping elements.

The \texttt{rgb()\ command} defines a new color using numerical values (0--255) for red, green and blue. In addition, we also set an alpha value (also 0--255), which sets the transparency (0 being fully transparent and 255 being ``solid'').

The following example takes the standard blue and makes it transparent (\textasciitilde50\%):

\texttt{mycol\ \textless{}-\ rgb(0,\ 0,\ 255,\ max\ =\ 255,\ alpha\ =\ 125,\ names\ =\ "blue50")}

You can also use \texttt{col2rgb()} to check the composition of an existing named R color and modify it to define your own favorite colors.

\textbf{Example 3}: We check the RGB code from several named R colors in the following

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{col2rgb}\NormalTok{(}\StringTok{"darkred"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]
## red    139
## green    0
## blue     0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{col2rgb}\NormalTok{(}\StringTok{"skyblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]
## red    135
## green  206
## blue   235
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{col2rgb}\NormalTok{(}\StringTok{"gold"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]
## red    255
## green  215
## blue     0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{col2rgb}\NormalTok{(}\StringTok{"purple"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]
## red    160
## green   32
## blue   240
\end{verbatim}

\textbf{Example 3} We define a color by modifying ``gold'' and ``purple''. I take the average of the RGB codes of ``gold'' and ``purple'' to see what the color looks like.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ratio }\OtherTok{=} \FloatTok{0.4}
\NormalTok{avggoldpurple }\OtherTok{=} \FunctionTok{round}\NormalTok{(ratio}\SpecialCharTok{*}\FunctionTok{col2rgb}\NormalTok{(}\StringTok{"gold"}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{ratio)}\SpecialCharTok{*}\FunctionTok{col2rgb}\NormalTok{(}\StringTok{"purple"}\NormalTok{))}
\DocumentationTok{\#\#}
\NormalTok{mycol}\FloatTok{.1} \OtherTok{=} \FunctionTok{rgb}\NormalTok{(avggoldpurple[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], avggoldpurple[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{], avggoldpurple[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{], }\AttributeTok{max =} \DecValTok{255}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{125}\NormalTok{, }\AttributeTok{names =} \StringTok{"goldpuple50"}\NormalTok{)}
\DocumentationTok{\#\#\#}
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{),  }\AttributeTok{col =}\NormalTok{ mycol}\FloatTok{.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-28-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ratio }\OtherTok{=} \FloatTok{0.8}
\NormalTok{avggoldpurple }\OtherTok{=} \FunctionTok{round}\NormalTok{(ratio}\SpecialCharTok{*}\FunctionTok{col2rgb}\NormalTok{(}\StringTok{"gold"}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{ratio)}\SpecialCharTok{*}\FunctionTok{col2rgb}\NormalTok{(}\StringTok{"purple"}\NormalTok{))}
\DocumentationTok{\#\#}
\NormalTok{mycol}\FloatTok{.2} \OtherTok{=} \FunctionTok{rgb}\NormalTok{(avggoldpurple[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], avggoldpurple[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{], avggoldpurple[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{], }\AttributeTok{max =} \DecValTok{255}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{125}\NormalTok{, }\AttributeTok{names =} \StringTok{"goldpuple50"}\NormalTok{)}
\DocumentationTok{\#\#\#}
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),  }\AttributeTok{col =}\NormalTok{ mycol}\FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-29-1} \end{center}

\hypertarget{effective-data-preparation}{%
\chapter{Effective Data Preparation}\label{effective-data-preparation}}

In this note, we use a health registry data set as an example to illustrate the use of some of the base R commands in creating an analytic data set analysis and modeling.

\hypertarget{data-set-description}{%
\section{Data Set Description}\label{data-set-description}}

The Current Population Survey (CPS, \url{http://www.bls.census.gov/cps/overmain.htm}) is a monthly survey of about 50,000 households conducted by the Bureau of the Census for the Bureau of Labor Statistics. The survey has been conducted for more than 50 years. The CPS is the primary source of information on the labor force characteristics of the U.S. population. The sample is scientifically selected to represent the civilian noninstitutional population. Respondents are interviewed to obtain information about the employment status of each member of the household 15 years of age and older. However, published data focus on those ages 16 and over. The sample provides estimates for the nation as a whole and serves as part of model-based estimates for individual states and other geographic areas.

Estimates obtained from the CPS include

\begin{itemize}
\tightlist
\item
  employment,
\item
  unemployment,
\item
  earnings,
\item
  hours of work, and
\item
  other indicators.
\end{itemize}

They are available by a variety of demographic characteristics including
* age,
* sex,
* race,
* marital status, and
* educational attainment.

They are also available by

\begin{itemize}
\tightlist
\item
  occupation,
\item
  industry, and
\item
  class of worker.
\end{itemize}

Supplemental questions to produce estimates on a variety of topics including

\begin{itemize}
\tightlist
\item
  School enrollment,
\item
  income,\\
\item
  previous work experience,
\item
  health,
\item
  employee benefits, and
\item
  work schedules
\end{itemize}

are also often added to the regular CPS questionnaire.

CPS data are used by government policymakers and legislators as important indicators of our nation's economic situation and for planning and evaluating many government programs. They are also used by the press, students, academics, and the general public.

In this note, we use a very small portion of the sample (\url{https://raw.githubusercontent.com/pengdsci/sta321/main/ww04/cps_00003.csv}) for illustrative purposes. The definitions of some of the variables can be found at (\url{https://www.bls.gov/cps/definitions.htm}). \textbf{We will not use this data to perform any meaningful analysis.}

The first few columns are what could be called administrative. They're unique identifiers for the different observations, the timing of the survey they've taken, and a bit of other information. So for now we don't need to pay much attention to MONTH, HWTFINL, CPSID, PERNUM, WTFINL, or CPSIDP. We will drop these variables.

The next few columns are concerned with the different geographies we have for the observations. This data is for individuals, but we also know the individual region (REGION), state (STATEFIP and STATECENSUS), and metropolitan area (METRO and METAREA). We can define a separate data set to store this geoinformation.

\hypertarget{base-r-commands-for-data-management}{%
\section{Base R Commands for Data Management}\label{base-r-commands-for-data-management}}

This note introduces several most commonly used R functions in data management.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\NormalTok{dat }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww04/cps\_00003.csv"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(dat))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
YEAR & SERIAL & MONTH & HWTFINL & CPSID & REGION & STATEFIP & METRO & METAREA & STATECENSUS & FAMINC & PERNUM & WTFINL & CPSIDP & AGE & SEX & RACE & EMPSTAT & LABFORCE & EDUC & VOTED & VOREG\\
\hline
2018 & 1 & 11 & 1703.832 & 20170800000000 & 32 & 1 & 2 & 3440 & 63 & 830 & 1 & 1703.832 & 20170800000000 & 26 & 2 & 100 & 10 & 2 & 111 & 98 & 98\\
\hline
2018 & 1 & 11 & 1703.832 & 20170800000000 & 32 & 1 & 2 & 3440 & 63 & 830 & 2 & 1845.094 & 20170800000000 & 26 & 1 & 100 & 10 & 2 & 123 & 98 & 98\\
\hline
2018 & 3 & 11 & 1957.313 & 20180900000000 & 32 & 1 & 2 & 5240 & 63 & 100 & 1 & 1957.313 & 20180900000000 & 48 & 2 & 200 & 21 & 2 & 73 & 2 & 99\\
\hline
2018 & 4 & 11 & 1687.784 & 20171000000000 & 32 & 1 & 2 & 5240 & 63 & 820 & 1 & 1687.784 & 20171000000000 & 53 & 2 & 200 & 10 & 2 & 81 & 2 & 99\\
\hline
2018 & 4 & 11 & 1687.784 & 20171000000000 & 32 & 1 & 2 & 5240 & 63 & 820 & 2 & 2780.421 & 20171000000000 & 16 & 1 & 200 & 10 & 2 & 50 & 99 & 99\\
\hline
2018 & 4 & 11 & 1687.784 & 20171000000000 & 32 & 1 & 2 & 5240 & 63 & 820 & 3 & 2780.421 & 20171000000000 & 16 & 1 & 200 & 10 & 2 & 50 & 99 & 99\\
\hline
\end{tabular}

The unique identifiers \textbf{CPSID} and \textbf{CPSIDP} are in the form of scientific notation, we need to convert them to a normal string version of the ID.

\hypertarget{working-with-scientific-notations}{%
\section{Working with Scientific Notations}\label{working-with-scientific-notations}}

Two global options we can use to print out the actual ID. See the self-explained options in the following code.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{digits =} \DecValTok{15}\NormalTok{, }\AttributeTok{scipen=}\DecValTok{999}\NormalTok{)}
\NormalTok{dat }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww04/cps\_00003.csv"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(dat))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
YEAR & SERIAL & MONTH & HWTFINL & CPSID & REGION & STATEFIP & METRO & METAREA & STATECENSUS & FAMINC & PERNUM & WTFINL & CPSIDP & AGE & SEX & RACE & EMPSTAT & LABFORCE & EDUC & VOTED & VOREG\\
\hline
2018 & 1 & 11 & 1703.8321 & 20170800000000 & 32 & 1 & 2 & 3440 & 63 & 830 & 1 & 1703.8321 & 20170800000000 & 26 & 2 & 100 & 10 & 2 & 111 & 98 & 98\\
\hline
2018 & 1 & 11 & 1703.8321 & 20170800000000 & 32 & 1 & 2 & 3440 & 63 & 830 & 2 & 1845.0939 & 20170800000000 & 26 & 1 & 100 & 10 & 2 & 123 & 98 & 98\\
\hline
2018 & 3 & 11 & 1957.3134 & 20180900000000 & 32 & 1 & 2 & 5240 & 63 & 100 & 1 & 1957.3134 & 20180900000000 & 48 & 2 & 200 & 21 & 2 & 73 & 2 & 99\\
\hline
2018 & 4 & 11 & 1687.7836 & 20171000000000 & 32 & 1 & 2 & 5240 & 63 & 820 & 1 & 1687.7836 & 20171000000000 & 53 & 2 & 200 & 10 & 2 & 81 & 2 & 99\\
\hline
2018 & 4 & 11 & 1687.7836 & 20171000000000 & 32 & 1 & 2 & 5240 & 63 & 820 & 2 & 2780.4215 & 20171000000000 & 16 & 1 & 200 & 10 & 2 & 50 & 99 & 99\\
\hline
2018 & 4 & 11 & 1687.7836 & 20171000000000 & 32 & 1 & 2 & 5240 & 63 & 820 & 3 & 2780.4215 & 20171000000000 & 16 & 1 & 200 & 10 & 2 & 50 & 99 & 99\\
\hline
\end{tabular}

A new R function \textbf{pander()} in the \texttt{pander\{\}} library was used in the above code to produce an R markdown table. We add more features to the output table (check the help document for more information and examples).

If the ID variable was truncated before saving to CSV format, then the truncated digits will not be recovered.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{digits =} \DecValTok{7}\NormalTok{)  }\CommentTok{\# change the default number digits}
\end{Highlighting}
\end{Shaded}

\hfill\break

\hypertarget{the-ifelse-function}{%
\section{\texorpdfstring{The \textbf{ifelse()} Function}{The ifelse() Function}}\label{the-ifelse-function}}

\textbf{ifelse} statement is also called a vectorized conditional statement. It is commonly used in defining new variables.

\begin{center}\includegraphics[width=0.7\linewidth]{img03/r-ifelse-function-syntax} \end{center}

For example, we can define a categorical variable, denoted by \textbf{groupAge}, based on the \textbf{AGE} variable in the original data frame. If
\texttt{AGE\ \textgreater{}\ 50,\ then\ groupAge\ =\ "(50,\ 150)"} otherwise \texttt{groupAge\ =\ "{[}16,\ 50{]}"} The following code defines this new variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{groupAge }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{AGE }\SpecialCharTok{\textgreater{}} \DecValTok{50}\NormalTok{, }\StringTok{"(50, 150)"}\NormalTok{, }\StringTok{"[16, 50]"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(dat[, }\FunctionTok{c}\NormalTok{(}\StringTok{"AGE"}\NormalTok{, }\StringTok{"groupAge"}\NormalTok{)]))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l}
\hline
AGE & groupAge\\
\hline
26 & [16, 50]\\
\hline
26 & [16, 50]\\
\hline
48 & [16, 50]\\
\hline
53 & (50, 150)\\
\hline
16 & [16, 50]\\
\hline
16 & [16, 50]\\
\hline
\end{tabular}

If we define another \textbf{groupAge} with more than two categories, we can still call \textbf{ifelse} multiple times. For example, we define \textbf{groupAge02} as: \texttt{is\ AGE\ \textgreater{}\ 50,\ then\ groupAge02\ =\ "(50,\ 150)",\ if\ 30\ \textless{}=\ AGE\ \textless{}\ 50,\ groupAge02\ =\ {[}30,\ 50),otherewise,\ groupAge02\ =\ "{[}16,\ 30)"}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{groupAge02 }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{AGE }\SpecialCharTok{\textgreater{}} \DecValTok{50}\NormalTok{, }\StringTok{"(50, 150)"}\NormalTok{, }\FunctionTok{ifelse}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{AGE }\SpecialCharTok{\textless{}} \DecValTok{30}\NormalTok{, }\StringTok{"[16, 30)"}\NormalTok{, }\StringTok{"[30, 50)"}\NormalTok{))}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(dat[, }\FunctionTok{c}\NormalTok{(}\StringTok{"AGE"}\NormalTok{, }\StringTok{"groupAge"}\NormalTok{, }\StringTok{"groupAge02"}\NormalTok{)]))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l|l}
\hline
AGE & groupAge & groupAge02\\
\hline
26 & [16, 50] & [16, 30)\\
\hline
26 & [16, 50] & [16, 30)\\
\hline
48 & [16, 50] & [30, 50)\\
\hline
53 & (50, 150) & (50, 150)\\
\hline
16 & [16, 50] & [16, 30)\\
\hline
16 & [16, 50] & [16, 30)\\
\hline
\end{tabular}

\textbf{Remark}: \textbf{ifelse()} is particularly useful when you want to combine categories of existing categorical variables.

\hfill\break

\hypertarget{the-cut-function}{%
\section{\texorpdfstring{The \textbf{cut()} Function}{The cut() Function}}\label{the-cut-function}}

The \textbf{cut()} function is more flexible than \textbf{ifelse()}.

\begin{verbatim}
Syntax
cut(num_vector,              # Numeric input vector
    breaks,                  # Number or vector of breaks
    labels = NULL,           # Labels for each group
    include.lowest = FALSE,  # Whether to include the lowest 'break' or not
    right = TRUE,            # Whether the right interval is closed (and the left open) or vice versa
    dig.lab = 3,             # Number of digits of the groups if labels = NULL
    ordered_result = FALSE,  # Whether to order the factor result or not
    …)                       # Additional arguments
\end{verbatim}

We still use the above example to discretize the age variable using \textbf{cut()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{cutAge01 }\OtherTok{=} \FunctionTok{cut}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{AGE, }\AttributeTok{breaks =}\FunctionTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{150}\NormalTok{), }\AttributeTok{labels=}\FunctionTok{c}\NormalTok{( }\StringTok{"[16, 30)"}\NormalTok{, }\StringTok{"[30, 50)"}\NormalTok{, }\StringTok{"(50, 150)"}\NormalTok{), }\AttributeTok{include.lowest =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(dat[, }\FunctionTok{c}\NormalTok{(}\StringTok{"AGE"}\NormalTok{, }\StringTok{"groupAge"}\NormalTok{, }\StringTok{"groupAge02"}\NormalTok{, }\StringTok{"cutAge01"}\NormalTok{)]))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l|l|l}
\hline
AGE & groupAge & groupAge02 & cutAge01\\
\hline
26 & [16, 50] & [16, 30) & [16, 30)\\
\hline
26 & [16, 50] & [16, 30) & [16, 30)\\
\hline
48 & [16, 50] & [30, 50) & [30, 50)\\
\hline
53 & (50, 150) & (50, 150) & (50, 150)\\
\hline
16 & [16, 50] & [16, 30) & [16, 30)\\
\hline
16 & [16, 50] & [16, 30) & [16, 30)\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{cutAge02 }\OtherTok{=} \FunctionTok{cut}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{AGE, }\AttributeTok{breaks =}\FunctionTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{150}\NormalTok{), }\AttributeTok{include.lowest =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(dat[, }\FunctionTok{c}\NormalTok{(}\StringTok{"AGE"}\NormalTok{, }\StringTok{"groupAge"}\NormalTok{, }\StringTok{"groupAge02"}\NormalTok{, }\StringTok{"cutAge01"}\NormalTok{, }\StringTok{"cutAge02"}\NormalTok{)]))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l|l|l|l}
\hline
AGE & groupAge & groupAge02 & cutAge01 & cutAge02\\
\hline
26 & [16, 50] & [16, 30) & [16, 30) & [16,30]\\
\hline
26 & [16, 50] & [16, 30) & [16, 30) & [16,30]\\
\hline
48 & [16, 50] & [30, 50) & [30, 50) & (30,50]\\
\hline
53 & (50, 150) & (50, 150) & (50, 150) & (50,150]\\
\hline
16 & [16, 50] & [16, 30) & [16, 30) & [16,30]\\
\hline
16 & [16, 50] & [16, 30) & [16, 30) & [16,30]\\
\hline
\end{tabular}

\hypertarget{with-and-within-functions}{%
\section{\texorpdfstring{\textbf{with()} and \textbf{within()} Functions}{with() and within() Functions}}\label{with-and-within-functions}}

\textbf{with()} and \textbf{within()} are two closely related yet different base R functions that are useful in data management.

\hypertarget{the-with-function}{%
\subsection{\texorpdfstring{The \textbf{with()} Function}{The with() Function}}\label{the-with-function}}

\texttt{with()} function enables us to define a new variable based on the variables in a \textbf{data frame} using basic \textbf{R expressions} that include mathematical and logical operations. We can add the newly defined variables to the existing data frame as usual.

\textbf{with()} Syntax

\begin{verbatim}
with(data-frame, R-expression)
\end{verbatim}

\textbf{Example 1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Num }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1400}\NormalTok{,}\DecValTok{1200}\NormalTok{,}\DecValTok{1100}\NormalTok{,}\DecValTok{1700}\NormalTok{,}\DecValTok{1500}\NormalTok{)}
\NormalTok{Cost }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1200}\NormalTok{,}\DecValTok{1300}\NormalTok{,}\DecValTok{1400}\NormalTok{,}\DecValTok{1500}\NormalTok{,}\DecValTok{1600}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{dataA }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Num,Cost,}\AttributeTok{stringsAsFactors =} \ConstantTok{FALSE}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{product }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(dataA, Num}\SpecialCharTok{*}\NormalTok{Cost)}
\NormalTok{quotient }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(dataA, Cost}\SpecialCharTok{/}\NormalTok{Num)}
\NormalTok{logical }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(dataA, Num }\SpecialCharTok{\textgreater{}}\NormalTok{ Cost)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{product =}\NormalTok{ product, }\AttributeTok{quotient =}\NormalTok{ quotient, }\AttributeTok{logical =}\NormalTok{ logical))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r}
\hline
product & quotient & logical\\
\hline
1680000 & 0.8571429 & 1\\
\hline
1560000 & 1.0833333 & 0\\
\hline
1540000 & 1.2727273 & 0\\
\hline
2550000 & 0.8823529 & 1\\
\hline
2400000 & 1.0666667 & 0\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# add the new variables to data frame dataA}
\NormalTok{dataA}\SpecialCharTok{$}\NormalTok{product }\OtherTok{=}\NormalTok{ product}
\NormalTok{dataA}\SpecialCharTok{$}\NormalTok{quotient }\OtherTok{=}\NormalTok{ quotient}
\NormalTok{dataA}\SpecialCharTok{$}\NormalTok{logical }\OtherTok{=}\NormalTok{ logical}
\DocumentationTok{\#\#}
\FunctionTok{kable}\NormalTok{(dataA)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|l}
\hline
Num & Cost & product & quotient & logical\\
\hline
1400 & 1200 & 1680000 & 0.8571429 & TRUE\\
\hline
1200 & 1300 & 1560000 & 1.0833333 & FALSE\\
\hline
1100 & 1400 & 1540000 & 1.2727273 & FALSE\\
\hline
1700 & 1500 & 2550000 & 0.8823529 & TRUE\\
\hline
1500 & 1600 & 2400000 & 1.0666667 & FALSE\\
\hline
\end{tabular}

\hfill\break
\#\#\# The \textbf{within()} Function

\textbf{within()} function allows us to create a copy of the data frame and add a column that would eventually store the result of the R expression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Num }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1400}\NormalTok{,}\DecValTok{1200}\NormalTok{,}\DecValTok{1100}\NormalTok{,}\DecValTok{1700}\NormalTok{,}\DecValTok{1500}\NormalTok{)}
\NormalTok{Cost }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1200}\NormalTok{,}\DecValTok{1300}\NormalTok{,}\DecValTok{1400}\NormalTok{,}\DecValTok{1500}\NormalTok{,}\DecValTok{1600}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{dataA }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Num,Cost,}\AttributeTok{stringsAsFactors =} \ConstantTok{FALSE}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{dataB }\OtherTok{\textless{}{-}} \FunctionTok{within}\NormalTok{(dataA, Product }\OtherTok{\textless{}{-}}\NormalTok{ Num}\SpecialCharTok{*}\NormalTok{Cost)   }\CommentTok{\# defined Product and added to dataA simultaneously}
\NormalTok{dataC }\OtherTok{\textless{}{-}} \FunctionTok{within}\NormalTok{(dataB, Quotient }\OtherTok{\textless{}{-}}\NormalTok{ Cost}\SpecialCharTok{/}\NormalTok{Num)}
\NormalTok{dataD }\OtherTok{\textless{}{-}} \FunctionTok{within}\NormalTok{(dataC, Logical }\OtherTok{\textless{}{-}}\NormalTok{ Num }\SpecialCharTok{\textgreater{}}\NormalTok{ Cost)}
\FunctionTok{kable}\NormalTok{(dataD)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|l}
\hline
Num & Cost & Product & Quotient & Logical\\
\hline
1400 & 1200 & 1680000 & 0.8571429 & TRUE\\
\hline
1200 & 1300 & 1560000 & 1.0833333 & FALSE\\
\hline
1100 & 1400 & 1540000 & 1.2727273 & FALSE\\
\hline
1700 & 1500 & 2550000 & 0.8823529 & TRUE\\
\hline
1500 & 1600 & 2400000 & 1.0666667 & FALSE\\
\hline
\end{tabular}

\hypertarget{the-merge-function---table-joins}{%
\section{\texorpdfstring{The \textbf{merge()} Function - Table Joins}{The merge() Function - Table Joins}}\label{the-merge-function---table-joins}}

The R \textbf{merge()} function allows merging two data frames by \textbf{row names} (common key). This function allows us to perform different database (SQL) joins, like left join, inner join, right join, or full join, among others. In this note, we only introduce four different ways of merging datasets in base R with examples. We will introduce the SQL clause in R later.

\hfill\break

\hypertarget{inner-join}{%
\subsection{Inner Join}\label{inner-join}}

The following figure illustrates how A \textbf{left joins} B and the resulting merged data set.

\begin{center}\includegraphics[width=0.7\linewidth]{img05/w5-innerJoin} \end{center}

The following code implements the above left-join.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Date =} \FunctionTok{c}\NormalTok{(}\StringTok{"1/1/2020"}\NormalTok{, }\StringTok{"1/2/2020"}\NormalTok{, }\StringTok{"1/3/2020"}\NormalTok{, }\StringTok{"1/4/2020"}\NormalTok{),}
               \AttributeTok{CountryID =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{),}
               \AttributeTok{Units =} \FunctionTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{35}\NormalTok{))}
\NormalTok{B }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ID=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{),}
               \AttributeTok{Country=}\FunctionTok{c}\NormalTok{( }\StringTok{"Panama"}\NormalTok{, }\StringTok{"Spain"}\NormalTok{))}
\NormalTok{AinnerB }\OtherTok{=} \FunctionTok{merge}\NormalTok{(A, B, }\AttributeTok{by.x =} \StringTok{"CountryID"}\NormalTok{, }\AttributeTok{by.y =} \StringTok{"ID"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(AinnerB)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l|r|l}
\hline
CountryID & Date & Units & Country\\
\hline
3 & 1/3/2020 & 30 & Panama\\
\hline
\end{tabular}

\hfill\break

\hypertarget{left-join}{%
\subsection{Left Join}\label{left-join}}

The following figure illustrates how A \textbf{left joins} B and the resulting merged data set.

\begin{center}\includegraphics[width=0.7\linewidth]{img05/w5-leftJoin} \end{center}

The following code implements the above left-join.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Date =} \FunctionTok{c}\NormalTok{(}\StringTok{"1/1/2020"}\NormalTok{, }\StringTok{"1/2/2020"}\NormalTok{, }\StringTok{"1/3/2020"}\NormalTok{, }\StringTok{"1/4/2020"}\NormalTok{),}
               \AttributeTok{CountryID =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{),}
               \AttributeTok{Units =} \FunctionTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{35}\NormalTok{))}
\NormalTok{B }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ID=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{),}
               \AttributeTok{Country=}\FunctionTok{c}\NormalTok{(}\StringTok{"USA"}\NormalTok{, }\StringTok{"Canada"}\NormalTok{, }\StringTok{"Panama"}\NormalTok{))}
\NormalTok{AleftB }\OtherTok{=} \FunctionTok{merge}\NormalTok{(A, B, }\AttributeTok{by.x =} \StringTok{"CountryID"}\NormalTok{, }\AttributeTok{by.y =} \StringTok{"ID"}\NormalTok{, }\AttributeTok{all.x =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(AleftB)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l|r|l}
\hline
CountryID & Date & Units & Country\\
\hline
1 & 1/1/2020 & 40 & USA\\
\hline
1 & 1/2/2020 & 25 & USA\\
\hline
3 & 1/3/2020 & 30 & Panama\\
\hline
4 & 1/4/2020 & 35 & NA\\
\hline
\end{tabular}

Note that, left-join produces missing values of the record in A and does not have any information in B.

\hfill\break

\hypertarget{right-join}{%
\subsection{Right Join}\label{right-join}}

The following figure illustrates how A \textbf{right joins} B and the resulting merged data set.

\begin{center}\includegraphics[width=0.7\linewidth]{img05/w5-rightJoin} \end{center}

The following code implements the above left-join.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Date =} \FunctionTok{c}\NormalTok{(}\StringTok{"1/1/2020"}\NormalTok{, }\StringTok{"1/2/2020"}\NormalTok{, }\StringTok{"1/3/2020"}\NormalTok{, }\StringTok{"1/4/2020"}\NormalTok{),}
               \AttributeTok{CountryID =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{),}
               \AttributeTok{Units =} \FunctionTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{35}\NormalTok{))}
\NormalTok{B }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ID=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{),}
               \AttributeTok{Country=}\FunctionTok{c}\NormalTok{(}\StringTok{"Panama"}\NormalTok{))}
\NormalTok{ArightB }\OtherTok{=} \FunctionTok{merge}\NormalTok{(A, B, }\AttributeTok{by.x =} \StringTok{"CountryID"}\NormalTok{, }\AttributeTok{by.y =} \StringTok{"ID"}\NormalTok{, }\AttributeTok{all.y =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(ArightB)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l|r|l}
\hline
CountryID & Date & Units & Country\\
\hline
3 & 1/3/2020 & 30 & Panama\\
\hline
\end{tabular}

Note also that right-join could also produce missing values.

\hfill\break

\hypertarget{full-outer-join}{%
\subsection{Full (outer) Join}\label{full-outer-join}}

The following figure illustrates how A \textbf{Full outer joins} B and the resulting merged data set.

\begin{center}\includegraphics[width=0.7\linewidth]{img05/w5-fullOuterJoin} \end{center}

The following code implements the above left-join.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Date =} \FunctionTok{c}\NormalTok{(}\StringTok{"1/1/2020"}\NormalTok{, }\StringTok{"1/2/2020"}\NormalTok{, }\StringTok{"1/3/2020"}\NormalTok{, }\StringTok{"1/4/2020"}\NormalTok{),}
               \AttributeTok{CountryID =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{),}
               \AttributeTok{Units =} \FunctionTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{35}\NormalTok{))}
\NormalTok{B }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ID=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{),}
               \AttributeTok{Country=}\FunctionTok{c}\NormalTok{(}\StringTok{"USA"}\NormalTok{, }\StringTok{"Canada"}\NormalTok{, }\StringTok{"Panama"}\NormalTok{, }\StringTok{"Spain"}\NormalTok{))}
\NormalTok{AfullB }\OtherTok{=} \FunctionTok{merge}\NormalTok{(A, B, }\AttributeTok{by.x =} \StringTok{"CountryID"}\NormalTok{, }\AttributeTok{by.y =} \StringTok{"ID"}\NormalTok{, }\AttributeTok{all =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(AfullB)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l|r|l}
\hline
CountryID & Date & Units & Country\\
\hline
1 & 1/1/2020 & 40 & USA\\
\hline
1 & 1/2/2020 & 25 & USA\\
\hline
2 & 1/4/2020 & 35 & Canada\\
\hline
3 & 1/3/2020 & 30 & Panama\\
\hline
4 & NA & NA & Spain\\
\hline
\end{tabular}

\hfill\break

\hypertarget{subsetting-data-frame}{%
\section{Subsetting Data Frame}\label{subsetting-data-frame}}

There are two different ways for subsetting a data frame: subsetting by rows and by columns.

We first define the following working data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{working.data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{id =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{14}\NormalTok{,}\DecValTok{15}\NormalTok{,}\DecValTok{16}\NormalTok{,}\DecValTok{17}\NormalTok{),}
  \AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}sai\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}ram\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}deepika\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}sahithi\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}kumar\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}scott\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Don\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Lin\textquotesingle{}}\NormalTok{),}
  \AttributeTok{gender =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{,}\ConstantTok{NA}\NormalTok{,}\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{),}
  \AttributeTok{dob =} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}1990{-}10{-}02\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}1981{-}3{-}24\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}1987{-}6{-}14\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}1985{-}8{-}16\textquotesingle{}}\NormalTok{,}
                  \StringTok{\textquotesingle{}1995{-}03{-}02\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}1991{-}6{-}21\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}1986{-}3{-}24\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}1990{-}8{-}26\textquotesingle{}}\NormalTok{)),}
  \AttributeTok{state =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}CA\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}NY\textquotesingle{}}\NormalTok{,}\ConstantTok{NA}\NormalTok{,}\ConstantTok{NA}\NormalTok{,}\StringTok{\textquotesingle{}DC\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}DW\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}AZ\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}PH\textquotesingle{}}\NormalTok{),}
  \AttributeTok{row.names=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}r1\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}r2\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}r3\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}r4\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}r5\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}r6\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}r7\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}r8\textquotesingle{}}\NormalTok{)}
\NormalTok{)}
\FunctionTok{kable}\NormalTok{(working.data)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|l|l|l|l}
\hline
  & id & name & gender & dob & state\\
\hline
r1 & 10 & sai & M & 1990-10-02 & CA\\
\hline
r2 & 11 & ram & M & 1981-03-24 & NY\\
\hline
r3 & 12 & deepika & NA & 1987-06-14 & NA\\
\hline
r4 & 13 & sahithi & F & 1985-08-16 & NA\\
\hline
r5 & 14 & kumar & M & 1995-03-02 & DC\\
\hline
r6 & 15 & scott & M & 1991-06-21 & DW\\
\hline
r7 & 16 & Don & M & 1986-03-24 & AZ\\
\hline
r8 & 17 & Lin & F & 1990-08-26 & PH\\
\hline
\end{tabular}

\hypertarget{subsetting-by-columns}{%
\subsection{Subsetting by Columns}\label{subsetting-by-columns}}

This is a relatively easy job - we can simply select or drop variables to make a subset. The following is just an example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# only keep id, name, dob}
\NormalTok{subset01 }\OtherTok{=}\NormalTok{ working.data[, }\FunctionTok{c}\NormalTok{(}\StringTok{"id"}\NormalTok{, }\StringTok{"name"}\NormalTok{, }\StringTok{"dob"}\NormalTok{)]}
\NormalTok{subset01}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    id    name        dob
## r1 10     sai 1990-10-02
## r2 11     ram 1981-03-24
## r3 12 deepika 1987-06-14
## r4 13 sahithi 1985-08-16
## r5 14   kumar 1995-03-02
## r6 15   scott 1991-06-21
## r7 16     Don 1986-03-24
## r8 17     Lin 1990-08-26
\end{verbatim}

We could also create the above subset by dropping \texttt{gender} and \texttt{state}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# drop gender, state}
\NormalTok{subset02 }\OtherTok{=}\NormalTok{ working.data[, }\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{)]}
\NormalTok{subset02}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    id    name        dob
## r1 10     sai 1990-10-02
## r2 11     ram 1981-03-24
## r3 12 deepika 1987-06-14
## r4 13 sahithi 1985-08-16
## r5 14   kumar 1995-03-02
## r6 15   scott 1991-06-21
## r7 16     Don 1986-03-24
## r8 17     Lin 1990-08-26
\end{verbatim}

\hypertarget{subsetting-by-rows}{%
\subsection{Subsetting by Rows}\label{subsetting-by-rows}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subset by row name}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{subset}\NormalTok{(working.data, }\AttributeTok{subset=}\FunctionTok{rownames}\NormalTok{(df) }\SpecialCharTok{==} \StringTok{\textquotesingle{}r1\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l|l|l|l}
\hline
id & name & gender & dob & state\\


\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subset row by the vector of row names}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{subset}\NormalTok{(working.data, }\FunctionTok{rownames}\NormalTok{(df) }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}r1\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}r2\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}r3\textquotesingle{}}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l|l|l|l}
\hline
id & name & gender & dob & state\\


\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subset by condition}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{subset}\NormalTok{(working.data, gender }\SpecialCharTok{==} \StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|l|l|l|l}
\hline
  & id & name & gender & dob & state\\
\hline
r1 & 10 & sai & M & 1990-10-02 & CA\\
\hline
r2 & 11 & ram & M & 1981-03-24 & NY\\
\hline
r5 & 14 & kumar & M & 1995-03-02 & DC\\
\hline
r6 & 15 & scott & M & 1991-06-21 & DW\\
\hline
r7 & 16 & Don & M & 1986-03-24 & AZ\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subset by condition with \%in\%}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{subset}\NormalTok{(working.data, state }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}CA\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}DC\textquotesingle{}}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|l|l|l|l}
\hline
  & id & name & gender & dob & state\\
\hline
r1 & 10 & sai & M & 1990-10-02 & CA\\
\hline
r5 & 14 & kumar & M & 1995-03-02 & DC\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subset by multiple conditions using |}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{subset}\NormalTok{(working.data, gender }\SpecialCharTok{==} \StringTok{\textquotesingle{}M\textquotesingle{}} \SpecialCharTok{|}\NormalTok{ state }\SpecialCharTok{==} \StringTok{\textquotesingle{}PH\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|l|l|l|l}
\hline
  & id & name & gender & dob & state\\
\hline
r1 & 10 & sai & M & 1990-10-02 & CA\\
\hline
r2 & 11 & ram & M & 1981-03-24 & NY\\
\hline
r5 & 14 & kumar & M & 1995-03-02 & DC\\
\hline
r6 & 15 & scott & M & 1991-06-21 & DW\\
\hline
r7 & 16 & Don & M & 1986-03-24 & AZ\\
\hline
r8 & 17 & Lin & F & 1990-08-26 & PH\\
\hline
\end{tabular}

\hypertarget{bootstrap-confidence-intervals}{%
\chapter{Bootstrap Confidence Intervals}\label{bootstrap-confidence-intervals}}

The bootstrap method is a data-based simulation method for statistical inference. The method assumes that

\begin{itemize}
\tightlist
\item
  The sample is a random sample that represents the population.
\item
  The sample size is large enough such that the empirical distribution is close to the true distribution.
\end{itemize}

\hypertarget{basic-idea-of-bootstrap-method.}{%
\section{Basic Idea of Bootstrap Method.}\label{basic-idea-of-bootstrap-method.}}

The objective is to estimate a population parameter such as mean, variance, correlation coefficient, regression coefficients, etc. from a random sample without assuming any probability distribution of the underlying distribution of the population.

For convenience, we assume that the population of interest has a cumulative distribution function \(F(x: \theta)\), where \(\theta\) is a vector of the population. For example, You can think about the following distributions

\begin{itemize}
\tightlist
\item
  \textbf{Normal distribution}: \(N(\mu, \sigma^2)\), the distribution function is given by
\end{itemize}

\[
f(x:\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\]

where \(\theta = (\mu, \sigma)\). Since the normal distribution is so fundamental in statistics, we use the special notation for the cumulative distribution \(\phi_{\mu, \sigma^2}(x)\) or simply \(\phi(x)\). The corresponding probability function

\begin{itemize}
\tightlist
\item
  \textbf{Binomial distribution}: \(Binom(n, p)\), the probability distribution is given by
\end{itemize}

\[ 
P(x) = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}, x = 0, 1, 2, \cdots, n-1, n.
\]
where \(\theta = p\). \emph{Caution}: \(n\) is NOT a parameter!

We have already learned how to make inferences about population means and variances under various assumptions in elementary statistics. In this note, we introduce a \textbf{new approach} to making inferences only based on a given random sample taken from the underlying population.

As an example, we focus on the population mean. For other parameters, we can follow the same idea to make bootstrap inferences.

\hypertarget{random-sample-from-population}{%
\subsection{Random Sample from Population}\label{random-sample-from-population}}

We have introduced various study designs and sampling plans to obtain random samples from a given population with the distribution function \(F(x:\theta)\). Let \(\mu\) be the population means.

\begin{itemize}
\item
  \textbf{Random Sample}. Let
  \[\{x_1, x_2, \cdots, x_n\} \to F(x:\theta)\]
  be a random sample from population \(F(x:\theta)\).
\item
  \textbf{Sample Mean}. The point estimate is given by
\end{itemize}

\[\hat{\mu} = \frac{\sum_{i=1}^n x_i}{n}\]

\begin{itemize}
\item
  \textbf{Sampling Distribution of \(\hat{\mu}\)}. In order to construct the confidence interval of \(\mu\) or make hypothesis testing about \(\mu\), we need to know the sampling distribution of \(\hat{\mu}\). From elementary statistics, we have the following results.

  \begin{itemize}
  \item
    \(\hat{\mu}\) is normally distributed if (1). \(n\) is large; or (2). the population is normal and population variance is known.
  \item
    the standardized \(\hat{\mu}\) follows a t-distribution if the population is normal and population variance is unknown.
  \item
    \(\hat{\mu}\) is \textbf{unknown} of the population is not normal and the sample size is not large enough.
  \end{itemize}
\item
  In the last case of the previous bullet point, we don't have the theory to derive the sampling distribution based on a \textbf{single} sample. However, if the sampling is not too expensive and time-consuming, we take following the sample study design and sampling plan to repeatedly take a large number, 1000, samples of the same size from the population. We calculate the mean of each of the 1000 samples and obtain 1000 sample means \(\{\hat{\mu}_1, \hat{\mu}_2, \cdots, \hat{\mu}_{1000}\}\). Then the empirical distribution of \(\hat{\mu}\).
\end{itemize}

The following figure depicts how to approximate the sampling distribution of the point estimator of the population parameter.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{img02/w02-ApproxSamplingDist} 

}

\caption{Steps for estimating the sampling distribution of a point estimator of the population parameter}\label{fig:unnamed-chunk-52}
\end{figure}

\textbf{Example 1:} {[}\textbf{Simulated data}{]} Assume that the particular numeric characteristics of the WCU student population are the heights of all students.

\begin{itemize}
\item
  We don't know the distribution of the heights.
\item
  We also don't know \emph{whether a specific sample size is large enough to use the central limit theorem}. This means we don't know whether it is appropriate to use the central limit theorem to characterize the sampling distribution of the mean height.
\end{itemize}

Due to the above constraints, we cannot find the sampling distribution of the sample means using only the knowledge of elementary statistics. However, if sampling is not expensive, we take repeated samples with the same sample size. The resulting sample means can be used to approximate the sampling distribution of the sample mean.

Next, we use R and the simulated data set \url{https://raw.githubusercontent.com/pengdsci/sta321/main/ww02/w02-wcuheights.txt} to implement the above idea. I will use simple code with comments to explain the task of each line of code so you can easily understand the coding logic.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\CommentTok{\# read the delimited data from URL}
\NormalTok{heightURL }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww02/w02{-}wcuheights.txt"}
\NormalTok{wcu.height }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(heightURL, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# define an empty vector to hold sample means of repeated samples.}
\NormalTok{sample.mean.vec }\OtherTok{=} \ConstantTok{NULL}      
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{)\{   }\CommentTok{\# taking repeated random samples with n = 81}
\NormalTok{  ith.sample }\OtherTok{=} \FunctionTok{sample}\NormalTok{( wcu.height}\SpecialCharTok{$}\NormalTok{Height,       }\CommentTok{\# population s}
                       \DecValTok{81}\NormalTok{,                      }\CommentTok{\# boot sample size = 81 }
                       \AttributeTok{replace =} \ConstantTok{FALSE}          \CommentTok{\# without replacement}
\NormalTok{                 )      }
\CommentTok{\# calculate the mean of i{-}th sample and save it into sample.mean.vec}
\NormalTok{   sample.mean.vec[i] }\OtherTok{=} \FunctionTok{mean}\NormalTok{(ith.sample)        }
\NormalTok{ \}}
\end{Highlighting}
\end{Shaded}

Next, we make a histogram of the sample means saved in \textbf{sample.mean.vec}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(sample.mean.vec,   }\CommentTok{\# data used for histogram}
     \AttributeTok{breaks =} \DecValTok{14}\NormalTok{,       }\CommentTok{\# specify number of vertical bars}
     \AttributeTok{xlab =} \StringTok{"sample means of repeated samples"}\NormalTok{, }\CommentTok{\# change the label of x{-}axis}
     \CommentTok{\# add multi{-}line title to the histogram}
     \AttributeTok{main=}\StringTok{"Approximated Sampling Distribution  }\SpecialCharTok{\textbackslash{}n}\StringTok{  of Sample Means"}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-54-1} 

}

\caption{Approximated sampling distribution of sample mean used the repeated samples.}\label{fig:unnamed-chunk-54}
\end{figure}

\hypertarget{bootstrap-sampling-and-bootstrap-sampling-distribution}{%
\subsection{Bootstrap Sampling and Bootstrap Sampling Distribution}\label{bootstrap-sampling-and-bootstrap-sampling-distribution}}

Recall the situation in \textbf{Example 1} in which we were not able to use the normality assumption of the population and the central limit theorem (CLT) but were allowed to take repeated samples from the population. In practice, taking samples from the population can be very expensive. Is there any way to estimate the sampling distribution of the \textbf{sample means}? The answer is YES under the assumption the sample yields a valid estimation of the original population distribution.

\begin{itemize}
\item
  \textbf{Bootstrap Sampling} with the assumption that the sample yields a good approximation of the population distribution, we can take bootstrap samples from the \textbf{actual} sample. Let
  \[\{x_1, x_2, \cdots, x_n\} \to F(x:\theta)\] be the actual random sample taken from the population. A \textbf{bootstrap sample} is obtained by taking a sample \textbf{with replacement} from the original data set (not the population!) with the same size as the original sample. Because \textbf{with replacement} was used, some values in the bootstrap sample appear once, some twice, and so on, and some do not appear at all.
\item
  \textbf{Notation of Bootstrap Sample}. We use \(\{x_1^{(i*)}, x_2^{(i*)}, \cdots, x_n^{(i*)}\}\) to denote the \(i^{th}\) bootstrap sample. Then the corresponding mean is called bootstrap sample mean and denoted by \(\hat{\mu}_i^*\), for \(i = 1, 2, ..., n\).
\item
  \textbf{Bootstrap sampling distribution} of the sample mean can be estimated by taking a large number, say B, of bootstrap samples. The resulting B bootstrap sample means are used to estimate the sampling distribution. Note that, in practice, B is bigger than 1000.
\end{itemize}

The above Bootstrap sampling process is illustrated in the following figure.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{img02/w02-BootSamplingDist} 

}

\caption{Steps for the Bootstrap sampling distribution of a point estimator of the population parameter}\label{fig:unnamed-chunk-55}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Example 2:} {[}\textbf{continue to use WCU Heights}{]}. We use the Bootstrap method to estimate the sampling distribution of the sample means.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\# read the delimited data from URL}
\NormalTok{heightURL }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww02/w02{-}wcuheights.txt"}
\NormalTok{wcu.height }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(heightURL, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# taking the original random sample from the population}
\NormalTok{original.sample }\OtherTok{=} \FunctionTok{sample}\NormalTok{( wcu.height}\SpecialCharTok{$}\NormalTok{Height,    }\CommentTok{\# population}
                       \DecValTok{81}\NormalTok{,                      }\CommentTok{\# boot sample size = 81 }
                       \AttributeTok{replace =} \ConstantTok{FALSE}          \CommentTok{\# without replacement}
\NormalTok{                 )                              }
\DocumentationTok{\#\#\# Bootstrap sampling begins}
\NormalTok{bt.sample.mean.vec }\OtherTok{=} \ConstantTok{NULL}      \CommentTok{\# empty vector to store boot sample means.}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{)\{               }
\NormalTok{  ith.bt.sample }\OtherTok{=} \FunctionTok{sample}\NormalTok{( original.sample,    }\CommentTok{\# Original sample }
                       \DecValTok{81}\NormalTok{,                    }\CommentTok{\# boot sample size = 81!!}
                       \AttributeTok{replace =} \ConstantTok{TRUE}         \CommentTok{\# MUST use WITH REPLACEMENT!!}
\NormalTok{                 )                           }
\NormalTok{  bt.sample.mean.vec[i] }\OtherTok{=} \FunctionTok{mean}\NormalTok{(ith.bt.sample) }
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

The following histogram shows the bootstrap sampling distribution of sample means with size \(n=81\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(bt.sample.mean.vec,  }\CommentTok{\# data used for histogram}
     \AttributeTok{breaks =} \DecValTok{14}\NormalTok{,         }\CommentTok{\# specify number of vertical bars}
     \AttributeTok{xlab =} \StringTok{"Bootstrap sample means"}\NormalTok{,  }\CommentTok{\# change the label of x{-}axis}
     \AttributeTok{main=}\StringTok{"Bootstrap Sampling Distribution }\SpecialCharTok{\textbackslash{}n}\StringTok{ of Sample Means"}\NormalTok{)   }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-57-1} 

}

\caption{Bootstrap sampling distribution of sample means}\label{fig:unnamed-chunk-57}
\end{figure}

\hypertarget{relationship-between-two-estimated-sampling-distributions}{%
\subsection{Relationship between Two Estimated Sampling Distributions}\label{relationship-between-two-estimated-sampling-distributions}}

We can see that the two sampling distributions are slightly different. If we are allowed to take repeated samples from the population, we should always use the repeated sample approach since it yields a better estimate of the true sampling distribution.

The bootstrap estimate of the sampling distribution is used when no theoretical confidence intervals are available and the repeated sample is not possible due to certain constraints. This does not mean that the bootstrap methods do not have limitations. In fact, the implicit assumption of the bootstrap method is that \textbf{the original sample has enough information to estimate the true population distribution}.

\hypertarget{bootstrap-confidence-intervals-1}{%
\section{Bootstrap Confidence Intervals}\label{bootstrap-confidence-intervals-1}}

First of all, all bootstrap confidence intervals are constructed based on the bootstrap sampling distribution of the underlying point estimator of the parameter of interest.

There are at least five different bootstrap confidence intervals. You can find these definitions from Chapter 4 of Roff's eBook \url{https://ebookcentral.proquest.com/lib/wcupa/reader.action?docID=261114\&ppg=7} (need WCU login credential to access the book). We only focus on the percentile method in which we simply define the confidence limit(s) by using the corresponding percentile(s) of the bootstrap estimates of the parameter of interest. R has a built-in function, \textbf{quantile()}, to find percentiles.

\begin{itemize}
\tightlist
\item
  \textbf{Example 3}: We construct a 95\% two-sided bootstrap percentile confidence interval of the mean height of WCU students. This is equivalent to finding the 2.5\% and 97.5\% percentiles. We use the following one-line code.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CI }\OtherTok{=} \FunctionTok{quantile}\NormalTok{(bt.sample.mean.vec, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{))}
\NormalTok{CI}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     2.5%    97.5% 
## 69.09877 71.01265
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#kable(CI, caption = "95\% bootstrap percentile confidence interval of the mean height")}
\end{Highlighting}
\end{Shaded}

Various bootstrap methods were implemented in the R library \textbf{\{boot\}}. UCLA Statistical Consulting \url{https://stats.idre.ucla.edu/r/faq/how-can-i-generate-bootstrap-statistics-in-r/} has a nice tutorial on bootstrap confidence intervals. You can use the built-in function \textbf{boot.ci()} to find all 5 bootstrap confidence intervals after you create the boot object. I will leave it to you if you want to explore more about the library.

\hypertarget{bootstrap-confidence-interval-of-correlation-coefficient}{%
\section{Bootstrap Confidence Interval of Correlation Coefficient}\label{bootstrap-confidence-interval-of-correlation-coefficient}}

As a case study, we will illustrate one bootstrap method to sample a random sample with multiple variables and use the bootstrap samples to calculate the corresponding bootstrap correlation coefficient. The bootstrap percentile confidence interval of the correlation coefficient.

\hypertarget{bootstrapping-data-set}{%
\subsection{Bootstrapping Data Set}\label{bootstrapping-data-set}}

There are different ways to take bootstrap samples. \textbf{The key point is that we cannot sample individual variables in the data frame separately to avoid mismatching!} The method we introduce here is also called bootstrap sampling cases. Here are the basic steps:

\begin{itemize}
\item
  Assume the data frame haven rows. We define the vector of row \(ID\). That is, \(ID = \{1, 2, 3, ..., n\}\).
\item
  Take a bootstrap sample from \(ID\) (i.e., sampling with replacement) with same size = n, denoted by \(ID^*\). As commented earlier, there will be replicates of \(ID^*\) and some values in \(ID\) are not in \(ID^*\).
\item
  Use \(ID^*\) to select the corresponding rows to form a bootstrap sample and then perform bootstrap analysis.
\end{itemize}

Here is an example of taking the bootstrap sample from the original sample with multiple variables. The data set we use here is well-known and is available at \url{https://raw.githubusercontent.com/pengdsci/sta321/main/ww02/w02-iris.txt}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read data into R from the URL}
\NormalTok{irisURL }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww02/w02{-}iris.txt"}
\NormalTok{iris }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(irisURL, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{n }\OtherTok{=} \FunctionTok{dim}\NormalTok{(iris)[}\DecValTok{1}\NormalTok{]   }\CommentTok{\# returns the dimension of the data frame: [row, col]           }
\NormalTok{bt.ID }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)   }\CommentTok{\# bootstrap IDs, MUST use replacement method!}
\FunctionTok{sort}\NormalTok{(bt.ID)      }\CommentTok{\# sort bt.ID. to see replicated ID}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1]   1   1   1   2   2   2   4   8   9   9  12  12  12  14  14  15  18  19  19
##  [20]  19  21  21  22  23  23  24  24  27  28  28  29  29  30  30  31  32  34  37
##  [39]  37  37  37  38  40  41  42  42  45  46  46  48  50  50  50  51  51  52  52
##  [58]  53  55  55  55  57  59  59  60  60  60  61  63  63  64  66  66  67  68  73
##  [77]  74  74  77  77  78  78  79  79  79  82  83  84  85  86  89  91  91  92  92
##  [96]  95  95  95  96  96  96  96  97  97 100 101 102 103 103 103 105 105 105 106
## [115] 108 109 110 110 111 113 114 115 116 117 119 120 121 123 123 124 126 128 128
## [134] 129 130 131 132 133 134 135 136 138 139 140 141 145 145 146 147 148
\end{verbatim}

Next, we use the above bt.ID to take the bootstrap sample from the original data set \textbf{iris}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bt.iris }\OtherTok{=}\NormalTok{ iris[bt.ID,]   }\CommentTok{\# taking bootstrap cases (or rows, records) using the bt.ID}
\NormalTok{bt.iris                  }\CommentTok{\# display the bootstrap sample}
\end{Highlighting}
\end{Shaded}

\hypertarget{confidence-interval-of-coefficient-correlation}{%
\subsection{Confidence Interval of Coefficient Correlation}\label{confidence-interval-of-coefficient-correlation}}

In this section, we construct a 95\% bootstrap percentile confidence interval for the coefficient correlation between the SepalLength and SepalWidth given in \textbf{iris}. Note that R built-in function \textbf{cor(x,y)} can be used to calculate the bootstrap correlation coefficient directly. The R code for constructing a bootstrap confidence interval for the coefficient correlation is given below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{irisURL }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww02/w02{-}iris.txt"}
\NormalTok{iris }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(irisURL, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{n }\OtherTok{=} \FunctionTok{dim}\NormalTok{(iris)[}\DecValTok{1}\NormalTok{]    }\CommentTok{\# 1st component is the number of rows            }
\DocumentationTok{\#\#}
\NormalTok{bt.cor.vec }\OtherTok{=} \ConstantTok{NULL}    \CommentTok{\# empty vector for boot correlation coefficients}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5000}\NormalTok{)\{   }\CommentTok{\# taking 5000 bootstrap samples for this example.}
\NormalTok{  bt.ID.i }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\# MUST use replacement method!}
\NormalTok{  bt.iris.i }\OtherTok{=}\NormalTok{ iris[bt.ID.i, ]            }\CommentTok{\# i{-}th bootstrap ID}
  \CommentTok{\# i{-}th bootstrap correlation coefficient}
\NormalTok{  bt.cor.vec[i] }\OtherTok{=} \FunctionTok{cor}\NormalTok{(bt.iris.i}\SpecialCharTok{$}\NormalTok{SepalLength, bt.iris.i}\SpecialCharTok{$}\NormalTok{SepalWidth)  }
\NormalTok{\}}
\NormalTok{bt.CI }\OtherTok{=} \FunctionTok{quantile}\NormalTok{(bt.cor.vec, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{) )}
\NormalTok{bt.CI}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        2.5%       97.5% 
## -0.25208993  0.03809699
\end{verbatim}

\textbf{Interpretation:} we are 95\% confident that there is no statistically significant correlation between sepal length and sepal width based on the given sample. This may be because the data set contains three different types of iris.

Next, we make two plots to visualize the relationship between the two variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))   }\CommentTok{\# layout a plot sheet: 1 row and 2 columns}
\DocumentationTok{\#\# histogram}
\FunctionTok{hist}\NormalTok{(bt.cor.vec, }\AttributeTok{breaks =} \DecValTok{14}\NormalTok{, }
       \AttributeTok{main=}\StringTok{"Bootstrap Sampling }\SpecialCharTok{\textbackslash{}n}\StringTok{  Distribution of Correlation"}\NormalTok{,}
      \AttributeTok{xlab =} \StringTok{"Bootstrap Correlation Coefficient"}\NormalTok{)}
\DocumentationTok{\#\# scatter plot}
\FunctionTok{plot}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalLength, iris}\SpecialCharTok{$}\NormalTok{SepalWidth,}
     \AttributeTok{main =} \StringTok{"Sepal Length vs Width"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Sepal Length"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Sepal Width"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-62-1} 

}

\caption{Left panel: histogram of the bootstrap coefficient of correlation. Right panel: the scatter plot of the sepal length and width.}\label{fig:unnamed-chunk-62}
\end{figure}

\hypertarget{problem-set}{%
\section{Problem Set}\label{problem-set}}

\hypertarget{data-set-description-1}{%
\subsection{Data Set Description}\label{data-set-description-1}}

This data set includes the \textbf{percentage of protein intake} from different types of food in countries around the world. The last couple of columns also includes counts of obesity and COVID-19 cases as percentages of the total population for comparison purposes.

Data can be found at \href{https://www.kaggle.com/mariaren/covid19-healthy-diet-dataset?select=Protein_Supply_Quantity_Data.csv}{kaggle.com}. I also upload this data set on the course web page \href{https://raw.githubusercontent.com/pengdsci/sta321/main/ww02/w02-Protein_Supply_Quantity_Data.csv}{STA321 Course Web Page}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{proteinURL }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww02/w02{-}Protein\_Supply\_Quantity\_Data.csv"}
\NormalTok{protein }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(proteinURL, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{var.name }\OtherTok{=} \FunctionTok{names}\NormalTok{(protein)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(var.name))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l}
\hline
var.name\\
\hline
Country\\
\hline
AlcoholicBeverages\\
\hline
AnimalProducts\\
\hline
Animalfats\\
\hline
CerealsExcludingBeer\\
\hline
Eggs\\
\hline
FishSeafood\\
\hline
FruitsExcludingWine\\
\hline
Meat\\
\hline
MilkExcludingButter\\
\hline
Offals\\
\hline
Oilcrops\\
\hline
Pulses\\
\hline
Spices\\
\hline
StarchyRoots\\
\hline
Stimulants\\
\hline
Treenuts\\
\hline
VegetalProducts\\
\hline
VegetableOils\\
\hline
Vegetables\\
\hline
Miscellaneous\\
\hline
Obesity\\
\hline
Confirmed\\
\hline
Deaths\\
\hline
Recovered\\
\hline
Active\\
\hline
Population\\
\hline
\end{tabular}

\hypertarget{problems}{%
\subsection{Problems}\label{problems}}

Choose a numerical variable from the variable list and do the following problems.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construct a confidence interval of the mean of the variable using methods in elementary statistics.
\item
  Use the Bootstrap method to construct the bootstrap confidence interval for the mean of the selected variable.
\item
  Plot the bootstrap sampling distribution of the sample mean.
\item
  Compare the two confidence intervals and interpret your findings and interpret the confidnce intervals.
\end{enumerate}

\hypertarget{bootstrapping-slr}{%
\chapter{Bootstrapping SLR}\label{bootstrapping-slr}}

In this note, we introduce how to use the bootstrap method to make inferences about the regression coefficients. Parametric inference of the regression models heavily depends on the assumptions of the underlying model. If there are violations of the model assumptions the resulting p-values produced in the outputs of software programs could be valid. In this case, if the sample size is large enough, we can use the Bootstrap method to make the inferences without imposing the distributional assumption of the response variable (hence the residuals).

\hypertarget{data-set-and-practical-questions}{%
\section{Data Set and Practical Questions}\label{data-set-and-practical-questions}}

The data set we use in this note is taken from the well-known \href{https://www.kaggle.com/}{Kaggle}, a web-based online community of data scientists and machine learning practitioners, that allows users to find and publish data sets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges (-- Wikipedia).

\hypertarget{data-description}{%
\subsection{Data Description}\label{data-description}}

The data in this note was found from \href{https://www.kaggle.com/}{Kaggle}. I renamed the original variables and modified the sales dates to define the sales year indicator. The modified data set was uploaded to the course web page at \url{https://raw.githubusercontent.com/pengdsci/sta321/main/ww03/w03-Realestate.csv}.

\begin{itemize}
\tightlist
\item
  ObsID
\item
  TransactionYear(\(X_1\)): transaction date\\
\item
  HouseAge(\(X_2\)): house age\\
\item
  Distance2MRT(\(X_3\)): distance to the nearest MRT station\\
\item
  NumConvenStores(\(X_4\)): number of convenience stores\\
\item
  Latitude(\(X_5\)): latitude
\item
  Longitude(\(X_6\)): longitude\\
\item
  PriceUnitArea(\(Y\)): house price of unit area
\end{itemize}

\hypertarget{practical-question}{%
\subsection{Practical Question}\label{practical-question}}

The primary practical question is to see how the variables in the data set or derived variables from the data set affect the price of the unit area. Since we focus on the simple linear regression model in this module. We will pick one of the variables to perform the simple linear regression model.

\hypertarget{exploratory-data-analysis}{%
\subsection{Exploratory Data Analysis}\label{exploratory-data-analysis}}

We first explore the pairwise association between the variables in the data set. Since longitude and latitude are included in the data set, we first make a map to see if we can define a variable according to the sales based on the geographic regions.

To start, we load the data to R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{library}\NormalTok{(scales)}
\NormalTok{houseURL }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww03/w03{-}Realestate.csv"}
\NormalTok{realestate }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(houseURL, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{realestate }\OtherTok{\textless{}{-}}\NormalTok{ realestate[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\CommentTok{\# longitude and latitude will be used to make a map in the upcoming analysis.}
\NormalTok{lon }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{Longitude}
\NormalTok{lat }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{Latitude }
\end{Highlighting}
\end{Shaded}

Next, we make a scatter plot of longitude and latitude to see the potential to create a geographic variable. The source does not mention the geographical locations where this data was collected. In the upcoming analysis, we will draw maps and will see the site of the houses sold.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(lon, lat, }\AttributeTok{main =} \StringTok{"Sites of houses sold in 2012{-}2013"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-65-1} \end{center}

We can see that there are clusters in the plot. We can use this information to define a cluster variable to capture the potential differences between the geographic clusters in terms of the sale prices.

We now make a simple pairwise plot to show the association between variables and pick one as an example for the simple linear regression model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(realestate, }\AttributeTok{main =}\StringTok{"Pair{-}wise Association: Scatter Plot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-66-1} \end{center}

The above pair-wise plot shows that some of the pair-wise associations are stronger than others. We will revisit this data set and use a multiple linear regression model to see which set of variables has a statistically significant impact on the sale prices.

In this module, We choose the distance to the nearest MRT station and the explanatory variable in the analysis.

\hypertarget{simple-linear-regression-review}{%
\section{Simple Linear Regression: Review}\label{simple-linear-regression-review}}

In this section, I list the basis of the simple linear regression model. I will use some mathematical equations to explain some key points. You can find the basic commands to create mathematical equations in the R Markdown on the following web page: \url{https://rpruim.github.io/s341/S19/from-class/MathinRmd.html}.

\hypertarget{structure-and-assumptions-of-simple-linear-regression-model}{%
\subsection{Structure and Assumptions of Simple Linear Regression Model}\label{structure-and-assumptions-of-simple-linear-regression-model}}

Let \(Y\) be the response variable (in our case, Y = Price of unit area and is assumed to be random) and X be the explanatory variable (in our case, X = distance to the nearest MRT station, X is assumed to be non-random). The mathematics expression of a typical simple linear regression is given by

\[
y = \beta_0 + \beta_1 x + \epsilon
\]

\hypertarget{assumptions}{%
\subsubsection{Assumptions}\label{assumptions}}

The basic assumptions of a linear regression model are listed below

\begin{itemize}
\item
  The responsible variable (\(y\)) and the explanatory variable (\(x\)) have a linear trend,
\item
  The residual \(\epsilon \sim N(0, \sigma^2)\). Equivalently, \(y \sim N(\beta_0 + \beta_1 x)\).
\end{itemize}

\hypertarget{interpretation-of-regression-coefficients}{%
\subsubsection{Interpretation of Regression Coefficients}\label{interpretation-of-regression-coefficients}}

The simple linear regression model has three unknown parameters: intercept parameters (\(\beta_0\)), slope parameter (\(\beta_1\)), and the variance of the response variable (\(\sigma^2\)). The key parameter of interest is the slope parameter since it captures the information on whether the response variable and the explanatory variable are (linearly) associated.

\begin{itemize}
\item
  If \(y\) and \(x\) are not linearly associated, that is, \(\beta_1 = 0\), then \(\beta_0\) is the mean of \(y\).
\item
  If \(\beta_1 > 0\), then \(y\) and \(x\) are positively linearly correlated. Furthermore, \(\beta_1\) is the increment of the response when the explanatory variable increases by one unit.
\item
  We can similarly interpret \(\beta_1\) when it is negative.
\end{itemize}

\hypertarget{potential-violations-to-model-assumptions}{%
\subsubsection{Potential Violations to Model Assumptions}\label{potential-violations-to-model-assumptions}}

There are potential violations of model assumptions. These violations are usually reflected in the residual plot in data analysis. You can find different residual plots representing different violations from any linear regression model. The residual plot that has no model violation should be similar to the following figure.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# I arbitrarily choose n = 70, mu =0 and constant variance 25}
\NormalTok{residual}\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{70}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{)  }
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{70}\NormalTok{, residual, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"navy"}\NormalTok{, }
     \AttributeTok{xlab =} \StringTok{""}\NormalTok{, }\AttributeTok{ylab =} \StringTok{""}\NormalTok{,}
     \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{15}\NormalTok{, }\DecValTok{15}\NormalTok{),}
     \AttributeTok{main =} \StringTok{"Ideal Residual Plot"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\DecValTok{1}\NormalTok{, }\AttributeTok{col=} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-67-1} \end{center}

\hypertarget{estimation-of-parameters}{%
\subsubsection{Estimation of Parameters}\label{estimation-of-parameters}}

Two methods: least square estimation (LSE) and maximum likelihood estimation (MLE) yield the estimate. LSE does not use the distributional information of the response variable. The MLE uses the assumption of the normal distribution of the response variable.

When making inferences on the regression coefficients, we need to use the assumption that the response variable is normally distributed.

\hypertarget{fitting-slr-to-data}{%
\section{Fitting SLR to Data}\label{fitting-slr-to-data}}

We use both parametric and bootstrap regression models to assess the association between the sale price and the distance to the nearest MRT station. As usual, we make a scatter plot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{distance }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{Distance2MRT}
\NormalTok{price }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{PriceUnitArea}
\FunctionTok{plot}\NormalTok{(distance, price, }\AttributeTok{pch =} \DecValTok{21}\NormalTok{, }\AttributeTok{col =}\StringTok{"navy"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Relationship between Distance and Price"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-68-1.pdf}
The above scatter plot indicates a negative linear association between the house price and distance to the nearest MRT station.

\hypertarget{parametric-slr}{%
\subsection{Parametric SLR}\label{parametric-slr}}

We use the built-in \textbf{lm()} to fit the SLR model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{distance }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{Distance2MRT}
\NormalTok{price }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{PriceUnitArea}
\NormalTok{parametric.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ distance)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(parametric.model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-69-1} \end{center}

\hypertarget{residual-plots}{%
\subsubsection{Residual Plots}\label{residual-plots}}

We can see from the residual plots that

\begin{itemize}
\item
  The top-left residual plot has three clusters indicating that some group variable is missing.
\item
  The bottom-left plot also reveals the same pattern.
\item
  The top-right plot reals the violation of the normality assumption.
\item
  The bottom-right plot indicates that there are no serious outliers,
\item
  In addition, the top-left residual plot also has a non-linear trend. We will not do the variable transformation to fix the problem,
\end{itemize}

\hypertarget{inferential-statistics}{%
\subsubsection{Inferential Statistics}\label{inferential-statistics}}

The inferential statistics based on the above model are summarized in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg.table }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(}\FunctionTok{summary}\NormalTok{(parametric.model))}
\FunctionTok{kable}\NormalTok{(reg.table, }\AttributeTok{caption =} \StringTok{"Inferential statistics for the parametric linear}
\StringTok{      regression model: house sale price and distance to the nearest MRT station"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-70}Inferential statistics for the parametric linear
      regression model: house sale price and distance to the nearest MRT station}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 45.8514271 & 0.6526105 & 70.25849 & 0\\
\hline
distance & -0.0072621 & 0.0003925 & -18.49971 & 0\\
\hline
\end{tabular}
\end{table}

We will not discuss the p-value since the residual plots indicate potential violations of the model assumption. In other words, the p-value may be wrong. We will wait for the bootstrap regression in the next sub-section.

A descriptive interpretation of the slope parameter is that, as the distance increases by \(1000\) feet, the corresponding price of unit area decreases by roughly \(\$7.3\).

\hypertarget{bootstrap-regression}{%
\subsection{Bootstrap Regression}\label{bootstrap-regression}}

There are two different non-parametric bootstrap methods for bootstrap regression modeling. We use the intuitive approach: sampling cases method.

\hypertarget{bootstrapping-cases}{%
\subsubsection{Bootstrapping Cases}\label{bootstrapping-cases}}

The idea is to take a bootstrap sample of the observation ID and then use the observation ID to take the corresponding records to form a bootstrap sample.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec.id }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(price)   }\CommentTok{\# vector of observation ID}
\NormalTok{boot.id }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(vec.id, }\FunctionTok{length}\NormalTok{(price), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)   }\CommentTok{\# bootstrap obs ID.}
\NormalTok{boot.price }\OtherTok{\textless{}{-}}\NormalTok{ price[boot.id]           }\CommentTok{\# bootstrap price}
\NormalTok{boot.distance }\OtherTok{\textless{}{-}}\NormalTok{ distance[boot.id]     }\CommentTok{\# corresponding bootstrap distance}
\end{Highlighting}
\end{Shaded}

With bootstrap price and bootstrap distance, we fit a bootstrap linear regression.

\hypertarget{bootstrap-regression-1}{%
\subsubsection{Bootstrap Regression}\label{bootstrap-regression-1}}

If we repeat the bootstrap sampling and regression modeling many times, we will have many bootstrap regression coefficients. These bootstrap coefficients can be used to construct the bootstrap confidence interval of the regression coefficient. Since the sample size is 414. The bootstrap regression method will produce a robust confidence interval of the slope of the distance. If 0 is not in the confidence interval, then the slope is significant.

The following steps construct bootstrap confidence intervals of regression coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}    \CommentTok{\# number of bootstrap replicates}
\CommentTok{\# define empty vectors to store bootstrap regression coefficients}
\NormalTok{boot.beta0 }\OtherTok{\textless{}{-}} \ConstantTok{NULL} 
\NormalTok{boot.beta1 }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\DocumentationTok{\#\# bootstrap regression models using for{-}loop}
\NormalTok{vec.id }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(price)   }\CommentTok{\# vector of observation ID}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B)\{}
\NormalTok{  boot.id }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(vec.id, }\FunctionTok{length}\NormalTok{(price), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)   }\CommentTok{\# bootstrap obs ID.}
\NormalTok{  boot.price }\OtherTok{\textless{}{-}}\NormalTok{ price[boot.id]           }\CommentTok{\# bootstrap price}
\NormalTok{  boot.distance }\OtherTok{\textless{}{-}}\NormalTok{ distance[boot.id]     }\CommentTok{\# corresponding bootstrap distance}
  \DocumentationTok{\#\# regression}
\NormalTok{  boot.reg }\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(price[boot.id] }\SpecialCharTok{\textasciitilde{}}\NormalTok{ distance[boot.id]) }
\NormalTok{  boot.beta0[i] }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(boot.reg)[}\DecValTok{1}\NormalTok{]   }\CommentTok{\# bootstrap intercept}
\NormalTok{  boot.beta1[i] }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(boot.reg)[}\DecValTok{2}\NormalTok{]   }\CommentTok{\# bootstrap slope}
\NormalTok{\}}
\DocumentationTok{\#\#  95\% bootstrap confidence intervals}
\NormalTok{boot.beta0.ci }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(boot.beta0, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{), }\AttributeTok{type =} \DecValTok{2}\NormalTok{)}
\NormalTok{boot.beta1.ci }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(boot.beta1, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{), }\AttributeTok{type =} \DecValTok{2}\NormalTok{)}
\NormalTok{boot.coef }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(boot.beta0.ci, boot.beta1.ci)) }
\FunctionTok{names}\NormalTok{(boot.coef) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"2.5\%"}\NormalTok{, }\StringTok{"97.5\%"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(boot.coef, }\AttributeTok{caption=}\StringTok{"Bootstrap confidence intervals of regression coefficients."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-72}Bootstrap confidence intervals of regression coefficients.}
\centering
\begin{tabular}[t]{l|r|r}
\hline
  & 2.5\% & 97.5\%\\
\hline
boot.beta0.ci & 44.5311478 & 47.2894820\\
\hline
boot.beta1.ci & -0.0080022 & -0.0065578\\
\hline
\end{tabular}
\end{table}

The 95\% bootstrap confidence interval of the slope is \((-0.0079753,-0.0065808)\). Since both limits are negative, the price of the unit area and the distance to the nearest MRT station are negatively associated. Note that zero is NOT in the confidence interval. Both parametric and bootstrap regression models indicate that the slope coefficient is significantly different from zero. This means the sale price and the distance to the nearest MRT station are statistically correlated.

\hypertarget{concluding-remarks}{%
\subsection{Concluding Remarks}\label{concluding-remarks}}

Here are several remarks about the parametric and bootstrap regression models.

\begin{itemize}
\item
  If there are serious violations to the model assumptions and the sample size is not too small, bootstrap confidence intervals of regression coefficients are more reliable than the parametric p-values since the bootstrap method is non-parametric inference.
\item
  If the form of the regression function is misspecified, the bootstrap confidence intervals are valid based on the misspecified form of the relationship between the response and the explanatory variable. However, the p-values could be wrong if the residual is not normally distributed.
\item
  If the sample size is significantly large, both Bootstrap and the parametric methods yield the same results.
\item
  If the sample size is too small, the bootstrap confidence interval could still be correct (depending on whether the sample empirical distribution is close to the true joint distribution of variables in the data set). The parametric regression is very sensitive to the normal assumption of the residual!
\item
  General Recommendations

  \begin{itemize}
  \item
    If there is no violation of the model assumption, always use the parametric regression. The residual plots reveal potential violations of the model assumption.
  \item
    If there are potential violations to the model assumptions and the violations cannot be fixed by remedy methods such as variable transformation, the bootstrap method is more reliable.
  \item
    if the same size is significantly large, bootstrap and parametric methods yield similar results.
  \end{itemize}
\end{itemize}

\hypertarget{bootstrap-residual-method}{%
\subsection{Bootstrap Residual Method}\label{bootstrap-residual-method}}

Bootstrapping residuals is another way to generate bootstrap random samples that are supposed to have the same distribution as that \(Y\) in the original random sample. The following flow chart explains the process of how to generate bootstrap random samples.

\begin{center}\includegraphics[width=1\linewidth]{img03/w03-BootstrapResiduals} \end{center}

Next, we use a simple to demonstrate the steps to generate bootstrap samples based on sampling bootstrap residuals. The data set at \url{https://raw.githubusercontent.com/pengdsci/sta321/main/ww03/handheight.txt} has three variables: sex, height, and hand span. We will this data set to assess the linear correlation between height and hand span. We first do some exploratory analysis to visualize the potential association between height and hand size.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myurl }\OtherTok{=} \StringTok{"https://online.stat.psu.edu/stat501/sites/stat501/files/data/handheight.txt"}
\NormalTok{handheight }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(myurl, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{MID }\OtherTok{=} \FunctionTok{which}\NormalTok{(handheight}\SpecialCharTok{$}\NormalTok{Sex}\SpecialCharTok{==}\StringTok{"Male"}\NormalTok{)}
\NormalTok{MaleData }\OtherTok{=}\NormalTok{ handheight[MID,]}
\NormalTok{FealeData }\OtherTok{=}\NormalTok{ handheight[}\SpecialCharTok{{-}}\NormalTok{MID,]}
\FunctionTok{plot}\NormalTok{(handheight}\SpecialCharTok{$}\NormalTok{Height, handheight}\SpecialCharTok{$}\NormalTok{HandSpan, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{, }\AttributeTok{col=}\StringTok{"white"}\NormalTok{,}
                        \AttributeTok{xlab =} \StringTok{"Hand Span"}\NormalTok{,}
                        \AttributeTok{ylab =} \StringTok{"Height"}\NormalTok{,}
                        \AttributeTok{main =} \StringTok{"Hand Span vs Height"}\NormalTok{,}
                        \AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{,}
                        \AttributeTok{cex.main =} \FloatTok{0.8}\NormalTok{,}
                        \AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(handheight}\SpecialCharTok{$}\NormalTok{Height[MID], handheight}\SpecialCharTok{$}\NormalTok{HandSpan[MID], }\AttributeTok{pch=}\DecValTok{16}\NormalTok{, }\AttributeTok{col=}\FunctionTok{alpha}\NormalTok{(}\StringTok{"darkred"}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\FunctionTok{points}\NormalTok{(handheight}\SpecialCharTok{$}\NormalTok{Height[}\SpecialCharTok{{-}}\NormalTok{MID], handheight}\SpecialCharTok{$}\NormalTok{HandSpan[}\SpecialCharTok{{-}}\NormalTok{MID], }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{col=}\FunctionTok{alpha}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-74-1} \end{center}

\hypertarget{implementing-bootstrapping-residuals}{%
\subsubsection{Implementing Bootstrapping Residuals}\label{implementing-bootstrapping-residuals}}

The following code reflects the steps in the above flow-chart

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height }\OtherTok{=}\NormalTok{ handheight}\SpecialCharTok{$}\NormalTok{Height}
\NormalTok{handspan }\OtherTok{=}\NormalTok{ handheight}\SpecialCharTok{$}\NormalTok{HandSpan}
\NormalTok{m0 }\OtherTok{=} \FunctionTok{lm}\NormalTok{(height}\SpecialCharTok{\textasciitilde{}}\NormalTok{handspan)}
\NormalTok{E }\OtherTok{=} \FunctionTok{resid}\NormalTok{(m0)            }\CommentTok{\# Original residuals}
\NormalTok{a.hat }\OtherTok{=} \FunctionTok{coef}\NormalTok{(m0)[}\DecValTok{1}\NormalTok{]}
\NormalTok{b.hat }\OtherTok{=} \FunctionTok{coef}\NormalTok{(m0)[}\DecValTok{2}\NormalTok{]}
\DocumentationTok{\#\#}
\NormalTok{B }\OtherTok{=} \DecValTok{1000}                 \CommentTok{\# generating 1000 bootstrap samples}
\NormalTok{bt.alpha }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, B)}
\NormalTok{bt.beta }\OtherTok{=}\NormalTok{ bt.alpha}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B)\{}
\NormalTok{  bt.e }\OtherTok{=} \FunctionTok{sample}\NormalTok{(E, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)          }\CommentTok{\# bootstrap residuals}
\NormalTok{  y.hat }\OtherTok{=}\NormalTok{ a.hat }\SpecialCharTok{+}\NormalTok{ b.hat}\SpecialCharTok{*}\NormalTok{handspan }\SpecialCharTok{+}\NormalTok{ bt.e     }\CommentTok{\# bootstrap heights}
  \DocumentationTok{\#\# bootstrap SLR}
\NormalTok{  bt.m }\OtherTok{=} \FunctionTok{lm}\NormalTok{(y.hat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ handspan)}
\NormalTok{  bt.alpha[i] }\OtherTok{=} \FunctionTok{coef}\NormalTok{(bt.m)[}\DecValTok{1}\NormalTok{]}
\NormalTok{  bt.beta[i] }\OtherTok{=} \FunctionTok{coef}\NormalTok{(bt.m)[}\DecValTok{2}\NormalTok{]}
\NormalTok{ \}}
\NormalTok{alpha.CI }\OtherTok{=} \FunctionTok{quantile}\NormalTok{(bt.alpha, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{))}
\NormalTok{beta.CI }\OtherTok{=} \FunctionTok{quantile}\NormalTok{(bt.beta, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{))}
\DocumentationTok{\#\#}
\NormalTok{per}\FloatTok{.025} \OtherTok{=} \FunctionTok{c}\NormalTok{(alpha.CI[}\DecValTok{1}\NormalTok{],beta.CI[}\DecValTok{1}\NormalTok{])     }\CommentTok{\# lower CI for alpha and beta}
\NormalTok{per}\FloatTok{.975} \OtherTok{=} \FunctionTok{c}\NormalTok{(alpha.CI[}\DecValTok{2}\NormalTok{],beta.CI[}\DecValTok{2}\NormalTok{])     }\CommentTok{\# upper CI for alpha and beta  }
\end{Highlighting}
\end{Shaded}

Next, we add the confidence limits to the output inferential table from the SLR based on the original sample.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.inference }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{((}\FunctionTok{summary}\NormalTok{(m0))}\SpecialCharTok{$}\NormalTok{coef)}
\NormalTok{lm.inference}\SpecialCharTok{$}\NormalTok{per}\FloatTok{.025} \OtherTok{=}\NormalTok{ per}\FloatTok{.025}
\NormalTok{lm.inference}\SpecialCharTok{$}\NormalTok{per}\FloatTok{.975} \OtherTok{=}\NormalTok{ per}\FloatTok{.975}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(lm.inference))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|) & per.025 & per.975\\
\hline
(Intercept) & 35.52504 & 2.3159512 & 15.33929 & 0 & 31.197906 & 40.228050\\
\hline
handspan & 1.56008 & 0.1105437 & 14.11278 & 0 & 1.338297 & 1.770703\\
\hline
\end{tabular}

\hypertarget{data-set-selection-for-project-one}{%
\section{Data Set Selection for Project One}\label{data-set-selection-for-project-one}}

It is time to find a data set for project \#1 focusing on the parametric and non-parametric linear regression model. As I did in this note, you will choose one of the variables in this data set to complete this week's assignment.

\hypertarget{data-set-requirements}{%
\subsection{Data Set Requirements}\label{data-set-requirements}}

The basic requirements of the data set are:

The desired data set must have

\begin{itemize}
\item
  the response variable \textbf{must} be continuous random variables.
\item
  at least two categorical explanatory variables.
\item
  at least one of the categorical variables has more than two categories.
\item
  at least two numerical explanatory variables.
\item
  at least 15 observations are required for estimating each regression coefficient. For example, if your final linear model has 11 variables (including dummy variables), you need \(12 \times 15 = 180\) observations.
\end{itemize}

\hypertarget{web-sites-with-data-sets}{%
\subsection{Web Sites with Data Sets}\label{web-sites-with-data-sets}}

The following sites have some data sets. You can also choose data from other sites.

\begin{itemize}
\item
  10 open data sets for linear regression: \url{https://lionbridge.ai/datasets/10-open-datasets-for-linear-regression/}
\item
  UFL Larry Winner's Teaching Data Sets: \url{http://users.stat.ufl.edu/~winner/datasets.html}
\item
  Kaggle site: \url{https://www.kaggle.com/datasets?tags=13405-Linear+Regression}
\end{itemize}

\hypertarget{analysis-and-writing-components}{%
\subsection{Analysis and Writing Components}\label{analysis-and-writing-components}}

\hypertarget{description-of-the-data-set}{%
\subsubsection{Description of the Data Set}\label{description-of-the-data-set}}

Write an essay to describe the data set. The following information is expected to be included in this description.

\begin{itemize}
\tightlist
\item
  How the data was collected?
\item
  List of all variables: names and their variable types.
\item
  What are your practical and analytic questions
\item
  Does the data set have enough information to answer the questions
\end{itemize}

\hypertarget{simple-linear-regression}{%
\subsubsection{Simple Linear Regression}\label{simple-linear-regression}}

Make a pair-wise scatter plot of all variables in your selected data set and choose an explanatory variable that is linearly correlated to the response variable.

\begin{itemize}
\item
  Make a pairwise scatter plot and comment on the relationship between the response and explanatory variables.

  \begin{itemize}
  \tightlist
  \item
    If there is a non-linear pattern, can you perform a transformation of one of the variables so that the transformed variable and the other original variable have a linear pattern?
  \item
    If you have a choice to transform either the response variable or the explanatory variable, what is your choice and why?
  \end{itemize}
\item
  Fit an ordinary least square regression (SLR) to capture the linear relationship between the two variables. If you transformed one of the variables to achieve the linear relationship, then use the transformed variable in the model. and then perform the model diagnostics. Comment on the residual plots and point out the violations to the model assumptions.
\item
  Using the bootstrap algorithm on the \textbf{previous final linear regression model} to estimate the bootstrap confidence intervals of regression coefficients (using \(95\%\) confidence level).
\item
  compare the p-values and bootstrap confidence intervals of corresponding regression coefficients of the final linear regression model, make a recommendation on which inferential result to be reported, and justify.
\end{itemize}

\hypertarget{multiple-linear-regression-model}{%
\chapter{Multiple Linear Regression Model}\label{multiple-linear-regression-model}}

The general purpose of multiple linear regression (MLR) is to identify a relationship in an explicit functional form between explanatory variables or predictors and the dependent response variable. This relationship will be used to achieve two primary tasks:

\begin{itemize}
\item
  \textbf{Association analysis} - understanding the association between predictors and the response variable. As a special association, the causal relationship can be assessed under certain conditions.
\item
  \textbf{Prediction} - the relationship between predictors and the response can be used to predict the response with new out-of-sample values of predictors.
\end{itemize}

\hypertarget{the-structure-of-mlr}{%
\section{The structure of MLR}\label{the-structure-of-mlr}}

Let \(\{x_1, x_2, \cdots, x_k \}\) be \(k\) explanatory variables and \(y\) be the response variables. The general form of the multiple linear regression model is defined as

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \epsilon.
\]
This is a very special form in that y is linear in both parameters and predictors. The actual linear regression only assumes that \(y\) is linear only in parameters but not predictors since the value of predictors will be observed in data.

\hypertarget{assumptions-based-on-the-above-special-form.}{%
\subsection{Assumptions based on the above special form.}\label{assumptions-based-on-the-above-special-form.}}

\begin{itemize}
\tightlist
\item
  The function form between \(y\) and \(\{x_1, x_2, \cdots, x_k \}\) must be correctly specified.
\item
  The residual \(\epsilon\) must be normally distributed with \(\mu = 0\) and a constant variance \(\sigma^2\).
\item
  An implicit assumption is that predictor variables are non-random.
\end{itemize}

\hypertarget{potential-violations}{%
\subsection{Potential Violations}\label{potential-violations}}

There are various potential violations of the model assumptions. The following is a shortlist of potential violations of the model assumptions.

\begin{itemize}
\tightlist
\item
  The potential incorrect functional relationship between the response and the predictor variables.

  \begin{itemize}
  \tightlist
  \item
    the correct form may have power terms.
  \item
    the correct form may have a cross-product form.
  \item
    the correct form may need important variables that are missing.
  \end{itemize}
\item
  The residual term does not follow the normal distribution \(N(0, \sigma^2)\). That means that

  \begin{itemize}
  \tightlist
  \item
    \(\epsilon\) is not normally distributed at all.
  \item
    \(\epsilon\) is normally distributed but the variance is not a constant.
  \end{itemize}
\end{itemize}

\hypertarget{variable-types}{%
\subsection{Variable types}\label{variable-types}}

Since there will be multiple variables involved in the multiple regression model.

\begin{itemize}
\item
  All explanatory variables are continuous variables - classical linear regression models.
\item
  All explanatory variables are categorical variables - analysis of variance (ANOVA) models.
\item
  The model contains both continuous and categorical variables - analysis of covariance (ANCOVA) model.
\end{itemize}

\hypertarget{dummy-and-discrete-numerical-explanatory-variables}{%
\subsection{Dummy and discrete numerical explanatory variables}\label{dummy-and-discrete-numerical-explanatory-variables}}

\begin{itemize}
\tightlist
\item
  Categorical variables with \(n\) (\(n > 2\)) categories MUST be dichotomized into \(n-1\) dummy variables (binary indicator variables).

  \begin{itemize}
  \tightlist
  \item
    \textbf{Caution}: categorical variable with a numerical coding - need to use R function \textbf{factor()} to automatically define a sequence of dummy variables for the category variable.
  \end{itemize}
\item
  \textbf{Discrete (numerical) variable} - If we want to use discrete numerical as a categorical variable, we must dichotomize it and define a sequence of dummy variables since interpretations of the two types of variables are different. In a categorical variable case, the coefficient of a dummy variable is the relative contribution to the response compared with the baseline category. In the discrete case, the regression coefficient is the relative contribution to the response variable compared with adjacent values and the relative contribution is constant across all adjacent values of the discrete predictor variable.
\end{itemize}

\hypertarget{interpretation-of-regression-coefficients-1}{%
\subsection{Interpretation of Regression Coefficients}\label{interpretation-of-regression-coefficients-1}}

A \_multiple linear regression model with \(k\) predictor variables has k+1 unknown parameters: intercept parameters (\(\beta_0\)), slope parameters (\(\beta_i, i = 1, 2, \cdots, k\)), and the variance of the response variable (\(\sigma^2\)). The key parameters of interest are the slope parameters since they capture the information on whether the response variable and the corresponding explanatory variables are (linearly) associated.

\begin{itemize}
\item
  If \(y\) and \(x_i\) are not linearly associated, that is, \(\beta_i = 0, i = 1, 2, \cdots, k\), then \(\beta_0\) is the mean of \(y\).
\item
  If \(\beta_i > 0\), then \(y\) and \(x_i\) are positively linearly correlated. Furthermore, \(\beta_i\) is the increment of the response when the explanatory variable increases by one unit.
\item
  We can similarly interpret \(\beta_i\) when it is negative.
\end{itemize}

\hypertarget{model-building}{%
\section{Model building}\label{model-building}}

Modeling building is an iterative process for searching for the best model to fit the data. An implicit assumption is that the underlying data is statistically valid.

\hypertarget{data-structure-sample-size-and-preparation-for-mlr}{%
\subsection{Data structure, sample size, and preparation for MLR}\label{data-structure-sample-size-and-preparation-for-mlr}}

In the model-building phase, we assume data is valid and has sufficient information to address the research hypothesis.

\begin{itemize}
\item
  Data records are independent - collected based on a cross-sectional design.
\item
  The sample size should be large enough such that each regression coefficient should have 14 distinct data points to warrant reliable and robust estimates of regression coefficients.
\item
  Imbalanced categorical variables and extremely distributed continuous explanatory variables need to be treated to a warrant valid estimate of regression coefficients. This includes combining categories in meaningful and practically interpretable ways and discretizing extremely skewed continuous variables.
\item
  New variable definition - sometimes we can extract information from several variables to define new variables to build a better model. This is an active area in machine learning fields and data science. There are many different methods and algorithms in literature and practice for creating new variables based on existing ones.

  \begin{itemize}
  \tightlist
  \item
    Empirical approach - based on experience and numerical pattern.
  \item
    Model-based approach - this may require a highly technical understanding of algorithms and modeling ideas. This is not the main consideration in this course.
  \end{itemize}
\end{itemize}

\hypertarget{candidate-models-and-residual-diagnostics}{%
\subsection{Candidate models and residual diagnostics}\label{candidate-models-and-residual-diagnostics}}

\begin{itemize}
\item
  Consider only the multiple linear regression models that have a linear relationship between response and predictor variables.
\item
  Perform residual analysis

  \begin{itemize}
  \tightlist
  \item
    if a curve pattern appears in residual plots, identify a curve linear relationship between the response and the individual predictor variable
  \item
    if non-constant variance appears in the residual plots, then perform an appropriate transformation to stabilize the constant variance - for example, Box-cox transformation.
  \item
    if the QQ plot indicates non-normal residuals, try transformations to convert it to a normal variable.
  \item
    if there are serial patterns in the residual plot, we need to remove the serial pattern with an appropriate method.
  \item
    if some clusters appear in the residual plot, create a group variable to capture the clustering information.
  \end{itemize}
\end{itemize}

\hypertarget{significant-test-goodness-of-fit-and-variable-selection}{%
\subsection{Significant test, goodness-of-fit, and Variable selection}\label{significant-test-goodness-of-fit-and-variable-selection}}

Significant tests and goodness-of-fit measures are used to identify the final model. Please keep in mind that a good statistical model must have the following properties;

\begin{itemize}
\item
  Interpretability
\item
  parsimony
\item
  Accuracy
\item
  Scalability
\end{itemize}

\hypertarget{significant-tests}{%
\subsubsection{Significant Tests}\label{significant-tests}}

Significant tests are used for selecting statistically significant variables to include in the model. However, in practical applications, some practically important variables should always be included in the model regardless of their statistical significance. The t-test is used for selecting (or dropping) individual statistically significant variables.

\hypertarget{variable-model-selection-criteria---goodness-of-fit-measures}{%
\subsubsection{Variable (model) selection criteria - Goodness-of-fit Measures}\label{variable-model-selection-criteria---goodness-of-fit-measures}}

There are many different methods based on different performance measures for model selection. Most of these goodness-of-fit measures are defined based on \textbf{sum of squares} that defined in the following (use SLR for illustration)

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img04/w04-SSS} 

}

\caption{Definitions of sum od squares}\label{fig:unnamed-chunk-77}
\end{figure}

\begin{itemize}
\tightlist
\item
  \(R^2\) - coefficient of determination. It explains the variation explained by the underlying regression model.
\end{itemize}

\[
R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}
\]

\(R^2\) is used to compare two candidate models. Adjusted \(R^2\) is used when there are many predictor variables in the model. It is defined by

\[
R^2_{adj} = 1 - \frac{n-1}{n-p} \frac{SSE}{SST}
\]

\begin{itemize}
\tightlist
\item
  Information criteria - likelihood-based measures: AIC and SBC.
\end{itemize}

\[
AIC_p = n \ln(SSE) - n\ln(n) + 2p
\]
\[
BIC_p = n\ln(SSE) - n\ln(n) + p\ln(n)
\]

\begin{itemize}
\item
  The \textbf{prediction sum of squares (PRESS)} is a model validation method used to assess a model's predictive ability that can also be used to compare regression models. It is defined based on an iterative algorithm - Leave-on-out (a special Jackknife resampling method to be discussed in the next module).
\item
  Likelihood ratio \(\chi^2\) test - comparing two candidate models with a hierarchical relationship.
\item
  Mallow's Cp - a residual-based measure that is used for comparing two models that do not necessarily have a hierarchical structure.
\end{itemize}

\hypertarget{variable-selection-methods}{%
\subsubsection{Variable selection methods}\label{variable-selection-methods}}

\begin{itemize}
\item
  Step-wise Procedures
\item
  Criterion-based procedures
\end{itemize}

This short note summarized the above two methods for \href{http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch10.pdf}{Variable Selection}(click the link to view the text).

\hypertarget{multicollinearity-detection---vif}{%
\subsubsection{Multicollinearity Detection - VIF}\label{multicollinearity-detection---vif}}

As the name suggests, a variance inflation factor (VIF) quantifies how much the variance is inflated. A variance inflation factor exists for each of the predictors in a multiple regression model. For example, the variance inflation factor for the estimated regression coefficient \(b_j\) ---denoted \(VIF_j\) ---is just the factor by which the variance of \(b_j\) is ``inflated'' by the existence of correlation among the predictor variables in the model. To be more specific, the VIF of j-th predictor is defined to be

\[
\text{VIF}_j = \frac{1}{1-R_j^2}
\]

Where \(R^2_j\) is the coefficient of determination of of regression \(E(x_j) = \alpha_0 + \alpha_1x_1 + \cdots+\alpha_{j-1}x_{j-1} + \alpha_{j+1}x_{j+1} + \cdots + \alpha_kx_k\).

A VIF of 1 means that there is no correlation among the jth predictor and the remaining predictor variables, and hence the variance of bj is not inflated at all. The general rule of thumb is that VIFs exceeding 4 warrant further investigation, while VIFs exceeding 10 are signs of serious multicollinearity requiring correction.

R function in the machine learning library \texttt{\{car\}}(classification and regression) can be used to calculate the variance inflation factor for individual predictor.

\hypertarget{case-study--factors-that-impact-the-house-sale-prices}{%
\section{Case Study -Factors That Impact the House Sale Prices}\label{case-study--factors-that-impact-the-house-sale-prices}}

We present a case study to implement various model-building techniques.

\hypertarget{data-description-1}{%
\subsection{Data Description}\label{data-description-1}}

The data in this note was found from \href{https://www.kaggle.com/}{Kaggle}. I renamed the original variables and modified the sales dates to define the sales year indicator. The modified data set was uploaded to the course web page at \url{https://raw.githubusercontent.com/pengdsci/sta321/main/ww03/w03-Realestate.csv}.

\begin{itemize}
\tightlist
\item
  ObsID
\item
  TransactionYear(X1): transaction date
\item
  HouseAge(X2): house age\\
\item
  Distance2MRT(X3): distance to the nearest MRT station
\item
  NumConvenStores(X4): number of convenience stores
\item
  Latitude(X5): latitude\\
\item
  Longitude(X6): longitude\\
\item
  PriceUnitArea(Y): house price of unit area
\end{itemize}

\hypertarget{practical-question-1}{%
\subsection{Practical Question}\label{practical-question-1}}

The primary question is to identify the association between the house sale price and relevant predictor variables available in the data set.

\hypertarget{exploratory-data-analysis-1}{%
\subsection{Exploratory Data Analysis}\label{exploratory-data-analysis-1}}

We first explore the pairwise association between the variables in the data set. Since longitude and latitude are included in the data set, we first make a map to see if we can define a variable according to the sales based on the geographic regions.

To start, we load the data to R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\NormalTok{realestate0 }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww03/w03{-}Realestate.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{realestate }\OtherTok{\textless{}{-}}\NormalTok{ realestate0[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\CommentTok{\# longitude and latitude will be used to make a map in the upcoming analysis.}
\NormalTok{lon }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{Longitude}
\NormalTok{lat }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{Latitude }
\FunctionTok{plot}\NormalTok{(lon, lat, }\AttributeTok{main =} \StringTok{"Sites of houses sold in 2012{-}2013"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FloatTok{121.529}\NormalTok{, }\AttributeTok{h=}\FloatTok{24.96}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-78-1} \end{center}

We use longitude and latitude to define a group variable, \textbf{geo.group}, in the following.

\textbf{geo.group = TRUE} if longitude \textgreater{} 121.529 AND latitude \textgreater{} 24.96;
\textbf{geo.group = FALSE} otherwise.

From the map representation of the locations of these houses given below (generated by Tableau Public), we can see that \textbf{geo. group} is an indicator

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img04/w04-map} 

}

\caption{Locations of houses for sale}\label{fig:unnamed-chunk-79}
\end{figure}

We also turn the variable \textbf{TransactionYear} into an indicator variable. At the same time, we scale the distance from the house to the nearest MRT by defining \textbf{Dist2MRT = Distance2MRT/1000}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{geo.group }\OtherTok{=}\NormalTok{ (lon }\SpecialCharTok{\textgreater{}} \FloatTok{121.529}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ (lat }\SpecialCharTok{\textgreater{}} \FloatTok{24.96}\NormalTok{)     }\CommentTok{\# define the geo.group variable}
                                                \CommentTok{\# top{-}right region = TRUE, other region = FALSE}
\NormalTok{realestate}\SpecialCharTok{$}\NormalTok{geo.group }\OtherTok{=} \FunctionTok{as.character}\NormalTok{(geo.group)  }\CommentTok{\# convert the logical values to character values.}
\NormalTok{realestate}\SpecialCharTok{$}\NormalTok{sale.year }\OtherTok{=} \FunctionTok{as.character}\NormalTok{(realestate}\SpecialCharTok{$}\NormalTok{TransactionYear) }\CommentTok{\# convert transaction year to dummy.}
\NormalTok{realestate}\SpecialCharTok{$}\NormalTok{Dist2MRT.kilo }\OtherTok{=}\NormalTok{ (realestate}\SpecialCharTok{$}\NormalTok{Distance2MRT)}\SpecialCharTok{/}\DecValTok{1000}   \CommentTok{\# re{-}scale distance: foot {-}\textgreater{} kilo feet}
\NormalTok{final.data }\OtherTok{=}\NormalTok{ realestate[, }\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{)]          }\CommentTok{\# keep only variables to be used in the candidate}
                                                \CommentTok{\# models}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(final.data))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|l|l|r}
\hline
HouseAge & NumConvenStores & PriceUnitArea & geo.group & sale.year & Dist2MRT.kilo\\
\hline
32.0 & 10 & 37.9 & TRUE & 2012 & 0.0848788\\
\hline
19.5 & 9 & 42.2 & TRUE & 2012 & 0.3065947\\
\hline
13.3 & 5 & 47.3 & TRUE & 2013 & 0.5619845\\
\hline
13.3 & 5 & 54.8 & TRUE & 2013 & 0.5619845\\
\hline
5.0 & 5 & 43.1 & TRUE & 2012 & 0.3905684\\
\hline
7.1 & 3 & 32.1 & FALSE & 2012 & 2.1750300\\
\hline
\end{tabular}

\hypertarget{fitting-mlr-to-data}{%
\subsection{Fitting MLR to Data}\label{fitting-mlr-to-data}}

We start the search process for the final model.

\hypertarget{full-model-and-diagnostics}{%
\subsubsection{Full model and diagnostics}\label{full-model-and-diagnostics}}

We start with a linear model that includes all predictor variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{full.model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(PriceUnitArea }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ final.data)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(full.model)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =}\StringTok{"Statistics of Regression Coefficients"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-81}Statistics of Regression Coefficients}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 35.9559432 & 1.7134680 & 20.984310 & 0.0000000\\
\hline
HouseAge & -0.3000749 & 0.0390329 & -7.687735 & 0.0000000\\
\hline
NumConvenStores & 1.0707846 & 0.1897962 & 5.641761 & 0.0000000\\
\hline
geo.groupTRUE & 7.5447005 & 1.3420266 & 5.621871 & 0.0000000\\
\hline
sale.year2013 & 3.0332784 & 0.9447021 & 3.210831 & 0.0014283\\
\hline
Dist2MRT.kilo & -3.6589504 & 0.5317107 & -6.881469 & 0.0000000\\
\hline
\end{tabular}
\end{table}

Next, we conduct residual diagnostic analysis to check the validity of the model before making an inference about the model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(full.model)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-82-1} 

}

\caption{Residual plots of the full model}\label{fig:unnamed-chunk-82}
\end{figure}

We can see from the residual plots that there are some minor violations:

\begin{itemize}
\item
  the variance of the residuals is not constant.
\item
  the QQ plot indicates the distribution of residuals is slightly off the normal distribution.
\item
  The residual plot seems to have a weak curve pattern.
\end{itemize}

We first perform Box-Cox transformation to correct the non-constant variance and correct the non-normality of the QQ plot.

\hypertarget{models-based-on-box-cox-transformation}{%
\subsubsection{Models Based on Box-Cox transformation}\label{models-based-on-box-cox-transformation}}

We first perform Box-Cox transformation and then choose appropriate transformations for both response and predictor variables to build candidate regression models.

\hypertarget{box-cox-transformations}{%
\paragraph{Box-Cox Transformations}\label{box-cox-transformations}}

Since non-constant variance, we perform the Box-Cox procedure to search for a transformation of the response variable. We perform several tried Box-Cox transformations with different transformed

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{pty =} \StringTok{"s"}\NormalTok{, }\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{oma=}\FunctionTok{c}\NormalTok{(.}\DecValTok{1}\NormalTok{,.}\DecValTok{1}\NormalTok{,.}\DecValTok{1}\NormalTok{,.}\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\DocumentationTok{\#\#}
\FunctionTok{boxcox}\NormalTok{(PriceUnitArea }\SpecialCharTok{\textasciitilde{}}\NormalTok{ HouseAge }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}  \FunctionTok{log}\NormalTok{(Dist2MRT.kilo)  }
       \SpecialCharTok{+}\NormalTok{ geo.group, }\AttributeTok{data =}\NormalTok{ final.data, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{10}\NormalTok{), }
       \AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(lambda, }\StringTok{": log dist2MRT"}\NormalTok{)))}
\DocumentationTok{\#\#}
\FunctionTok{boxcox}\NormalTok{(PriceUnitArea }\SpecialCharTok{\textasciitilde{}}\NormalTok{ HouseAge }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}\NormalTok{  Dist2MRT.kilo  }\SpecialCharTok{+} 
\NormalTok{       geo.group, }\AttributeTok{data =}\NormalTok{ final.data, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{10}\NormalTok{), }
       \AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(lambda, }\StringTok{": dist2MRT"}\NormalTok{)))}
\DocumentationTok{\#\#}
\FunctionTok{boxcox}\NormalTok{(PriceUnitArea }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{HouseAge) }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}\NormalTok{  Dist2MRT.kilo  }\SpecialCharTok{+} 
\NormalTok{      geo.group, }\AttributeTok{data =}\NormalTok{ final.data, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{10}\NormalTok{), }
       \AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(lambda, }\StringTok{": log{-}age"}\NormalTok{)))}
\DocumentationTok{\#\#}
\FunctionTok{boxcox}\NormalTok{(PriceUnitArea }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{HouseAge) }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}  \FunctionTok{log}\NormalTok{(Dist2MRT.kilo)  }\SpecialCharTok{+} 
\NormalTok{      geo.group, }\AttributeTok{data =}\NormalTok{ final.data, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{10}\NormalTok{), }
      \AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(lambda, }\StringTok{": log{-}age, log.dist2MRT"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-83-1} \end{center}

The above Box-cox transformation plots indicate the optimal \(\lambda\) under different transformed predictor variables. log-Transformed distance from MRT impacts the coefficient of the power transformation: \(\lambda\).

As a special power transformation, if \(\lambda = 0\), the transformation degenerates to log transformation.

\hypertarget{square-root-transformation}{%
\paragraph{Square-root Transformation}\label{square-root-transformation}}

We perform Box-Cox transformation with log-transformed distance to the nearest MRT in the following.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sqrt.price.log.dist }\OtherTok{=} \FunctionTok{lm}\NormalTok{((PriceUnitArea)}\SpecialCharTok{\^{}}\FloatTok{0.5} \SpecialCharTok{\textasciitilde{}}\NormalTok{ HouseAge }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}  \FunctionTok{log}\NormalTok{(Dist2MRT.kilo)  }\SpecialCharTok{+}\NormalTok{ geo.group, }\AttributeTok{data =}\NormalTok{ final.data)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(sqrt.price.log.dist)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =} \StringTok{"log{-}transformed model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-84}log-transformed model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 5.3978808 & 0.0923831 & 58.429342 & 0.0000000\\
\hline
HouseAge & -0.0209198 & 0.0029561 & -7.076954 & 0.0000000\\
\hline
NumConvenStores & 0.0513220 & 0.0153946 & 3.333767 & 0.0009351\\
\hline
sale.year2013 & 0.2951406 & 0.0707905 & 4.169214 & 0.0000374\\
\hline
log(Dist2MRT.kilo) & -0.4905637 & 0.0481611 & -10.185897 & 0.0000000\\
\hline
geo.groupTRUE & 0.5709120 & 0.0973966 & 5.861723 & 0.0000000\\
\hline
\end{tabular}
\end{table}

Residual plots are given below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(sqrt.price.log.dist)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-85-1} \end{center}

There are two improvements in the above residual diagnostic plots: (1) the weak curve pattern has been removed from the residual plot; (2) the non-constant variance has also been corrected. However, the violation of the normality assumption is still an issue.

\hypertarget{log-transformation}{%
\paragraph{Log-Transformation}\label{log-transformation}}

We take the log transformation of the sale price according to the Box-Cox transformation and then build a linear regression based on the log price.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log.price }\OtherTok{=} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(PriceUnitArea) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ HouseAge }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}\NormalTok{  Dist2MRT.kilo  }\SpecialCharTok{+}\NormalTok{ geo.group, }\AttributeTok{data =}\NormalTok{ final.data)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(log.price)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =} \StringTok{"log{-}transformed model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-86}log-transformed model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 3.5724282 & 0.0443235 & 80.599030 & 0.0000000\\
\hline
HouseAge & -0.0075712 & 0.0010097 & -7.498507 & 0.0000000\\
\hline
NumConvenStores & 0.0274872 & 0.0049096 & 5.598667 & 0.0000000\\
\hline
sale.year2013 & 0.0805519 & 0.0244373 & 3.296272 & 0.0010655\\
\hline
Dist2MRT.kilo & -0.1445122 & 0.0137541 & -10.506820 & 0.0000000\\
\hline
geo.groupTRUE & 0.1825871 & 0.0347151 & 5.259583 & 0.0000002\\
\hline
\end{tabular}
\end{table}

Residual plots are given below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(log.price)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-87-1} \end{center}

The above residual diagnostic plots are similar to that of the previous model. The Q-Q plots of all three models are similar to each other, this means that the assumption of normal residuals is not satisfied for all three models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#define plotting area}
\FunctionTok{par}\NormalTok{(}\AttributeTok{pty =} \StringTok{"s"}\NormalTok{, }\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\CommentTok{\#Q{-}Q plot for original model}
\FunctionTok{qqnorm}\NormalTok{(full.model}\SpecialCharTok{$}\NormalTok{residuals, }\AttributeTok{main =} \StringTok{"Full{-}Model"}\NormalTok{)}
\FunctionTok{qqline}\NormalTok{(full.model}\SpecialCharTok{$}\NormalTok{residuals)}
\CommentTok{\#Q{-}Q plot for Box{-}Cox transformed model}
\FunctionTok{qqnorm}\NormalTok{(log.price}\SpecialCharTok{$}\NormalTok{residuals, }\AttributeTok{main =} \StringTok{"Log{-}Price"}\NormalTok{)}
\FunctionTok{qqline}\NormalTok{(log.price}\SpecialCharTok{$}\NormalTok{residuals)}
\CommentTok{\#display both Q{-}Q plots}
\FunctionTok{qqnorm}\NormalTok{(sqrt.price.log.dist}\SpecialCharTok{$}\NormalTok{residuals, }\AttributeTok{main =} \StringTok{"sqrt price log dist"}\NormalTok{)}
\FunctionTok{qqline}\NormalTok{(sqrt.price.log.dist}\SpecialCharTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-88-1} \end{center}

\hypertarget{goodness-of-fit-measures}{%
\paragraph{Goodness-of-fit Measures}\label{goodness-of-fit-measures}}

Next, we extract several other goodness-of-fit from each of the three candidate models and summarize them in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{select}\OtherTok{=}\ControlFlowTok{function}\NormalTok{(m)\{ }\CommentTok{\# m is an object: model}
\NormalTok{ e }\OtherTok{=}\NormalTok{ m}\SpecialCharTok{$}\NormalTok{resid                           }\CommentTok{\# residuals}
\NormalTok{ n0 }\OtherTok{=} \FunctionTok{length}\NormalTok{(e)                        }\CommentTok{\# sample size}
\NormalTok{ SSE}\OtherTok{=}\NormalTok{(m}\SpecialCharTok{$}\NormalTok{df)}\SpecialCharTok{*}\NormalTok{(}\FunctionTok{summary}\NormalTok{(m)}\SpecialCharTok{$}\NormalTok{sigma)}\SpecialCharTok{\^{}}\DecValTok{2}       \CommentTok{\# sum of squared error}
\NormalTok{ R.sq}\OtherTok{=}\FunctionTok{summary}\NormalTok{(m)}\SpecialCharTok{$}\NormalTok{r.squared             }\CommentTok{\# Coefficient of determination: R square!}
\NormalTok{ R.adj}\OtherTok{=}\FunctionTok{summary}\NormalTok{(m)}\SpecialCharTok{$}\NormalTok{adj.r                }\CommentTok{\# Adjusted R square}
\NormalTok{ MSE}\OtherTok{=}\NormalTok{(}\FunctionTok{summary}\NormalTok{(m)}\SpecialCharTok{$}\NormalTok{sigma)}\SpecialCharTok{\^{}}\DecValTok{2}              \CommentTok{\# square error}
\NormalTok{ Cp}\OtherTok{=}\NormalTok{(SSE}\SpecialCharTok{/}\NormalTok{MSE)}\SpecialCharTok{{-}}\NormalTok{(n0}\DecValTok{{-}2}\SpecialCharTok{*}\NormalTok{(n0}\SpecialCharTok{{-}}\NormalTok{m}\SpecialCharTok{$}\NormalTok{df))         }\CommentTok{\# Mellow\textquotesingle{}s p}
\NormalTok{ AIC}\OtherTok{=}\NormalTok{n0}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(SSE)}\SpecialCharTok{{-}}\NormalTok{n0}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(n0)}\SpecialCharTok{+}\DecValTok{2}\SpecialCharTok{*}\NormalTok{(n0}\SpecialCharTok{{-}}\NormalTok{m}\SpecialCharTok{$}\NormalTok{df)          }\CommentTok{\# Akaike information criterion}
\NormalTok{ SBC}\OtherTok{=}\NormalTok{n0}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(SSE)}\SpecialCharTok{{-}}\NormalTok{n0}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(n0)}\SpecialCharTok{+}\NormalTok{(}\FunctionTok{log}\NormalTok{(n0))}\SpecialCharTok{*}\NormalTok{(n0}\SpecialCharTok{{-}}\NormalTok{m}\SpecialCharTok{$}\NormalTok{df)  }\CommentTok{\# Schwarz Bayesian Information criterion}
\NormalTok{ X}\OtherTok{=}\FunctionTok{model.matrix}\NormalTok{(m)                     }\CommentTok{\# design matrix of the model}
\NormalTok{ H}\OtherTok{=}\NormalTok{X}\SpecialCharTok{\%*\%}\FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{X)}\SpecialCharTok{\%*\%}\FunctionTok{t}\NormalTok{(X)          }\CommentTok{\# hat matrix}
\NormalTok{ d}\OtherTok{=}\NormalTok{e}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{diag}\NormalTok{(H))                       }
\NormalTok{ PRESS}\OtherTok{=}\FunctionTok{t}\NormalTok{(d)}\SpecialCharTok{\%*\%}\NormalTok{d   }\CommentTok{\# predicted residual error sum of squares (PRESS){-} a cross{-}validation measure}
\NormalTok{ tbl }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{SSE=}\NormalTok{SSE, }\AttributeTok{R.sq=}\NormalTok{R.sq, }\AttributeTok{R.adj =}\NormalTok{ R.adj, }\AttributeTok{Cp =}\NormalTok{ Cp, }\AttributeTok{AIC =}\NormalTok{ AIC, }\AttributeTok{SBC =}\NormalTok{ SBC, }\AttributeTok{PRD =}\NormalTok{ PRESS))}
 \FunctionTok{names}\NormalTok{(tbl)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"SSE"}\NormalTok{, }\StringTok{"R.sq"}\NormalTok{, }\StringTok{"R.adj"}\NormalTok{, }\StringTok{"Cp"}\NormalTok{, }\StringTok{"AIC"}\NormalTok{, }\StringTok{"SBC"}\NormalTok{, }\StringTok{"PRESS"}\NormalTok{)}
\NormalTok{ tbl}
\NormalTok{ \}}
\end{Highlighting}
\end{Shaded}

\textbf{\color{red}CAUTION}: \emph{\color{red}SSE, AIC, SBC, and PRESS are dependent on the magnitude of the response. \(R^2\), \(R^2_{adj}\), and \(C_p\) are scaled (index-like measure) and therefore independent on the magnitude of Response. This means that models with transformed response and those with the original magnitude can NOT be compared using SSE, AIC, SBC, and PRESS!}

\textbf{\color{blue}In the following table, three models have different magnitudes due to different transformations, the valid goodness of fit measures are \(R^2\), \(R^2_{adj}\), and \(C_p\)!}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{output.sum }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(}\FunctionTok{select}\NormalTok{(full.model), }\FunctionTok{select}\NormalTok{(sqrt.price.log.dist), }\FunctionTok{select}\NormalTok{(log.price))}
\FunctionTok{row.names}\NormalTok{(output.sum) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"full.model"}\NormalTok{, }\StringTok{"sqrt.price.log.dist"}\NormalTok{, }\StringTok{"log.price"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(output.sum, }\AttributeTok{caption =} \StringTok{"Goodness{-}of{-}fit Measures of Candidate Models"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-90}Goodness-of-fit Measures of Candidate Models}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r}
\hline
  & SSE & R.sq & R.adj & Cp & AIC & SBC & PRESS\\
\hline
full.model & 31831.19255 & 0.5836958 & 0.5785940 & 6 & 1809.7271 & 1833.8823 & 32792.58816\\
\hline
sqrt.price.log.dist & 177.56943 & 0.6587132 & 0.6545308 & 6 & -338.4528 & -314.2976 & 182.95839\\
\hline
log.price & 21.29945 & 0.6651882 & 0.6610851 & 6 & -1216.4146 & -1192.2594 & 21.94809\\
\hline
\end{tabular}
\end{table}

We can see from the above table that the goodness-of-fit measures of the third model are unanimously better than the other two models. Considering the interpretability, goodness-of-fit, and simplicity, we choose the last model as the final model.

\hypertarget{final-model}{%
\subsubsection{Final Model}\label{final-model}}

The inferential statistics of the final working model are summarized in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(log.price)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =} \StringTok{"Inferential Statistics of Final Model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-91}Inferential Statistics of Final Model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 3.5724282 & 0.0443235 & 80.599030 & 0.0000000\\
\hline
HouseAge & -0.0075712 & 0.0010097 & -7.498507 & 0.0000000\\
\hline
NumConvenStores & 0.0274872 & 0.0049096 & 5.598667 & 0.0000000\\
\hline
sale.year2013 & 0.0805519 & 0.0244373 & 3.296272 & 0.0010655\\
\hline
Dist2MRT.kilo & -0.1445122 & 0.0137541 & -10.506820 & 0.0000000\\
\hline
geo.groupTRUE & 0.1825871 & 0.0347151 & 5.259583 & 0.0000002\\
\hline
\end{tabular}
\end{table}

Since the sample size (414) is large, the argument for validating p-values is the Central Limit Theorem (CLT). all p-values are close to 0 meaning that all coefficients are significantly different from 0.

In this specific case study, there is no need to perform variable selection to determine the final model.

\hypertarget{summary-of-the-model}{%
\subsection{Summary of the model}\label{summary-of-the-model}}

We can explicitly write the final model in the following

\[
\log(price) =3.5723 - 0.0076\times HouseAge +0.0275\times NumConvenStores + \]\[   0.0805\times Sale.year2013 - 0.1445\times Dist2MRT.kilo + 0.1826\times geo.groupTRUE
\]

Note that the estimated regression coefficients are based on log price. we now consider \textbf{the set of} all houses with ages \(x_0\) and \(x_0 + 1\) that are in the same conditions except for the sale prices. The exact practical interpretation is given below. Let \(p_{x_0}\) and \(p_{x_0+1}\) be the mean prices of houses with ages \(x_0\) and \(x_0 + 1\), respectively. Since the two types of houses are in the same conditions except for the age and the prices. Then

\[
\log (p_{x_0+1}) - \log(p_{x_0}) = -0.0076  \to \log(p_{x_0+1}/p_{x_0}) = -0.0076 \to p_{x_0+1} = 0.9924p_{x_0}
\]
We re-express the above equation can be re-written as

\[
p_{x_0+1} - p_{x_0} = -0.0076p_{x_0} \to \frac{p_{x_0+1}-p_{x_0}}{p_{x_0}} = -0.076 = -0.76\%
\]

That is, as the house age increases by one year, the house price \textbf{decreases} by 0.76\%. We can similarly interpret other regression coefficients.

The distance to the nearest MRT is also negatively associated with the sale price. The rest of the factors are positively associated with house prices.

\hypertarget{discussions}{%
\subsection{Discussions}\label{discussions}}

We use various regression techniques such as Box-Cox transformation for response variables and other transformations of the explanatory variables to search for the final model in the case study. Since there are five variables in the data set and all are significant, we did not perform any variable selection procedure.

All candidate models have the same number of variables. We use commonly-used global goodness-of-fit measures as model selection criteria.

The interpretation of the regression coefficients is not straightforward since the response variable was transformed into a log scale. We used some algebra to derive the practical interpretation of the regression coefficients associated with the variables at their original scales.

The violation of the normal assumption of the residuals remains uncorrected. The inference on the regression coefficients is based on the central limit theorem. We will introduce bootstrap methods to construct bootstrap confidence intervals of the regression coefficients of the \textbf{final model}.

\hypertarget{written-assignment}{%
\section{Written Assignment}\label{written-assignment}}

This assignment focuses on the multiple regression model using various techniques you learned from your previous courses. You will use the data set you selected last week. That data set will also be used for the next assignment that is based on this week's report.

Please study the first three sections of the class note. Your data analysis and write-up should be similar to what did in the case study in section 4 of the note. In fact, the case study of this chapter can be considered a standalone statistical report.

Your analysis and write-up should have all components in the case study of this chapter.

\begin{itemize}
\tightlist
\item
  Description of your data set
\item
  What are the research questions
\item
  Exploratory analysis of the data set and prepare the analytic data for the regression

  \begin{itemize}
  \tightlist
  \item
    create new variables based on existing ones?
  \item
    drop some irrelevant variables based on your judgment.
  \end{itemize}
\item
  Initial full model with all relevant variables and conduct residual diagnostic

  \begin{itemize}
  \tightlist
  \item
    special patterns in residual plots?
  \item
    violation of model assumptions?
  \end{itemize}
\item
  Need to transform the response variable with Box-Cox?
\item
  Want to transform explanatory variables to improve goodness-of-fit measures?

  \begin{itemize}
  \tightlist
  \item
    Please feel free to use my code to extract the goodness-of-fit measures. If you forgot the meaning of the goodness-of-fit measures, please check your old textbook or the eBook that I suggested on the course web page.
  \item
    Several packages have a Box-Cox transformation function. The one that I used in the case study is from the library \{MASS\}. You can check the help document if you are sure how to use it.
  \end{itemize}
\item
  Build several candidate models and then select the best one as your model.
\item
  Summarize the output including residual diagnostic plots
\item
  Interpret the regression coefficient as I did in the case study.
\item
  Conclusions and discussion.
\end{itemize}

\hypertarget{bootstrapping-mlr-model}{%
\chapter{Bootstrapping MLR Model}\label{bootstrapping-mlr-model}}

In this note, we introduce two versions of bootstrap procedures to generate bootstrap samples to estimate the confidence intervals of the coefficients of the regression model identified in the previous note. We first fit the linear model to the data and then use bootstrap methods to construct the confidence intervals of regression coefficients.

\hypertarget{bootstrap-method-revisited}{%
\section{Bootstrap Method Revisited}\label{bootstrap-method-revisited}}

The \textbf{bootstrap} is one of the computationally intensive techniques that is now part of the broad umbrella of \textbf{nonparametric statistics} that are commonly called \textbf{resampling methods}.

We have used this method to construct the confidence interval for a unknown population mean and correlation coefficient of two numerical variables (populations). We also fit bootstrap simple linear regression in two different ways and will use it to build bootstrap MLR.

Based on the limited experience using the bootstrap procedure we had, it is time to look into the resampling technique a little more closer to how and why it works. We will also discuss the implicit assumptions for bootstrap resampling procedures. For ease of illustration, we need to review some concepts learned earlier: probability density function (\texttt{pdf}) and cumulative distribution function (\texttt{CDF}).

\hypertarget{pdf-vs-cdf}{%
\subsection{\texorpdfstring{\texttt{pdf} vs \texttt{CDF}}{pdf vs CDF}}\label{pdf-vs-cdf}}

Let \(f(x)\) denote the density function of the distribution of the population of interest. The corresponding cumulative distribution is given by

\[
F(x) = \int_{-\infty}^x f(y)dy
\]

That is, \(F^{\prime}(x) = f(x)\). The following animated graph illustrates the relationship geometrically.

\url{https://github.com/pengdsci/STA504/blob/main/topic03/w04-PDFvsCDF.gif?raw=true}

\hypertarget{comparing-two-distributions}{%
\subsection{Comparing Two Distributions}\label{comparing-two-distributions}}

Next, we use two examples to show how to do visual comparison between distributions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{x }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{, }\AttributeTok{length =} \DecValTok{200}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(x, }\FunctionTok{dnorm}\NormalTok{(x,}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{""}\NormalTok{, }\AttributeTok{ylab =} \StringTok{""}\NormalTok{,}
        \AttributeTok{main =} \StringTok{"Comparing Two Normal Density Curves"}\NormalTok{,}
        \AttributeTok{cex.main =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, }\FunctionTok{dnorm}\NormalTok{(x, }\DecValTok{7}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lty =}\DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"N(5,1)"}\NormalTok{, }\StringTok{"N(7,2)"}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\AttributeTok{lty=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }
                   \AttributeTok{lwd =} \FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\DocumentationTok{\#\#\#}
\NormalTok{dist }\OtherTok{=} \FunctionTok{abs}\NormalTok{(}\FunctionTok{pnorm}\NormalTok{(x, }\DecValTok{7}\NormalTok{, }\DecValTok{2}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(x,}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{max.id }\OtherTok{=} \FunctionTok{which}\NormalTok{(dist }\SpecialCharTok{==} \FunctionTok{max}\NormalTok{(dist))}
\FunctionTok{plot}\NormalTok{(x, }\FunctionTok{pnorm}\NormalTok{(x,}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{""}\NormalTok{, }\AttributeTok{ylab =} \StringTok{""}\NormalTok{, }
        \AttributeTok{main =} \StringTok{"Comparing Two Normal CDF Curves"}\NormalTok{,}
        \AttributeTok{cex.main =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, }\FunctionTok{pnorm}\NormalTok{(x, }\DecValTok{7}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{segments}\NormalTok{(x[}\DecValTok{63}\NormalTok{],}\FunctionTok{pnorm}\NormalTok{(x, }\DecValTok{7}\NormalTok{, }\DecValTok{2}\NormalTok{)[}\DecValTok{63}\NormalTok{], x[}\DecValTok{63}\NormalTok{], }\FunctionTok{pnorm}\NormalTok{(x,}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{)[}\DecValTok{63}\NormalTok{], }\AttributeTok{col =} \StringTok{"purple"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{rep}\NormalTok{(x[}\DecValTok{63}\NormalTok{],}\DecValTok{2}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\FunctionTok{pnorm}\NormalTok{(x, }\DecValTok{7}\NormalTok{, }\DecValTok{2}\NormalTok{)[}\DecValTok{63}\NormalTok{], }\FunctionTok{pnorm}\NormalTok{(x,}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{)[}\DecValTok{63}\NormalTok{]), }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{col =} \StringTok{"purple"}\NormalTok{)}
\DocumentationTok{\#\#\#}
\FunctionTok{legend}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"N(5,1)"}\NormalTok{, }\StringTok{"N(7,2)"}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\AttributeTok{lty=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }
                   \AttributeTok{lwd =} \FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-93-1} 

}

\caption{Comparison between two normal distributions}\label{fig:unnamed-chunk-93}
\end{figure}

We can see from the above figure that the \textbf{maximum distance between the two CDF curves} (the length of the purple line segment) measures that discrepancy the two distributions.

\hypertarget{empirical-distribution-and-cdf}{%
\subsection{Empirical Distribution and CDF}\label{empirical-distribution-and-cdf}}

An empirical distribution is a non-parametric estimation of a theoretical distribution based on a random sample taken from the population. Assume that \(\{x_1, x_2, x_3, \cdots, x_n \}\) is a random sample taken from a population with CDF \(F(x|\theta)\) (\(\theta\) is the population parameter). **The empirical distribution of \(F(x|\theta)\) based on the given random sample is defined to be

\[
F_n(x) = \frac{\text{number of values in random sample } \le x}{n}
\]

For example, let's look at the emprical distribution based on the toy data set \(\{ 1, 1.4, 2.1, 2.5, 5\}\).

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-94-1} \end{center}

\hypertarget{relationship-between-fx-and-f_nx}{%
\subsection{\texorpdfstring{Relationship between \(F(x)\) and \(F_n(X)\)}{Relationship between F(x) and F\_n(X)}}\label{relationship-between-fx-and-f_nx}}

As stated earlier, An empirical distribution, \(F_n(x)\), is used to estimate the theoretical CDF, \(F(x)\). Next, we use several examples to visually check the goodness of the estimation.

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-95-1} \end{center}

We can see two general but important facts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The goodness of the estimation is dependent on the sample size. The bigger the sample size, the more accurate the estimation.
\item
  With the same size, the goodness of estimation is dependent on the population distribution.
\end{enumerate}

\hypertarget{why-bootstrap-procedure-works}{%
\subsection{Why Bootstrap Procedure Works?}\label{why-bootstrap-procedure-works}}

For a given population, if the sample size is large enough so that the empirical distribution is close to the theoretical distribution, we can take bootstrap samples for non-parametric inference. The following chart explains how bootstrap method works.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img05/w05-HowBootstrapWorks} 

}

\caption{How bootstrap procedure works.}\label{fig:unnamed-chunk-96}
\end{figure}

The above

\hypertarget{a-bigger-picture}{%
\subsection{A Bigger Picture}\label{a-bigger-picture}}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img05/w05-Bootstrap-Monte-Carlo} 

}

\caption{Monte Carlo, Bootstrap, parametric, and asymptoic confidence intervals}\label{fig:unnamed-chunk-97}
\end{figure}

\hfill\break

\hypertarget{other-resampling-methods}{%
\section{Other Resampling Methods}\label{other-resampling-methods}}

In the last module, we listed several goodness-of-fit measures. The \textbf{predicted residual error sum of squares (PRESS)} is one of them. It definition is not clearly given. Before we discuss this measure in little bit more details, we first introduce a popular resampling method - Jackknife.

\hypertarget{jackknif-resampling}{%
\subsection{Jackknif Resampling}\label{jackknif-resampling}}

The jackknife estimator of a parameter is found by systematically \textbf{leaving out each observation from a dataset} and \textbf{calculating the parameter estimate over the remaining observations} and \textbf{then aggregating these calculations}. It was developed by Maurice Quenouille in 1949 and refined in 1956. John Tukey expanded on the technique in 1958 and name the method ``jackknife'' because it is handy like a physical jack-knife. Jackknife resampling can improvise a solution for many specific problems more efficiently than some purpose-designed tools.

The basic leave-one-out Jackknife resampling method is illustrated in the following chart.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{img05/w05-JackknifeResampling} 

}

\caption{Illustration of leave-one-out Jackknife resampling method}\label{fig:unnamed-chunk-98}
\end{figure}

\hypertarget{jackknife-for-predictive-sum-of-squares-press}{%
\subsection{Jackknife for Predictive Sum of Squares (PRESS)}\label{jackknife-for-predictive-sum-of-squares-press}}

A fitted model having been produced, each observation in turn is removed and the model is refitted using the remaining observations. The out-of-sample predicted value is calculated for the omitted observation in each case, and the PRESS statistic is calculated as the sum of the squares of all the resulting prediction errors.

\[
\text{PRESS}=\sum _{{i=1}}^{n}(y_{i}-{\hat  {y}}_{{i,-i}})^{2}
\]

The PRESS statistic can be calculated for a number of candidate models for the same dataset, with the lowest values of PRESS indicating the best model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{realestate }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww03/w03{-}Realestate.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{unitPrice }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{PriceUnitArea}
\NormalTok{HouseAge }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{HouseAge}
\NormalTok{nn }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(HouseAge)}
\NormalTok{JK.pred }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nn)\{}
\NormalTok{  unitPrice0 }\OtherTok{\textless{}{-}}\NormalTok{ unitPrice[}\SpecialCharTok{{-}}\NormalTok{i]}
\NormalTok{  HouseAge0 }\OtherTok{\textless{}{-}}\NormalTok{ HouseAge[}\SpecialCharTok{{-}}\NormalTok{i]}
\NormalTok{  mm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(unitPrice0 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ HouseAge0)}
\NormalTok{  JK.pred[i] }\OtherTok{=} \FunctionTok{as.vector}\NormalTok{(}\FunctionTok{predict}\NormalTok{(mm, }\AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{HouseAge0 =}\NormalTok{ HouseAge[i])))}
\NormalTok{\}}
\NormalTok{PRESS }\OtherTok{=} \FunctionTok{sum}\NormalTok{((unitPrice}\SpecialCharTok{{-}}\NormalTok{JK.pred)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{PRESS}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 73815.25
\end{verbatim}

\hypertarget{some-comments-on-jackknife}{%
\subsection{Some Comments on Jackknife}\label{some-comments-on-jackknife}}

\begin{itemize}
\item
  Jackknife samples can also be used for estimating population parameters just like what we did in bootstrap methods, but the procedure is not as straightforward as that of bootstrap. We will not go to details in this direction.
\item
  The logic of Jackknife sampling is used to define various data-driven methods that used in modern statistics, data science and machine learning. One of such methods is \textbf{cross-validation}. We will introduce this method in the second part this course - generalized linear regression.
\end{itemize}

/

\hypertarget{parametric-linear-regression-model}{%
\section{Parametric Linear Regression Model}\label{parametric-linear-regression-model}}

For reference, we copy the code from the last note that creates the analytic data set for the final model and the summarized statistics of the model as well.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\NormalTok{housingURL }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww03/w03{-}Realestate.csv"}
\NormalTok{realestate0 }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(housingURL , }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{realestate }\OtherTok{\textless{}{-}}\NormalTok{ realestate0[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\CommentTok{\# longitude and latitude will be used to make a map in the upcoming analysis.}
\NormalTok{lat }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{Latitude}
\NormalTok{lon }\OtherTok{\textless{}{-}}\NormalTok{ realestate}\SpecialCharTok{$}\NormalTok{Longitude}
\DocumentationTok{\#\# define the geo.group variable}
\DocumentationTok{\#\# top{-}right region = TRUE, other region = FALSE}
\NormalTok{geo.group }\OtherTok{\textless{}{-}}\NormalTok{ (lon }\SpecialCharTok{\textgreater{}} \FloatTok{121.529}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ (lat }\SpecialCharTok{\textgreater{}} \FloatTok{24.96}\NormalTok{)     }
\CommentTok{\# convert the logical values to character values.}
\NormalTok{realestate}\SpecialCharTok{$}\NormalTok{geo.group }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(geo.group)  }
\CommentTok{\# convert transaction year to dummy.}
\NormalTok{realestate}\SpecialCharTok{$}\NormalTok{sale.year }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(realestate}\SpecialCharTok{$}\NormalTok{TransactionYear) }
\CommentTok{\# re{-}scale distance: foot {-}\textgreater{} kilo feet}
\NormalTok{realestate}\SpecialCharTok{$}\NormalTok{Dist2MRT.kilo }\OtherTok{\textless{}{-}}\NormalTok{ (realestate}\SpecialCharTok{$}\NormalTok{Distance2MRT)}\SpecialCharTok{/}\DecValTok{1000} 
\CommentTok{\# keep only variables to be used in the candidate models}
\NormalTok{final.data }\OtherTok{=}\NormalTok{ realestate[, }\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{)]          }
\NormalTok{final.data}\SpecialCharTok{$}\NormalTok{logAreaPrice }\OtherTok{=} \FunctionTok{log}\NormalTok{(final.data}\SpecialCharTok{$}\NormalTok{PriceUnitArea)  }\CommentTok{\# }
\DocumentationTok{\#\# the final model}
\NormalTok{log.price }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(PriceUnitArea) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ HouseAge }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}  
\NormalTok{                 Dist2MRT.kilo  }\SpecialCharTok{+}\NormalTok{ geo.group, }\AttributeTok{data =}\NormalTok{ final.data)}
\NormalTok{log.price02 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(logAreaPrice }\SpecialCharTok{\textasciitilde{}}\NormalTok{ HouseAge }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}  
\NormalTok{                 Dist2MRT.kilo  }\SpecialCharTok{+}\NormalTok{ geo.group, }\AttributeTok{data =}\NormalTok{ final.data)}
\NormalTok{cmtrx }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{summary}\NormalTok{(log.price)}\SpecialCharTok{$}\NormalTok{coef,}\DecValTok{4}\NormalTok{)}
\NormalTok{cmtrx02 }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{summary}\NormalTok{(log.price02)}\SpecialCharTok{$}\NormalTok{coef)}
\FunctionTok{kable}\NormalTok{(cmtrx, }\AttributeTok{caption =} \StringTok{"Inferential Statistics of Final Model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-100}Inferential Statistics of Final Model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 3.5724 & 0.0443 & 80.5990 & 0.0000\\
\hline
HouseAge & -0.0076 & 0.0010 & -7.4985 & 0.0000\\
\hline
NumConvenStores & 0.0275 & 0.0049 & 5.5987 & 0.0000\\
\hline
sale.year2013 & 0.0806 & 0.0244 & 3.2963 & 0.0011\\
\hline
Dist2MRT.kilo & -0.1445 & 0.0138 & -10.5068 & 0.0000\\
\hline
geo.groupTRUE & 0.1826 & 0.0347 & 5.2596 & 0.0000\\
\hline
\end{tabular}
\end{table}

The explicit expression of the final model is given by

\[
\log(price) =3.5723 - 0.0076\times HouseAge +0.0275\times NumConvenStores + 
\]
\[   
0.0805\times Sale.year2013 - 0.1445\times Dist2MRT.kilo + 0.1826\times geo.groupTRUE
\]

As another example of the interpretation of the regression coefficient, we choose the coefficient associated with \textbf{geo.group}. In the output, you see the name of the dummy variable with the suffix \texttt{TRUE}, \textbf{geo.groupTRUE}. The suffix \textbf{TRUE} indicates that the dummy variable represents the category `TRUE' of the category variable \textbf{geo.group}. The associated coefficient reflects the \textbf{mean} difference between the category \texttt{TRUE} and the baseline category \texttt{FALSE}. In R, the default baseline category is the lowest value of the categorical variable (in alphabetical order).

Let's consider \textbf{the set of all houses} that are in the same conditions except the regions (region \texttt{TRUE} and region \texttt{FALSE} ) and the sale prices.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img05/w05-map} 

}

\caption{Location of Houses for Sale}\label{fig:unnamed-chunk-101}
\end{figure}

\hfill\break

\hypertarget{bootstrap-cases-boot.c}{%
\section{Bootstrap Cases (BOOT.C)}\label{bootstrap-cases-boot.c}}

In this section, we use bootstrapping cases to find the confidence intervals for the coefficients in the final regression model. The method was used in bootstrap simple linear regression (SLR) in week \#3. The following code finds the confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log.price }\OtherTok{=} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(PriceUnitArea) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ HouseAge }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}  
\NormalTok{                 Dist2MRT.kilo  }\SpecialCharTok{+}\NormalTok{ geo.group, }\AttributeTok{data =}\NormalTok{ final.data)}
\DocumentationTok{\#\#}
\NormalTok{B }\OtherTok{=} \DecValTok{1000}    \CommentTok{\# choose the number of bootstrap replicates.}
\DocumentationTok{\#\# }
\NormalTok{num.p }\OtherTok{=} \FunctionTok{dim}\NormalTok{(}\FunctionTok{model.frame}\NormalTok{(log.price))[}\DecValTok{2}\NormalTok{]  }\CommentTok{\# returns number of parameters }
\NormalTok{smpl.n }\OtherTok{=} \FunctionTok{dim}\NormalTok{(}\FunctionTok{model.frame}\NormalTok{(log.price))[}\DecValTok{1}\NormalTok{] }\CommentTok{\# sample size}
\DocumentationTok{\#\# zero matrix to store bootstrap coefficients }
\NormalTok{coef.mtrx }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, B}\SpecialCharTok{*}\NormalTok{num.p), }\AttributeTok{ncol =}\NormalTok{ num.p)       }
\DocumentationTok{\#\# }
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B)\{}
\NormalTok{  bootc.id }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{smpl.n, smpl.n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{) }
  \CommentTok{\# fit final model to the bootstrap sample}
\NormalTok{  log.price.btc }\OtherTok{=} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(PriceUnitArea) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ HouseAge }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}  
\NormalTok{                       Dist2MRT.kilo  }\SpecialCharTok{+}\NormalTok{ geo.group, }\AttributeTok{data =}\NormalTok{ final.data[bootc.id,])  }
  \CommentTok{\# extract coefs from bootstrap regression model  }
\NormalTok{  coef.mtrx[i,] }\OtherTok{=} \FunctionTok{coef}\NormalTok{(log.price.btc)      }
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We define an R function to make histograms of the bootstrap regression coefficients in the following. I will also use this function to make histograms for the residual bootstrap estimated regression coefficients as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot.hist }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(cmtrx, bt.coef.mtrx, var.id, var.nm)\{}
  \DocumentationTok{\#\# bt.coef.mtrx = matrix for storing bootstrap estimates of coefficients}
  \DocumentationTok{\#\# var.id = variable ID (1, 2, ..., k+1)}
  \DocumentationTok{\#\# var.nm = variable name on the hist title, }
  \DocumentationTok{\#\# must be the string in the double quotes}
  \DocumentationTok{\#\# coefficient matrix of the final model}
  \DocumentationTok{\#\# Bootstrap sampling distribution of the estimated coefficients}
\NormalTok{  x1}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FunctionTok{min}\NormalTok{(bt.coef.mtrx[,var.id]), }\FunctionTok{max}\NormalTok{(bt.coef.mtrx[,var.id]), }\AttributeTok{length=}\DecValTok{300}\NormalTok{ )}
\NormalTok{  y1}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{dnorm}\NormalTok{(x1}\FloatTok{.1}\NormalTok{, }\FunctionTok{mean}\NormalTok{(bt.coef.mtrx[,var.id]), }\FunctionTok{sd}\NormalTok{(bt.coef.mtrx[,var.id]))}
  \CommentTok{\# height of the histogram {-} use it to make a nice{-}looking histogram.}
\NormalTok{  highestbar }\OtherTok{=} \FunctionTok{max}\NormalTok{(}\FunctionTok{hist}\NormalTok{(bt.coef.mtrx[,var.id], }\AttributeTok{plot =} \ConstantTok{FALSE}\NormalTok{)}\SpecialCharTok{$}\NormalTok{density) }
\NormalTok{  ylimit }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(}\FunctionTok{c}\NormalTok{(y1}\FloatTok{.1}\NormalTok{,highestbar))}
  \FunctionTok{hist}\NormalTok{(bt.coef.mtrx[,var.id], }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{main =}\NormalTok{ var.nm, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }
       \AttributeTok{col =} \StringTok{"azure1"}\NormalTok{,}\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,ylimit), }\AttributeTok{border=}\StringTok{"lightseagreen"}\NormalTok{)}
  \FunctionTok{lines}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x1}\FloatTok{.1}\NormalTok{, }\AttributeTok{y =}\NormalTok{ y1}\FloatTok{.1}\NormalTok{, }\AttributeTok{col =} \StringTok{"red3"}\NormalTok{)}
  \FunctionTok{lines}\NormalTok{(}\FunctionTok{density}\NormalTok{(bt.coef.mtrx[,var.id], }\AttributeTok{adjust=}\DecValTok{2}\NormalTok{), }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{) }
  \CommentTok{\#legend("topright", c(""))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The following histograms of the bootstrap estimates of regression coefficients represent the sampling distributions of the corresponding estimates in the final model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))  }\CommentTok{\# histograms of bootstrap coefs}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{coef.mtrx, }\AttributeTok{var.id=}\DecValTok{1}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"Intercept"}\NormalTok{ )}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{coef.mtrx, }\AttributeTok{var.id=}\DecValTok{2}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"House Age"}\NormalTok{ )}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{coef.mtrx, }\AttributeTok{var.id=}\DecValTok{3}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"Num Conven Stores"}\NormalTok{ )}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{coef.mtrx, }\AttributeTok{var.id=}\DecValTok{4}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"Year Sold"}\NormalTok{ )}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{coef.mtrx, }\AttributeTok{var.id=}\DecValTok{5}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"Dist to MRT"}\NormalTok{ )}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{coef.mtrx, }\AttributeTok{var.id=}\DecValTok{6}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"Region"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-104-1} 

}

\caption{(Case) Bootstrap confidence intervals of regression coefficients}\label{fig:unnamed-chunk-104}
\end{figure}

Two normal-density curves were placed on each of the histograms.

\begin{itemize}
\item
  The red \textbf{density curve} uses the estimated regression coefficients and their corresponding standard error in the output of the regression procedure. The p-values reported in the output are based on the red curve.
\item
  The \textbf{blue curve} is a non-parametric data-driven estimate of the density of bootstrap sampling distribution. The bootstrap confidence intervals of the regressions are based on these non-parametric bootstrap sampling distributions.
\end{itemize}

We can see from the above histograms that the two density curves in all histograms are close to each other. we would expect that significance test results and the corresponding bootstrap confidence intervals are consistent. Next, we find 95\% bootstrap confidence intervals of each regression coefficient and combined them with the output of the final model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num.p }\OtherTok{=} \FunctionTok{dim}\NormalTok{(coef.mtrx)[}\DecValTok{2}\NormalTok{]  }\CommentTok{\# number of parameters}
\NormalTok{btc.ci }\OtherTok{=} \ConstantTok{NULL}
\NormalTok{btc.wd }\OtherTok{=} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{num.p)\{}
\NormalTok{  lci}\FloatTok{.025} \OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{quantile}\NormalTok{(coef.mtrx[, i], }\FloatTok{0.025}\NormalTok{, }\AttributeTok{type =} \DecValTok{2}\NormalTok{),}\DecValTok{8}\NormalTok{)}
\NormalTok{  uci}\FloatTok{.975} \OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{quantile}\NormalTok{(coef.mtrx[, i],}\FloatTok{0.975}\NormalTok{, }\AttributeTok{type =} \DecValTok{2}\NormalTok{ ),}\DecValTok{8}\NormalTok{)}
\NormalTok{  btc.wd[i] }\OtherTok{=}\NormalTok{  uci}\FloatTok{.975} \SpecialCharTok{{-}}\NormalTok{ lci}\FloatTok{.025}
\NormalTok{  btc.ci[i] }\OtherTok{=} \FunctionTok{paste}\NormalTok{(}\StringTok{"     ["}\NormalTok{, }\FunctionTok{round}\NormalTok{(lci}\FloatTok{.025}\NormalTok{,}\DecValTok{4}\NormalTok{),}\StringTok{", "}\NormalTok{, }\FunctionTok{round}\NormalTok{(uci}\FloatTok{.975}\NormalTok{,}\DecValTok{4}\NormalTok{),}\StringTok{"]"}\NormalTok{)}
\NormalTok{ \}}
\CommentTok{\#btci.95 = as.data.frame(btc.ci)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(cmtrx, btc.ci), }
      \AttributeTok{caption =} \StringTok{"Regression Coefficient Matrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-105}Regression Coefficient Matrix}
\centering
\begin{tabular}[t]{l|l|l|l|l|l}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|) & btc.ci\\
\hline
(Intercept) & 3.5724 & 0.0443 & 80.599 & 0 & [ 3.4742 ,  3.6668 ]\\
\hline
HouseAge & -0.0076 & 0.001 & -7.4985 & 0 & [ -0.0097 ,  -0.0056 ]\\
\hline
NumConvenStores & 0.0275 & 0.0049 & 5.5987 & 0 & [ 0.017 ,  0.0379 ]\\
\hline
sale.year2013 & 0.0806 & 0.0244 & 3.2963 & 0.0011 & [ 0.0361 ,  0.125 ]\\
\hline
Dist2MRT.kilo & -0.1445 & 0.0138 & -10.5068 & 0 & [ -0.176 ,  -0.1148 ]\\
\hline
geo.groupTRUE & 0.1826 & 0.0347 & 5.2596 & 0 & [ 0.1175 ,  0.2489 ]\\
\hline
\end{tabular}
\end{table}

We can see from the above table of summarized statistics, the significance tests of regression coefficients based on the p-values and the corresponding 95\% confidence intervals are consistent.

\hypertarget{bootstrap-residuals-boot.r}{%
\section{Bootstrap Residuals (BOOT.R)}\label{bootstrap-residuals-boot.r}}

In this section, we introduce bootstrap residual methods to construct bootstrap confidence intervals. The idea is straightforward and is summarized in the following.

\begin{center}\includegraphics[width=1\linewidth]{img05/w05-BootsResiduals} \end{center}

\hypertarget{fitted-model}{%
\subsection{Fitted Model}\label{fitted-model}}

Assume that the fitted regression model is given by

\[
\begin{array}{ccc} 
y_1 & = &  \hat{\beta}_0 + \hat{\beta}_1 x_{11} + \hat{\beta}_2 x_{12} + \cdots + \hat{\beta}_k x_{1k} + e_1  \\
y_2 & = &  \hat{\beta}_0 + \hat{\beta}_1 x_{21} + \hat{\beta}_2 x_{22} + \cdots + \hat{\beta}_k x_{2k} + e_2  \\
y_3 & = &  \hat{\beta}_0 + \hat{\beta}_1 x_{31} + \hat{\beta}_2 x_{32} + \cdots + \hat{\beta}_k x_{3k} + e_3  \\
\vdots & \vdots & \vdots \\
y_n & = &  \hat{\beta}_0 + \hat{\beta}_1 x_{n1} + \hat{\beta}_2 x_{n2} + \cdots + \hat{\beta}_k x_{nk} + e_n
\end{array}
\]

where \(\{e_1, e_2, \cdots, e_n \}\) is the set of residuals obtained from the final model. \(\{x_{i1}, x_{i2}, \cdots, x_{ik} \}\) is the i-th record from the data, and \(\{ \hat{\beta}_0, \hat{\beta}_1, \cdots, \hat{\beta}_k \}\)

The distribution of the residuals is depicted in the following histogram.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{sort}\NormalTok{(log.price}\SpecialCharTok{$}\NormalTok{residuals),}\AttributeTok{n=}\DecValTok{40}\NormalTok{,}
     \AttributeTok{xlab=}\StringTok{"Residuals"}\NormalTok{,}
     \AttributeTok{col =} \StringTok{"lightblue"}\NormalTok{,}
     \AttributeTok{border=}\StringTok{"navy"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Histogram of Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-107-1} 

}

\caption{Distribution of residuals of the linear regression model}\label{fig:unnamed-chunk-107}
\end{figure}

The above histogram reveals the same information as we saw in the residual plot in the last note: (1) one out-lier; (2). The distribution is skewed to the right.

\hypertarget{residual-bootstrap-samples}{%
\subsection{Residual Bootstrap Samples}\label{residual-bootstrap-samples}}

The residual bootstrap sample of \(y\) is defined in the following:

\begin{itemize}
\item
  Take a \textbf{bootstrap sample} from the set of residuals \(\{e_1, e_2, \cdots, e_n \}\), denoted by \(\{e_1^{*}, e_2^{*}, \cdots, e_n^{*} \}\).
\item
  The residual bootstrap sample of \(\{y_1^{*}, y_2^{*}, \cdots, y_n^{*} \}\) is defined by
\end{itemize}

\[
\begin{array}{ccc} 
y_1^{*} & = &  \hat{\beta}_0 + \hat{\beta}_1 x_{11} + \hat{\beta}_2 x_{12} + \cdots + \hat{\beta}_k x_{1k} + e_1^{*}  \\
y_2^{*} & = &  \hat{\beta}_0 + \hat{\beta}_1 x_{21} + \hat{\beta}_2 x_{22} + \cdots + \hat{\beta}_k x_{2k} + e_2^{*}  \\
y_3^{*} & = &  \hat{\beta}_0 + \hat{\beta}_1 x_{31} + \hat{\beta}_2 x_{32} + \cdots + \hat{\beta}_k x_{3k} + e_3^{*}  \\
\vdots & \vdots & \vdots \\
y_n^{*} & = &  \hat{\beta}_0 + \hat{\beta}_1 x_{n1} + \hat{\beta}_2 x_{n2} + \cdots + \hat{\beta}_k x_{nk} + e_n^{*}
\end{array}
\]

The above definition implies that the residual bootstrap is equal to the \textbf{fitted value + bootstrap residuals}.

\begin{itemize}
\tightlist
\item
  The resulting \textbf{residual bootstrap sample} is given by
\end{itemize}

\[
\begin{array}{cccccc} 
y_1^{*} &  x_{11} &  x_{12} & \cdots & x_{1k}  \\
y_2^{*} &  x_{21} &  x_{22} & \cdots & x_{2k}  \\
y_3^{*} &  x_{31} &  x_{32} & \cdots & x_{3k}  \\
\vdots  &  \vdots &  \vdots & \vdots & \vdots  \\
y_n^{*} &  x_{n1} &  x_{n2} & \cdots & x_{nk} 
\end{array}
\]

\begin{itemize}
\tightlist
\item
  We fit the final model to the \textbf{residual bootstrap sample} and denote the bootstrap estimates of regression coefficients in the following
\end{itemize}

\[
\{ \hat{\beta}_0^{*}, \hat{\beta}_1^{*},  \cdots, \hat{\beta}_k^{*} \}
\]

\begin{itemize}
\tightlist
\item
  Repeat the above steps B times, we obtain the following bootstrap estimates
\end{itemize}

\[
\begin{array}{cccccc} 
\hat{\beta}_0^{1*} & \hat{\beta}_1^{1*} &  \cdots  & \hat{\beta}_k^{1*}  \\
\hat{\beta}_0^{2*} & \hat{\beta}_1^{2*} &  \cdots  & \hat{\beta}_k^{2*}  \\
\hat{\beta}_0^{3*} & \hat{\beta}_1^{3*} &  \cdots  & \hat{\beta}_k^{3*}  \\
\vdots  &  \vdots &  \vdots & \vdots  \\
\hat{\beta}_0^{b*} & \hat{\beta}_1^{b*} &  \cdots  & \hat{\beta}_k^{b*}  \\
\vdots  &  \vdots &  \vdots & \vdots  \\
\hat{\beta}_0^{B*} & \hat{\beta}_1^{B*} &  \cdots  & \hat{\beta}_k^{B*}  \\
\end{array}
\]

The residual bootstrap confidence intervals of regression coefficients can be estimated from the above bootstrap coefficients.

\hypertarget{implementation-of-residual-bootstrap-regression}{%
\subsection{Implementation of Residual Bootstrap Regression}\label{implementation-of-residual-bootstrap-regression}}

The following code generates bootstrap confidence intervals of regression coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Final model}
\NormalTok{log.price }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(PriceUnitArea) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ HouseAge }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}  
\NormalTok{                 Dist2MRT.kilo  }\SpecialCharTok{+}\NormalTok{ geo.group, }\AttributeTok{data =}\NormalTok{ final.data)}
\NormalTok{model.resid }\OtherTok{=}\NormalTok{ log.price}\SpecialCharTok{$}\NormalTok{residuals}
\DocumentationTok{\#\#}
\NormalTok{B}\OtherTok{=}\DecValTok{1000}
\NormalTok{num.p }\OtherTok{=} \FunctionTok{dim}\NormalTok{(}\FunctionTok{model.matrix}\NormalTok{(log.price))[}\DecValTok{2}\NormalTok{]   }\CommentTok{\# number of parameters}
\NormalTok{samp.n }\OtherTok{=} \FunctionTok{dim}\NormalTok{(}\FunctionTok{model.matrix}\NormalTok{(log.price))[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# sample size}
\NormalTok{btr.mtrx }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{6}\SpecialCharTok{*}\NormalTok{B), }\AttributeTok{ncol=}\NormalTok{num.p) }\CommentTok{\# zero matrix to store boot coefs}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B)\{}
  \DocumentationTok{\#\# Bootstrap response values}
\NormalTok{  bt.lg.price }\OtherTok{=}\NormalTok{ log.price}\SpecialCharTok{$}\NormalTok{fitted.values }\SpecialCharTok{+} 
        \FunctionTok{sample}\NormalTok{(log.price}\SpecialCharTok{$}\NormalTok{residuals, samp.n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\# bootstrap residuals}
  \CommentTok{\# replace PriceUnitArea with bootstrap log price}
\NormalTok{  final.data}\SpecialCharTok{$}\NormalTok{bt.lg.price }\OtherTok{=}\NormalTok{  bt.lg.price   }\CommentTok{\#  send the boot response to the data}
\NormalTok{  btr.model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(bt.lg.price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ HouseAge }\SpecialCharTok{+}\NormalTok{ NumConvenStores }\SpecialCharTok{+}\NormalTok{ sale.year }\SpecialCharTok{+}  
\NormalTok{                 Dist2MRT.kilo  }\SpecialCharTok{+}\NormalTok{ geo.group, }\AttributeTok{data =}\NormalTok{ final.data)   }\CommentTok{\# b}
\NormalTok{  btr.mtrx[i,]}\OtherTok{=}\NormalTok{btr.model}\SpecialCharTok{$}\NormalTok{coefficients}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Next, we make histograms of the residual bootstrap estimates of the regression coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot.hist }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(bt.coef.mtrx, var.id, var.nm)\{}
  \DocumentationTok{\#\# bt.coef.mtrx = matrix for storing bootstrap estimates of coefficients}
  \DocumentationTok{\#\# var.id = variable ID (1, 2, ..., k+1)}
  \DocumentationTok{\#\# var.nm = variable name on the hist title, }
  \DocumentationTok{\#\# must be the string in the double quotes}
  \DocumentationTok{\#\# Bootstrap sampling distribution of the estimated coefficients}
\NormalTok{  x1}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FunctionTok{min}\NormalTok{(bt.coef.mtrx[,var.id]), }\FunctionTok{max}\NormalTok{(bt.coef.mtrx[,var.id]), }\AttributeTok{length=}\DecValTok{300}\NormalTok{ )}
\NormalTok{  y1}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{dnorm}\NormalTok{(x1}\FloatTok{.1}\NormalTok{, }\FunctionTok{mean}\NormalTok{(bt.coef.mtrx[,var.id]), }\FunctionTok{sd}\NormalTok{(bt.coef.mtrx[,var.id]))}
  \CommentTok{\# height of the histogram {-} use it to make a nice{-}looking histogram.}
\NormalTok{  highestbar }\OtherTok{=} \FunctionTok{max}\NormalTok{(}\FunctionTok{hist}\NormalTok{(bt.coef.mtrx[,var.id], }\AttributeTok{plot =} \ConstantTok{FALSE}\NormalTok{)}\SpecialCharTok{$}\NormalTok{density) }
\NormalTok{  ylimit }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(}\FunctionTok{c}\NormalTok{(y1}\FloatTok{.1}\NormalTok{,highestbar))}
  \FunctionTok{hist}\NormalTok{(bt.coef.mtrx[,var.id], }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{main =}\NormalTok{ var.nm, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }
       \AttributeTok{col =} \StringTok{"azure1"}\NormalTok{,}\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,ylimit), }\AttributeTok{border=}\StringTok{"lightseagreen"}\NormalTok{)}
  \FunctionTok{lines}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x1}\FloatTok{.1}\NormalTok{, }\AttributeTok{y =}\NormalTok{ y1}\FloatTok{.1}\NormalTok{, }\AttributeTok{col =} \StringTok{"red3"}\NormalTok{)       }\CommentTok{\# normal density curve         }
  \FunctionTok{lines}\NormalTok{(}\FunctionTok{density}\NormalTok{(bt.coef.mtrx[,var.id], }\AttributeTok{adjust=}\DecValTok{2}\NormalTok{), }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)    }\CommentTok{\# loess curve}
\NormalTok{\} }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))  }\CommentTok{\# histograms of bootstrap coefs}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{btr.mtrx, }\AttributeTok{var.id=}\DecValTok{1}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"Intercept"}\NormalTok{ )}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{btr.mtrx, }\AttributeTok{var.id=}\DecValTok{2}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"House Age"}\NormalTok{ )}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{btr.mtrx, }\AttributeTok{var.id=}\DecValTok{3}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"Num Conven Stores"}\NormalTok{ )}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{btr.mtrx, }\AttributeTok{var.id=}\DecValTok{4}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"Year Sold"}\NormalTok{ )}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{btr.mtrx, }\AttributeTok{var.id=}\DecValTok{5}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"Dist to MRT"}\NormalTok{ )}
\FunctionTok{boot.hist}\NormalTok{(}\AttributeTok{bt.coef.mtrx=}\NormalTok{btr.mtrx, }\AttributeTok{var.id=}\DecValTok{6}\NormalTok{, }\AttributeTok{var.nm =}\StringTok{"Region"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-110-1} 

}

\caption{(Residual) Bootstrap confidence intervals}\label{fig:unnamed-chunk-110}
\end{figure}

The residual bootstrap sampling distributions of each estimated regression coefficient. The normal and LOESS curves are close to each other. This also indicated that the inference of the significance of variables based on p-values and residual bootstrap will yield the same results.

The 95\% residual bootstrap confidence intervals are given in the following

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#}
\NormalTok{num.p }\OtherTok{=} \FunctionTok{dim}\NormalTok{(coef.mtrx)[}\DecValTok{2}\NormalTok{]  }\CommentTok{\# number of parameters}
\NormalTok{btr.ci }\OtherTok{=} \ConstantTok{NULL}
\NormalTok{btr.wd }\OtherTok{=} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{num.p)\{}
\NormalTok{  lci}\FloatTok{.025} \OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{quantile}\NormalTok{(btr.mtrx[, i], }\FloatTok{0.025}\NormalTok{, }\AttributeTok{type =} \DecValTok{2}\NormalTok{),}\DecValTok{8}\NormalTok{)}
\NormalTok{  uci}\FloatTok{.975} \OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{quantile}\NormalTok{(btr.mtrx[, i],}\FloatTok{0.975}\NormalTok{, }\AttributeTok{type =} \DecValTok{2}\NormalTok{ ),}\DecValTok{8}\NormalTok{)}
\NormalTok{  btr.wd[i] }\OtherTok{=}\NormalTok{ uci}\FloatTok{.975} \SpecialCharTok{{-}}\NormalTok{ lci}\FloatTok{.025}
\NormalTok{  btr.ci[i] }\OtherTok{=} \FunctionTok{paste}\NormalTok{(}\StringTok{"["}\NormalTok{, }\FunctionTok{round}\NormalTok{(lci}\FloatTok{.025}\NormalTok{,}\DecValTok{4}\NormalTok{),}\StringTok{", "}\NormalTok{, }\FunctionTok{round}\NormalTok{(uci}\FloatTok{.975}\NormalTok{,}\DecValTok{4}\NormalTok{),}\StringTok{"]"}\NormalTok{)}
\NormalTok{\}}
\CommentTok{\#as.data.frame(btc.ci)}
\FunctionTok{cbind}\NormalTok{(cmtrx, }\AttributeTok{btr.ci.95=}\NormalTok{btr.ci)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 Estimate  Std. Error t value    Pr(>|t|)
## (Intercept)     "3.5724"  "0.0443"   "80.599"   "0"     
## HouseAge        "-0.0076" "0.001"    "-7.4985"  "0"     
## NumConvenStores "0.0275"  "0.0049"   "5.5987"   "0"     
## sale.year2013   "0.0806"  "0.0244"   "3.2963"   "0.0011"
## Dist2MRT.kilo   "-0.1445" "0.0138"   "-10.5068" "0"     
## geo.groupTRUE   "0.1826"  "0.0347"   "5.2596"   "0"     
##                 btr.ci.95               
## (Intercept)     "[ 3.4854 ,  3.6623 ]"  
## HouseAge        "[ -0.0096 ,  -0.0056 ]"
## NumConvenStores "[ 0.0174 ,  0.037 ]"   
## sale.year2013   "[ 0.0367 ,  0.1292 ]"  
## Dist2MRT.kilo   "[ -0.172 ,  -0.1144 ]" 
## geo.groupTRUE   "[ 0.1153 ,  0.2514 ]"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#kable(cbind(cmtrx, btr.ci.95=btr.ci), }
\CommentTok{\#      caption = "Regression Coefficient Matrix with 95\% Residual Bootstrap CI")}
\end{Highlighting}
\end{Shaded}

As expected, the residual bootstrap confidence intervals yield the same results as p-values do. This is because the sample size is large enough so that the sampling distributions of estimated coefficients have sufficiently good approximations of normal distributions.

\hypertarget{combining-all-inferential-statistics}{%
\subsection{Combining All Inferential Statistics}\label{combining-all-inferential-statistics}}

Finally, we put all inferential statistics in a single table so we can compare these results.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(cmtrx[,}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{], }\AttributeTok{btc.ci.95=}\NormalTok{btc.ci,}\AttributeTok{btr.ci.95=}\NormalTok{btr.ci), }
      \AttributeTok{caption=}\StringTok{"Final Combined Inferential Statistics: p{-}values and Bootstrap CIs"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-112}Final Combined Inferential Statistics: p-values and Bootstrap CIs}
\centering
\begin{tabular}[t]{l|l|l|l|l|l}
\hline
  & Estimate & Std. Error & Pr(>|t|) & btc.ci.95 & btr.ci.95\\
\hline
(Intercept) & 3.5724 & 0.0443 & 0 & [ 3.4742 ,  3.6668 ] & [ 3.4854 ,  3.6623 ]\\
\hline
HouseAge & -0.0076 & 0.001 & 0 & [ -0.0097 ,  -0.0056 ] & [ -0.0096 ,  -0.0056 ]\\
\hline
NumConvenStores & 0.0275 & 0.0049 & 0 & [ 0.017 ,  0.0379 ] & [ 0.0174 ,  0.037 ]\\
\hline
sale.year2013 & 0.0806 & 0.0244 & 0.0011 & [ 0.0361 ,  0.125 ] & [ 0.0367 ,  0.1292 ]\\
\hline
Dist2MRT.kilo & -0.1445 & 0.0138 & 0 & [ -0.176 ,  -0.1148 ] & [ -0.172 ,  -0.1144 ]\\
\hline
geo.groupTRUE & 0.1826 & 0.0347 & 0 & [ 0.1175 ,  0.2489 ] & [ 0.1153 ,  0.2514 ]\\
\hline
\end{tabular}
\end{table}

The above table shows that

\begin{itemize}
\tightlist
\item
  All three methods yield the same results in terms of the significance of individual explanatory variables. The reason is that the final working model does not have a serious violation of the model assumption.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(btc.wd, btr.wd), }\AttributeTok{caption=}\StringTok{"width of the two bootstrap confidence intervals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-113}width of the two bootstrap confidence intervals}
\centering
\begin{tabular}[t]{r|r}
\hline
btc.wd & btr.wd\\
\hline
0.1925855 & 0.1769694\\
\hline
0.0040454 & 0.0039738\\
\hline
0.0209244 & 0.0196569\\
\hline
0.0888693 & 0.0925008\\
\hline
0.0612208 & 0.0575790\\
\hline
0.1313630 & 0.1361225\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\tightlist
\item
  The widths of residual bootstrap and case-bootstrap confidence intervals are similar to each other. See the above table for the widths of each confidence interval.
\end{itemize}

We see from the above table that the widths of residual-bootstrap and case-bootstrap confidence intervals are similar to each other.

\hfill\break

\hypertarget{discussions-on-model-reporting}{%
\section{Discussions on Model Reporting}\label{discussions-on-model-reporting}}

We have used different methods to perform regression analysis. The inferential results of all three major methods yield similar results. We chose the parametric model to report.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(cmtrx, }\AttributeTok{caption =} \StringTok{"Inferential Statistics of Final Model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-114}Inferential Statistics of Final Model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 3.5724 & 0.0443 & 80.5990 & 0.0000\\
\hline
HouseAge & -0.0076 & 0.0010 & -7.4985 & 0.0000\\
\hline
NumConvenStores & 0.0275 & 0.0049 & 5.5987 & 0.0000\\
\hline
sale.year2013 & 0.0806 & 0.0244 & 3.2963 & 0.0011\\
\hline
Dist2MRT.kilo & -0.1445 & 0.0138 & -10.5068 & 0.0000\\
\hline
geo.groupTRUE & 0.1826 & 0.0347 & 5.2596 & 0.0000\\
\hline
\end{tabular}
\end{table}

Regression coefficients explain the correlation between log price and the corresponding explanatory variables. As an example, We explain the estimated regression coefficient of 0.1826. Let \(p_{TRUE}\) be the mean price of a house in the region \texttt{TRUE} and \(p_{FALSE}\) be the mean price of houses in the region \texttt{FALSE}. Then

\[
\log (p_{TRUE}) - \log(p_{FALSE}) = 0.1826  \to \log(p_{TRUE}/p_{FALSE}) = 0.1826 \to p_{TRUE} = 1.20p_{FALSE}
\]
We re-express the above equation can be re-written as

\[
p_{TRUE} - p_{FALSE} = 0.206p_{FALSE} \to \frac{p_{TRUE}-p_{FALSE}}{p_{FALSE}} = 0.20 = 20\%.
\]

That is, the average house sales price in the \texttt{TRUE} region (top right corner on the map) is about 20\% higher than that in the \texttt{FALSE} region. We can similarly interpret other regression coefficients.

\hypertarget{some-concluding-remarks}{%
\section{Some Concluding Remarks}\label{some-concluding-remarks}}

\begin{center}\includegraphics[width=0.8\linewidth]{img05/w05-LetDataSpeak} \end{center}

\hypertarget{key-elements-in-storytelling}{%
\subsection{Key Elements in Storytelling}\label{key-elements-in-storytelling}}

Data storytelling is an effective way of communicating insights. It involves constructing a clear and relevant narrative in context, analyzing and visualizing data, and presenting findings effectively and persuasively. Data storytelling is an essential skill for statisticians.

Effective data storytelling involves three core elements: data, narrative, and visuals.

\textbf{Data} is the foundation of every data story.

\textbf{Narrative} is the structure of the data story. It's not just about the words or context that are used to explain the information extracted from data, it is about how to organize the information into a meaningful and engaging story.

\textbf{Visuals} are the scenes of the data story. When the data is complex, visualizing it in data charts helps the audience to \textbf{see} the \textbf{information} they might not otherwise see.

\hypertarget{title-and-heading-in-statistics-report}{%
\subsection{Title and Heading in Statistics Report}\label{title-and-heading-in-statistics-report}}

Although headings and titles are similar, they are distinct: A title leads the entire document and captures its content in one or two phrases; a heading leads only a chapter or section and captures only the content of that chapter or section.

\begin{itemize}
\tightlist
\item
  A good \textbf{title} should be

  \begin{itemize}
  \tightlist
  \item
    descriptive,
  \item
    Direct
  \item
    Accurate
  \item
    appropriate
  \item
    concise
  \item
    precise
  \item
    unique and
  \item
    should \textbf{not} be misleading.
  \end{itemize}
\item
  A good \textbf{heading} should be

  \begin{itemize}
  \tightlist
  \item
    Headings should be as long as it takes to communicate the content of the sections they head.
  \item
    Higher-level headings often make do with a single word (e.g.~\textbf{Introduction} or \textbf{Methods}), while lower-level headings are often longer.
  \item
    Headings should be as descriptive as possible.
  \item
    Headings should be unique.
  \end{itemize}
\end{itemize}

\hypertarget{regression-modeling-and-reporting}{%
\subsection{Regression Modeling and Reporting}\label{regression-modeling-and-reporting}}

There are different approaches to regression modeling and reporting. We summarized the modeling process and techniques learned and suggested guidelines for reporting.

\hypertarget{modeling-workflow}{%
\subsubsection{Modeling Workflow}\label{modeling-workflow}}

Linear regression workflow is summarized in the following chart

\begin{center}\includegraphics[width=1\linewidth]{img05/w05-LinearRegWorkflow} \end{center}

The specific model diagnostic methods and remedies to be included in the analysis are

\begin{itemize}
\item
  residual analysis - normality, constant variance, linearity of the regression equation, etc.
\item
  multicollinearity - variance inflation factor
\item
  potential transformation of the response variable - conventional and Box-cox methods
\item
  variable selection - expert-assisted variable selection (focusing on practical importance) and automatic variable selection (focusing on statistical significance).
\item
  non-parametric robust estimation of regression coefficients - two types of bootstrap methods.
\end{itemize}

\hypertarget{reporting-guidelines}{%
\subsubsection{Reporting Guidelines}\label{reporting-guidelines}}

We have discussed the basic components of a statistics report. The following book chapter summarizes methods of writing and organizing statistical reports.

\url{https://instruction.bus.wisc.edu/jfrees/jfreesbooks/Regression\%20Modeling/BookWebDec2010/Writing/Chap20FreesRegBook03May2009.pdf}

We have adopted the major components for our project reporting. Please review this document and pay attention to the sections related to how to report tables and figures.

\hypertarget{modeling-assignment}{%
\section{Modeling Assignment}\label{modeling-assignment}}

This assignment focuses on bootstrapping the multiple regression model you identified in your last assignment. You are expected to use both bootstrap sampling methods to find the confidence intervals for regression coefficients.

Please follow what did in the lecture note to do a similar analysis using your data and summarize the bootstrap analysis results. The mini-project report combines the results you obtained last week with this week's bootstrap results.

The format of the report should have the following components using last and this week's work as building blocks.

\begin{itemize}
\tightlist
\item
  Introduction

  \begin{itemize}
  \tightlist
  \item
    rationale of the project - research questions
  \item
    description of data - sampling?
  \item
    sufficient information for addressing the research questions
  \end{itemize}
\item
  Data preparation and exploratory analysis

  \begin{itemize}
  \tightlist
  \item
    create new variables based on existing variables?
  \item
    discretizing continuous variable? combining categories of categorical variables?
  \end{itemize}
\item
  Model building

  \begin{itemize}
  \tightlist
  \item
    starting with a full model and performing residual analysis
  \item
    identifying violations and finding remedies
  \item
    need model transformations like the Cox-Cox procedure?
  \item
    variable selection? -backward and forward selection, stepwise selection.
  \item
    model selection using goodness-of-fit measures.
  \item
    selection of the final model.
  \end{itemize}
\item
  Bootstrapping the final model

  \begin{itemize}
  \tightlist
  \item
    bootstrapping records
  \item
    bootstrapping residuals of the final model obtained in the previous model
  \end{itemize}
\item
  Combining the results of the regular model and bootstrap results

  \begin{itemize}
  \tightlist
  \item
    Summarizing the results
  \item
    Correct interpretations of the coefficients of the regression model.
  \item
    Interpretation with caution if your response variable was transformed on a different scale.
  \end{itemize}
\item
  Summary and discussion

  \begin{itemize}
  \tightlist
  \item
    outline main findings
  \item
    drawbacks and future improvements
  \item
    recommendations on the applications of the model.
  \end{itemize}
\end{itemize}

\hypertarget{simple-logistic-regression-model}{%
\chapter{Simple Logistic Regression Model}\label{simple-logistic-regression-model}}

We have systematically reviewed linear regression models and used parametric and bootstrap methods to build the linear regression models.

\hfill\break

\begin{center}\includegraphics[width=0.95\linewidth]{img06/w06-ModelSummary4MAT321} \end{center}

\hfill\break

Starting from this note, we will study several models in the family of generalized linear models (GLM). We first overview the major members of this family.

\hfill\break

\begin{center}\includegraphics[width=0.95\linewidth]{img06/w06-GLMClassification} \end{center}

\hfill\break

Three models will be introduced in the subsequent notes. We primarily focus on the commonly used models that are used in statistics and machine learning fields. The \textbf{binary logistic regression models} and their variants are probably the most popular models that have been used in almost all areas of quantitative decision.

Before taking presenting the technical introduction of the binary logistic regression model, we look at the major structural differences between the regular linear regression model and logistic regression models.

\begin{center}\includegraphics[width=0.8\linewidth]{img06/w06-LMvsGLM} \end{center}

\hfill\break

\hypertarget{simple-binary-logistic-regression}{%
\section{Simple Binary Logistic Regression}\label{simple-binary-logistic-regression}}

In this module, we study a new regression model with a binary response variable that takes on exactly one of two possible values such as \textbf{success} v.s. \textbf{failure}, \textbf{diseased} v.s. \textbf{disease-free}, etc.

For a binary population, we are interested in the proportion of one of the two values. For example, if we study a disease of a certain population, the primary interest is the prevalence of a disease - the proportion of subjects in the population who had the disease. We also know that the relative frequency (proportion) of the disease can be used to estimate the disease probability of the population. A natural and practical question is whether the disease probability is impacted by some factors - this is a repression problem. In the subsequent sections, we present a brief but little technical introduction to \textbf{\color{red}the binary logistic regression model with a single predictor variable}.

\hypertarget{the-model-structure}{%
\subsection{The Model Structure}\label{the-model-structure}}

Let \(Y\) be a binary variable that takes on exactly one of the two possible values, say \textbf{success} or \textbf{failure}. Assume the proportion of \textbf{success} to be \(p\). That is, \(P(Y = \text{success}) = p\). Let \(x\) be a factor that may impact the success probability. Clearly, \(Y\) is a Bernoulli random variable. Since we are interested in the success probability, we encode the values of variable \(Y\) by setting \textbf{success} to 1 and \textbf{failure} to 0.

Recall that the linear regression model is defined by \(E[Y] = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k\) and

\[
\epsilon = Y - E[Y] \to N(0, \sigma^2).
\]
where \(Y\) is a continuous random variable and \(X\)'s are assumed to be non-random.

\hfill\break

When we have a binary population (i.e., \(Y\) takes only two possible distinct values: \textbf{success} or \textbf{failure}), we are primarily interested in the proportion of \textbf{success} or \textbf{failure}. Since the \(Y\) is a Bernoulli (random) variable, the expected value of the binary response variable \(Y\) is given by.

\[
E[Y] = 1\times p + 0\times (1-p) = p.
\]

That is, the success probability \(p\) is the expected value of the binary random variable. The regression problem is to look at how the \textbf{success} probability is affected by potential factors. One initial guess is to use the formulation of the linear regression model by setting

\[
p =E[Y] = \beta_0 + \beta_1 x.
\]
The above formulation is inappropriate since the right-hand side of the equation can take on any real value while the left-hand side equation can take on only values in the interval \([0,1]\).

One of the most important criteria for a good statistical model is its interpretability. We cannot link the success probability with the linear function of the predictor. Next, we link the \textbf{odds of success} (\(p/(1-p)\)) with the linear function of the predictor variable in the following.

\[
\frac{p}{1-p} = \beta_0 + \beta_1x.
\]

The above expression shows the association between the \textbf{odds of success}, \(p/(1-p)\), and the predictor variable \(x\). It is easy to interpret. But the \textbf{odds of success} take on the value in \([0, \infty)\). The above formulation is still inappropriate.

However, \(-\infty <\log\frac{p}{1-p} < \infty\), it is reasonable to explore the association between the logarithm of odds of success and the predictor variable \(x\). That is,

\[
\log\frac{p}{1-p} = \beta_0 + \beta_1x.
\]

The above model is called the \textbf{simple logistic regression model}.

\textbf{Remarks on the model formulation}

\begin{itemize}
\item
  The above logistic regression model was formulated based on the interpretability of the transformed mean response, \(E[Y]\), and its linear association with the predictor variable.
\item
  Mathematically, there are different transformations of the mean response to define a family of regression models \(g(p) = \beta_0 + \beta_1x\). Clearly, the above simple logistic regression model is a special member of this family - generalized linear regression models. There are two other less famous but commonly used in some specific fields: the complementary log-log model and the probit model.
\end{itemize}

\hypertarget{interpretation-of-regression-coefficients-2}{%
\subsection{Interpretation of Regression Coefficients}\label{interpretation-of-regression-coefficients-2}}

If we use numerical coding 1 = ``success'' and 0 = ``failure'', the simple logistic regression model can be explicitly expressed in the following form

\[
\log \frac{P(Y=1|x)}{1-P(Y=1|x)}= \beta_0 + \beta_1x.
\]

The expression \(P(Y=1|x)\) highlights that the success probability is dependent on the predictor variable \(x\).

The regression coefficients of the simple logistic regression model have a meaningful interpretation.

\begin{itemize}
\item
  Intercept \(\beta_0\) is the baseline log odds of success. In other words, if the success probability is not impacted by any factors, \(\beta_0\) is the log odds of success of the homogeneous population.
\item
  the slope parameter \(\beta_1\) is called log odds ratio of two categories corresponding to \(x\) and \(x+1\). To see this, denote \(p_x = P(Y=1|x)\) and \(p_{x+1} = P(Y=1|x+1)\), then
\end{itemize}

\[
\log \frac{p_x}{1-p_x} = \beta_0 + \beta_1x\hspace{3mm} \mbox{and} \hspace{3mm} \log \frac{p_{x+1}}{1-p_{x+1}} = \beta_0 + \beta_1(x+1).
\]

Taking the difference between the above two equations, we have

\[
\log \frac{p_{x+1}}{1-p_{x+1}}- \log \frac{p_x}{1-p_x} = \beta_1
\]

Therefore,

\[
\log \frac{p_{x+1}/(1-p_{x+1})}{p_x/(1-p_x)} = \beta_1,
\]

that is, \(\beta_1\) is the ratio of log odds of success in two sub-populations.

\begin{center}\includegraphics[width=0.8\linewidth]{img06/w06-InterpretInterceptParameter} \end{center}

\hypertarget{use-of-simple-logistic-regression-model}{%
\subsection{Use of Simple Logistic Regression Model}\label{use-of-simple-logistic-regression-model}}

We first express the success probability with respect to the predictor variable \(x\) in the following

\[
P(Y=1|x) = \frac{e^{\beta_0 + \beta_1x}}{1+e^{\beta_0 + \beta_1x}}.
\]

Using the above expression, we can do one of the following analyses.

\begin{itemize}
\item
  \textbf{Association Analysis} - if \(\beta_1 \ne 0\), then the success probability is impacted by the predictor variable \(x\). Note that, this is a non-linear association.
\item
  \textbf{Predictive Analysis} - predicting the success probability for a given new value of the predictor variable. That is,
\end{itemize}

\[
\widehat{P(Y=1|x_\text{new})} = \frac{e^{\hat{\beta}_0 + \hat{\beta}_1x_\text{new}}}{1+e^{\hat{\beta}_0 + \hat{\beta}_1x_\text{new}}},
\]

where \(\hat{\beta}_0\) and \(\hat{\beta}_1\) are estimated from the data.

\begin{center}\includegraphics[width=0.9\linewidth]{img06/w06-ANN} \end{center}

\begin{itemize}
\item
  \textbf{Classification Analysis} - predicting the status of success. That is, for a given new value of the predictor variable, we predict the value of \(Y\) through \(P(Y=1|x_{new})\).

  \begin{itemize}
  \item
    To predict whether the value of \(Y\) is \textbf{success} or \textbf{failure}, we need to identify the cut-off probability to determine the value of \(Y\).
  \item
    The predicted model can be used in an \textbf{intervention analysis} - this means values of \(X\) can alter the value of \(Y\). This is commonly used in clinical studies. For example, an effective treatment (\(X\)) can permanently cure a disease (\(Y\)).
  \item
    The predicted model can be used for membership classification. For example, the response value is the gender (\(Y\)) of a car buyer at a car dealer, the predictor variable is the purchase status (\(X\)). If a customer bought a car from the dealer, the fitted model can identify whether the customer is a man or woman. This is apparently different from the intervention analysis since purchase status cannot change the gender (\(Y\)) of the customer.
  \end{itemize}
\end{itemize}

\hypertarget{parameter-estimation}{%
\subsection{Parameter Estimation}\label{parameter-estimation}}

In linear regression models, both likelihood and least square methods can be used for estimating the coefficients of linear regression models. Both methods yield the same estimates. However, in the logistic regression model, we can only use the likelihood methods to estimate the regression coefficients.

Let \(\{(y_1, x_1), (y_2, x_2), \cdots, (y_n, x_n)\}\) be a random sample taken from a binary population associated with \(Y\). \(x\) is a nonrandom predictor variable associated with \(Y\). The logistic model is defined to be

\[
p(x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}.
\]

Since \(Y_i\) is a Bernoulli random variable with success probability \(p_x\). We use numerical coding: 1 = ``success'' and 0 = ``failure''. The likelihood function of \((\beta_0, \beta_1)\) is given by

\[
L(\beta_0, \beta_1)= \prod_{i=1}^n p(x_i)^{y_i}\left[1 - p(x_i) \right]^{1-y_i} 
= \prod_{i=1}^n \left[\frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}\right]^{y_i}\times \left[\frac{1}{1 + e^{\beta_0 + \beta_1 x_i}} \right]^{1-y_i}
\]

The maximum likelihood estimate (MLE) of \(\beta_0\) and \(\beta_1\), denoted by \(\hat{\beta}_0\) and \(\hat{\beta}_1\), maximizes the above likelihood. We will use the R build-in function \textbf{glm()} to find the MLE of the parameters and related statistics.

\hypertarget{model-assumptions-and-diagnostics}{%
\subsection{Model Assumptions and Diagnostics}\label{model-assumptions-and-diagnostics}}

In linear regression, we assume the response variable follows a normal distribution with a constant variance. With this assumption, several effective diagnostic methods were developed based on the residual analysis. In logistic regression, we don't have many diagnostic methods. However, several likelihood-based goodness of fit metrics such as AIC and deviance can be used for comparing the performance of candidate models.

More technical discussion of diagnostics with the left to the future specialized courses in generalized linear regression models.

\hypertarget{concluding-remarks-1}{%
\subsection{Concluding Remarks}\label{concluding-remarks-1}}

We only introduced the basic logistic regression modeling in this note. some important topics you may want to study but not mentioned in this note are

\begin{itemize}
\item
  logistic regression as a machine learning algorithm for predictive modeling.
\item
  logistic regression model with a large number of predictor variables - regularized logistic regression.
\item
  performance metrics based on prediction errors.
\end{itemize}

\hypertarget{a-case-study}{%
\section{A Case Study}\label{a-case-study}}

The diabetes data set in this case study contains 768 observations on 9 variables. The data set is available in the UCI machine learning data repository. R library \{mlbench\} has two versions of this data. The data set contains a significant number of missing values.

\hypertarget{data-and-variable-descriptions}{%
\subsection{Data and Variable Descriptions}\label{data-and-variable-descriptions}}

There are 9 variables in the data set.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{pregnant}: Number of times pregnant
\item
  \textbf{glucose}: Plasma glucose concentration (glucose tolerance test)
\item
  \textbf{pressure}: Diastolic blood pressure (mm Hg)
\item
  \textbf{triceps}: Triceps skin fold thickness (mm)
\item
  \textbf{insulin}: 2-Hour serum insulin (mu U/ml)
\item
  \textbf{mass}: Body mass index (weight in kg/(height in m)\^{}2)
\item
  \textbf{pedigree}: Diabetes pedigree function
\item
  \textbf{age}: Age (years)
\item
  \textbf{diabetes}: Class variable (test for diabetes)
\end{enumerate}

I load the data from R \textbf{library\{mlbench\}} in the following code.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{library}\NormalTok{(mlbench)}
\FunctionTok{data}\NormalTok{(PimaIndiansDiabetes2)           }\CommentTok{\# load the data to R work{-}space}
\NormalTok{diabetes}\FloatTok{.0} \OtherTok{=}\NormalTok{ PimaIndiansDiabetes2    }\CommentTok{\# make a copy of the data for data cleansing }
\NormalTok{diabetes }\OtherTok{=} \FunctionTok{na.omit}\NormalTok{(diabetes}\FloatTok{.0}\NormalTok{)       }\CommentTok{\# Delete all records with missing components}
\NormalTok{y0}\OtherTok{=}\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{diabetes}
\NormalTok{diabete}\FloatTok{.01} \OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{length}\NormalTok{(y0))      }\CommentTok{\# define a 0{-}1 to test which probability is used in glm()}
\NormalTok{diabete}\FloatTok{.01}\NormalTok{[}\FunctionTok{which}\NormalTok{(y0}\SpecialCharTok{==}\StringTok{"pos"}\NormalTok{)] }\OtherTok{=} \DecValTok{1}
\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{diabetes}\FloatTok{.01} \OtherTok{=}\NormalTok{ diabete}\FloatTok{.01}
\CommentTok{\# head(diabetes)}
\end{Highlighting}
\end{Shaded}

For convenience, I delete all records with missing values and keep only the records with complete records in this case study. The final analytic data set has 392 records.

\hypertarget{clinical-question}{%
\subsection{Clinical Question}\label{clinical-question}}

Many studies indicated that body mass index (BMI) is a more powerful risk factor for diabetes than genetics. The objective of this case study is to explore the \textbf{association} between BMI and diabetes.

The general interpretation of BMI for adults is given below:

\begin{itemize}
\tightlist
\item
  \textbf{underweight}: \textless{} 18.5
\item
  \textbf{Normal and Healthy Weight}: {[}18.5, 24.9{]}
\item
  \textbf{Overweight}: {[}25.0, 29.9{]}
\item
  \textbf{Obese}: \textgreater{} 30.0
\end{itemize}

\hypertarget{building-the-simple-logistic-regression}{%
\subsection{Building the Simple Logistic Regression}\label{building-the-simple-logistic-regression}}

Since we only study the simple logistic regression model, only one predictor variable is included in the model. We first perform exploratory data analysis on the predictor variable to make sure the variable is not extremely skewed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ylimit }\OtherTok{=} \FunctionTok{max}\NormalTok{(}\FunctionTok{density}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{mass)}\SpecialCharTok{$}\NormalTok{y)}
\FunctionTok{hist}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{mass, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{main =} \StringTok{"BMI Distribution"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }
       \AttributeTok{col =} \StringTok{"azure1"}\NormalTok{, }\AttributeTok{border=}\StringTok{"lightseagreen"}\NormalTok{)}
  \FunctionTok{lines}\NormalTok{(}\FunctionTok{density}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{mass, }\AttributeTok{adjust=}\DecValTok{2}\NormalTok{), }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-123-1} \end{center}

Since the simple logistic regression contains only one continuous variable of a binary categorical variable as the predictor variable, no there is no issue of potential imbalance. We will not transform BMI and fit a logistic regression directly to the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s.logit }\OtherTok{=} \FunctionTok{glm}\NormalTok{(diabetes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mass, }
          \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),  }\CommentTok{\# family is the binomial, logit(p) = log(p/(1{-}p))!}
          \AttributeTok{data =}\NormalTok{ diabetes)                    }\CommentTok{\# the data frame is a subset of the original iris data}
\FunctionTok{summary}\NormalTok{(s.logit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = diabetes ~ mass, family = binomial(link = "logit"), 
##     data = diabetes)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7737  -0.9032  -0.6794   1.3004   1.9017  
## 
## Coefficients:
##             Estimate Std. Error z value     Pr(>|z|)    
## (Intercept) -3.60614    0.59173  -6.094 0.0000000011 ***
## mass         0.08633    0.01705   5.062 0.0000004143 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 498.10  on 391  degrees of freedom
## Residual deviance: 469.03  on 390  degrees of freedom
## AIC: 473.03
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

Note that the response variable is a binary factor variable, R uses alphabetical order to define the level of the factor variable. In our case, ``neg'' = 0 and ``pos'' = 1. The ``success'' probability is defined to be P(diabetes = ``pos''). The simple logistic regression is fitted in the following.

The summary of major statistics is given below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.coef.stats }\OtherTok{=} \FunctionTok{summary}\NormalTok{(s.logit)}\SpecialCharTok{$}\NormalTok{coef       }\CommentTok{\# output stats of coefficients}
\NormalTok{conf.ci }\OtherTok{=} \FunctionTok{confint}\NormalTok{(s.logit)                     }\CommentTok{\# confidence intervals of betas}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Waiting for profiling to be done...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.stats }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(model.coef.stats, }\AttributeTok{conf.ci.95=}\NormalTok{conf.ci)   }\CommentTok{\# rounding off decimals}
\FunctionTok{kable}\NormalTok{(sum.stats,}\AttributeTok{caption =} \StringTok{"The summary stats of regression coefficients"}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-125}The summary stats of regression coefficients}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|) & 2.5 \% & 97.5 \%\\
\hline
(Intercept) & -3.6061432 & 0.5917329 & -6.094208 & 0.0000000 & -4.8064211 & -2.4817472\\
\hline
mass & 0.0863295 & 0.0170535 & 5.062276 & 0.0000004 & 0.0538266 & 0.1208267\\
\hline
\end{tabular}
\end{table}

From the above table, we can see that BMI is positively associated with the status of diabetes since \(\beta_1 = 0.0863\) with a p-value close to 0. The 95\% confidence interval {[}0.0538, 0.1208{]}. This also supports the results of the research in the literature.

It is more common to interpret the association results from a practical perspective using the odds ratio. Next, we convert the estimated regression coefficients to the odds ratio.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Odds ratio}
\NormalTok{model.coef.stats }\OtherTok{=} \FunctionTok{summary}\NormalTok{(s.logit)}\SpecialCharTok{$}\NormalTok{coef}
\NormalTok{odds.ratio }\OtherTok{=} \FunctionTok{exp}\NormalTok{(}\FunctionTok{coef}\NormalTok{(s.logit))}
\NormalTok{out.stats }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(model.coef.stats, }\AttributeTok{odds.ratio =}\NormalTok{ odds.ratio)                 }
\FunctionTok{kable}\NormalTok{(out.stats,}\AttributeTok{caption =} \StringTok{"Summary Stats with Odds Ratios"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-126}Summary Stats with Odds Ratios}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|) & odds.ratio\\
\hline
(Intercept) & -3.6061432 & 0.5917329 & -6.094208 & 0.0000000 & 0.0271564\\
\hline
mass & 0.0863295 & 0.0170535 & 5.062276 & 0.0000004 & 1.0901655\\
\hline
\end{tabular}
\end{table}

The odds ratio associated with BMI is 1.09 meaning that as the BMI increases by one unit, the odds of being tested positive for diabetes increase by about \(9\%\). This is a practically significant risk factor for diabetes.

Some global goodness-of-fit measures are summarized in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Other global goodness{-}of{-}fit}
\NormalTok{dev.resid }\OtherTok{=}\NormalTok{ s.logit}\SpecialCharTok{$}\NormalTok{deviance}
\NormalTok{dev.}\FloatTok{0.}\NormalTok{resid }\OtherTok{=}\NormalTok{ s.logit}\SpecialCharTok{$}\NormalTok{null.deviance}
\NormalTok{aic }\OtherTok{=}\NormalTok{ s.logit}\SpecialCharTok{$}\NormalTok{aic}
\NormalTok{goodness }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{Deviance.residual =}\NormalTok{dev.resid, }\AttributeTok{Null.Deviance.Residual =}\NormalTok{ dev.}\FloatTok{0.}\NormalTok{resid,}
      \AttributeTok{AIC =}\NormalTok{ aic)}
\FunctionTok{kable}\NormalTok{(goodness)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r}
\hline
Deviance.residual & Null.Deviance.Residual & AIC\\
\hline
469.031 & 498.0978 & 473.031\\
\hline
\end{tabular}

Since the above global goodness-of-fit is based on the likelihood function, we don't have other candidate models with corresponding likelihood at the same scale to compare in this simple logistic regression model, we will not interpret these goodness-of-fit measures.

The success probability curve (so-called S curve) is given below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bmi.range }\OtherTok{=} \FunctionTok{range}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{mass)}
\NormalTok{x }\OtherTok{=} \FunctionTok{seq}\NormalTok{(bmi.range[}\DecValTok{1}\NormalTok{], bmi.range[}\DecValTok{2}\NormalTok{], }\AttributeTok{length =} \DecValTok{200}\NormalTok{)}
\NormalTok{beta.x }\OtherTok{=} \FunctionTok{coef}\NormalTok{(s.logit)[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+} \FunctionTok{coef}\NormalTok{(s.logit)[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{*}\NormalTok{x}
\NormalTok{success.prob }\OtherTok{=} \FunctionTok{exp}\NormalTok{(beta.x)}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(beta.x))}
\NormalTok{failure.prob }\OtherTok{=} \DecValTok{1}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(beta.x))}
\NormalTok{ylimit }\OtherTok{=} \FunctionTok{max}\NormalTok{(success.prob, failure.prob)}
\DocumentationTok{\#\#}
\NormalTok{beta1 }\OtherTok{=} \FunctionTok{coef}\NormalTok{(s.logit)[}\DecValTok{2}\NormalTok{]}
\NormalTok{success.prob.rate }\OtherTok{=}\NormalTok{ beta1}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(beta.x)}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(beta.x))}\SpecialCharTok{\^{}}\DecValTok{2}
\DocumentationTok{\#\#}
\DocumentationTok{\#\#}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(x, success.prob, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"navy"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"The probability of being }\SpecialCharTok{\textbackslash{}n}\StringTok{  tested positive in diabetes"}\NormalTok{, }
     \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.1}\SpecialCharTok{*}\NormalTok{ylimit),}
     \AttributeTok{xlab =} \StringTok{"BMI"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"probability"}\NormalTok{,}
     \AttributeTok{axes =} \ConstantTok{FALSE}\NormalTok{,}
     \AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{,}
     \AttributeTok{cex.main =} \FloatTok{0.8}\NormalTok{)}
\CommentTok{\# lines(x, failure.prob,lwd = 2, col = "darkred")}
\FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{pos =} \DecValTok{0}\NormalTok{)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\CommentTok{\# legend(30, 1, c("Success Probability", "Failure Probability"), lwd = rep(2,2), }
\CommentTok{\#       col = c("navy", "darkred"), cex = 0.7, bty = "n")}
\DocumentationTok{\#\#}
\NormalTok{y.rate }\OtherTok{=} \FunctionTok{max}\NormalTok{(success.prob.rate)}
\FunctionTok{plot}\NormalTok{(x, success.prob.rate, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"navy"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"The rate of change in the probability }\SpecialCharTok{\textbackslash{}n}\StringTok{  of being tested positive in diabetes"}\NormalTok{, }
     \AttributeTok{xlab =} \StringTok{"BMI"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Rate of Change"}\NormalTok{,}
     \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1.1}\SpecialCharTok{*}\NormalTok{y.rate),}
     \AttributeTok{axes =} \ConstantTok{FALSE}\NormalTok{,}
     \AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{,}
     \AttributeTok{cex.main =} \FloatTok{0.8}
\NormalTok{     )}
\FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{pos =} \DecValTok{0}\NormalTok{)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-128-1} \end{center}

The left-hand side plot in the above figure is the standard \textbf{S curve} representing how the probability of a positive test increases as the BMI increases. After diving deeper to see the rate of change in the probability of a positive test, we obtain the curve on the right-hand side that indicates that the rate of change in the probability of positive test increases when BMI is less than 40 and decreases when BMI is greater than 40. The turning point is about 40.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

This note focuses on the structure and association analysis of the simple logistic regression model. The case study uses a real-world diabetes data set to illustrate the steps for carrying out the simple logistic regression model.

In the following modules, we will discuss multiple logistic regression models that will include more modeling techniques.

Since logistic regression has been used as a standard machine-learning algorithm for classification, I will use a standalone module to discuss this topic.

\hypertarget{analysis-assignment}{%
\section{Analysis Assignment}\label{analysis-assignment}}

This assignment focuses on the simple linear regression model. Please follow the instructions to pick an appropriate data set for the next three assignments that will be ensembled for project \#2.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find a data set that contains
\end{enumerate}

\begin{itemize}
\tightlist
\item
  at least \textbf{two} categorical variables,
\item
  \textbf{three} numerical variables, and
\item
  a \textbf{binary response variable}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  The number of observations is at least 150. This number of observations is slightly larger than the one required in project \#1 since we will introduce a machine-learning algorithm using the logistic regression model.
\item
  If you have a data set with no binary response variable but you really want to use it for your assignments, you can dichotomize the response variable in a meaningful way. For example, you have a data set with several predictor variables that are potentially associated with the response variable GPA. You can dichotomize the continuous GPA in the following

  bin.gpa = 1 if GPA \textless{} 2.75

  bin.gpa = 0 if GPA \textgreater= 2.75

  This dichotomization is meaningful since most graduate schools use 2.75 as the admission cut-off GPA.
\item
  Choose one numerical predictor variable or a \textbf{binary} predictor variable to fit a simple logistic regression model in this assignment as I did in the case study. Other variables will be used in subsequent assignments.
\end{enumerate}

To be more specific, you need to provide the following key components in your analysis report.

\begin{verbatim}
(a) Describe your data set and the variables.

(b) Formulate a practically meaningful analytic question.

(c) Perform exploratory data analysis using a graphical or numerical approach.

(d) Build a simple logistic regression model.

(e) Interpret the regression coefficients from the practical perspective (odds ratio).

(f) study the behavior of the success probability (probability curve and the rate of change in success probability).
\end{verbatim}

\hypertarget{multiple-logistic-regression-model}{%
\chapter{Multiple Logistic Regression Model}\label{multiple-logistic-regression-model}}

The general multivariable linear regression model is given below.

\[
y = \beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k + \epsilon,
\]

where \(y\) is the response variable that is assumed to be a random variable and \(\epsilon \to N(0, \sigma^2)\). This also implies that

\[
E[y] = \beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k
\]

For a population with binary data, the underlying random variable can only take exactly two values, say \(Y=1\) or \(Y=0\), and \(P(Y=1) = p\), then \(E[Y] = 1\times p + 0\times (1-p) = p.\)

That is, the success probability is the expected value of the binary random variable. If we mimic the formulation of the linear regression model by setting

The simple linear regression model (also called the log-odds regression model) is also formulated with the mean response \(E[Y]\)
\[
\frac{E[Y]}{1-E[Y]} = \beta_0 + \beta_1x.
\]

Let \(g(t) = t/(1-t)\) (also called logit function), the simple logistic regression is re-expressed as \(g(E[Y]) = \beta_0 + \beta_1x\).

\hypertarget{multiple-logistic-regression-model-1}{%
\section{Multiple Logistic Regression Model}\label{multiple-logistic-regression-model-1}}

Let \(Y\) be the binary response variable and \(\{x_1, x_2, \cdots, x_n \}\) be the set of predictor variables. If \(Y\) takes on either 1 or 0, the multiple logistic regression model is then defined as

\[
\frac{E[Y]}{1-E[Y]} = \beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k
\]

The success probability function

\[
p(x_1, x_2, \cdots, x_k)=P(Y=1|x_1, x_2, \cdots, x_k) =\frac{\exp(\beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k)}{1+\exp(\beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k)}
\]

If \(Y\) takes on character values, R uses chooses the alphabetically higher value to model the above probability. For example, if \(Y\) = ``disease'' or ``no.disease'', by default, the logistic regression will be defined as

\[
p(x_1, x_2, \cdots, x_k)=P(Y="no.disease"|x_1, x_2, \cdots, x_k) =\frac{\exp(\beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k)}{1+\exp(\beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k)}
\]
Of cause, you can also redefine the factor level of the response variable to model the probability of the desired category.

\hypertarget{data-requirements-sources-layout-and-cleansing}{%
\subsection{Data Requirements: Sources, Layout, and Cleansing}\label{data-requirements-sources-layout-and-cleansing}}

The logistic regression models we are discussing require an I.I.D. sample collected from a cross-sectional study design. Auto-correlation between observations is \textbf{not} allowed. For longitudinal data that involves auto-correlation, different models can be used to handle the correlation between observations taken from the same subject in the study.

The general data layout for fitting a logistic regression in R has the following form.

\begin{table}

\caption{\label{tab:table2}Data set layout for multiple logistic regression model}
\centering
\begin{tabular}[t]{l|l|l|l|l}
\hline
Y & X1 & X2 &   & Xk\\
\hline
Y1 & X11 & X21 & ... & Xk1\\
\hline
Y2 & X12 & X22 & ... & Xk2\\
\hline
... & ... & ... & ... & ...\\
\hline
Yn & X1n & X2n & ... & Xkn\\
\hline
\end{tabular}
\end{table}

\hypertarget{issues-of-predictor-variables-and-variable-inspection-transformation}{%
\subsection{Issues of Predictor Variables and Variable Inspection-Transformation}\label{issues-of-predictor-variables-and-variable-inspection-transformation}}

All models have some explicit and implicit assumptions about the predictor variables and structure of the models. Unlike multiple linear regression models in which the diagnostic residual plots reveal some special patterns of potential violations of the model assumptions, in logistic regression modeling, we don't have many diagnostic tools to use. Some pre-processing procedures should be performed on predictor variables before a logistic regression model is fit to the data.

The variable inspection-transformation is an iterative process, some of the following potential issues of predictor variables may be considered in the inspection-transformation-inspection workflow.

\begin{itemize}
\item
  \textbf{Variable Types}: Predictor variables could be numeric, categorical, or a mixture of numeric and categorical.
\item
  \textbf{Collinearity}: Predictor variables are assumed to be non-linearly correlated since the multicollinearity causes unstable estimates of the regression coefficients, hence, fails to obtain a valid model. Remedy for collinearity

  \begin{itemize}
  \item
    Remove some of the highly correlated independent variables - Variable selection.
  \item
    Perform an analysis designed for highly correlated variables such as principal components analysis or partial least squares regression - variable extraction.
  \item
    Variable centralization.
  \item
    Non-probabilistic variable selection - Regularization.
  \end{itemize}
\item
  \textbf{Dummy Variables}: If categorical predictor variables were numerically coded, we have to turn these numerically coded variables into factor variables. For example, the status of a disease could be ``severe'', ``mild'', and ``disease-free'', if a numerical coding: 2 = ``severe'', 1 = ``mild'' and 0 = ``disease-free'', then you need to R function \textbf{factor()} to convert the numerically coded disease status to a factor variable.
\item
  \textbf{``Fake Variable''}: The observation ID is \textbf{NOT} a variable, you should \textbf{never} include the observation ID in any of your regression models.
\item
  \textbf{Sparse Category Variables}: Group the categories in a meaningful way if necessary. For example, Assume that you have a data set of information about cars of different models from various manufacturers. If you want to build regression on a data set with a relatively small sample size, the use of the car-model as a categorical variable is not appropriate since too many different car models will result in too many dummy variables. From a mathematical point of view, the number of parameters should be less than the number of data points. However, from the statistical point of view, the desired sample size is 15 times the number of parameters to ensure stable estimates of model parameters.
\item
  \textbf{Variable transformation}: - In logistic regression models, the response variable has already been transformed in the form of log odds of ``success''. The predictor variables could be transformed in different ways for different purposes.

  \begin{itemize}
  \item
    \textbf{Association Analysis} - a transformation of predictor variables makes the interpretation of the coefficient much more difficult.
  \item
    \textbf{Predictive Analysis} - transforming all \emph{numerical variables} to the same scale may improve the performance of predictive models. One of the benefits of standardizing predictor variables is to make variable selection (model regularization) straightforward and interpretable.
  \end{itemize}
\item
  \textbf{Variable Discretization} - several methods can be used for the discretization: (1) \textbf{empirical approaches} include equally spaced and equal frequency, (2) \textbf{model-assisted approaches} include decision tree and k-mean. Discretization is commonly used for different purposes.

  \begin{itemize}
  \item
    Model interpretability and understandability - it is easier to understand continuous data (such as age) when divided and stored into meaningful categories or groups. It is commonly used in association analysis.
  \item
    Fixing the potential imbalance issues that could potentially lead to an unstable estimate of the coefficients.
  \end{itemize}
\end{itemize}

\hypertarget{estimation-and-interpretation-of-regression-coefficients}{%
\subsection{Estimation and Interpretation of Regression Coefficients}\label{estimation-and-interpretation-of-regression-coefficients}}

As mentioned in the simple logistic regression model, regression coefficients are estimated by using the maximum likelihood approach.

Let \(\{(y_1, x_{11}, x_{21}, \cdots, x_{k1}), (y_2, x_{12}, x_{22}, \cdots, x_{k2}), \cdots, (y_n, x_{1n}, x_{2n}, \cdots, x_{kn})\}\) be a random sample taken from a binary population associated with \(Y\). \(x\) is a nonrandom predictor variable associated with \(Y\). The logistic model is defined to be

\[
p(x) = \frac{e^{\beta_0 + \beta_1 x_1 +\cdots + \beta_kx_k}}{1 + e^{\beta_0 + \beta_1 x_1 + \cdots + \beta_kx_k}}.
\]
The likelihood function of \((\beta_0, \beta_1, \cdots, \cdots, \beta_k)\) is given by

\[
L(\beta_0, \beta_1,\cdots, \beta_k)
= \prod_{i=1}^n \left[\frac{e^{\beta_0 + \beta_1 x_{i1}+\cdots+\beta_kx_{ik}}}{1 + e^{\beta_0 + \beta_1 x_{i1}+\cdots+\beta_kx_{ik}}}\right]^{y_i}\times \left[\frac{1}{1 + e^{\beta_0 + \beta_1 x_{i1}+\cdots+\beta_kx_{ik}}} \right]^{1-y_i}
\]

The maximum likelihood estimate (MLE) of \(\beta_0, \beta_1, \cdots, \beta_k\), denoted by \(\hat{\beta}_0,\hat{\beta}_1, \cdots, \hat{\beta}_k\), maximizes the above likelihood. The R build-in function \textbf{glm()} uses the MLE method to estimate parameters and reports related to MLE-based statistics.

The coefficients are interpreted similarly as used in the simple logistic regression model. To interpret \(\beta_j\) in the multiple logistic regression model,

\[
\log\left(\frac{P[Y=1|\cdots,x_j,\cdots]}{1-P[Y=1|\cdots,x_j,\cdots]}\right) = \beta_0 + \beta_1x_1 + \cdots + \beta_{j-1}x_{j-1}+ \beta_jx_j + \beta_{j+1} + \cdots+\beta_kx_k
\]
If we \textbf{fix the values of all \(X_i\) except for increasing \(x_j\) by one unit}, then

\[
\log\left(\frac{P[Y=1|\cdots,(x_j+1),\cdots]}{1-P[Y=1|\cdots,(x_j+1),\cdots]}\right) = \beta_0 + \beta_1x_1 + \cdots + \beta_{j-1}x_{j-1}+ \beta_j(x_j+1) + \beta_{j+1}x_{j+1} + \cdots+\beta_kx_k
\]
Then

\[
\beta_j = \log\left(\frac{P[Y=1|\cdots,(x_j+1),\cdots]}{1-P[Y=1|\cdots,(x_j+1),\cdots]}\right) - \log\left(\frac{P[Y=1|\cdots,x_j,\cdots]}{1-P[Y=1|\cdots,x_j,\cdots]}\right)
=\log \left(\frac{\frac{P[Y=1|\cdots,(x_j+1),\cdots]}{1-P[Y=1|\cdots,(x_j+1),\cdots]}}{\frac{P[Y=1|\cdots,x_j,\cdots]}{1-P[Y=1|\cdots,x_j,\cdots]}}\right)
\]

Therefore, \(\beta_j\) (for \(j=1, 2, \cdots, k\)) is the log odds ratio as explained in the simple logistic regression model.

If \(\beta_j = 0\), then \(x_j\) is insignificant meaning that the odds of ``success'' in a subset of subjects with predictor values \(\{x_1, \cdots, x_{j-1}, x_{j+1}, x_{j+1}, \cdots, x_k \}\) is equal to the odds of ``success'' in other subsets with predictor values \(\{x_1, \cdots, x_{j-1}, x_{j}, x_{j+1}, \cdots, x_k \}\).

\hypertarget{building-blocks-for-predictive-performance}{%
\section{Building Blocks for Predictive Performance}\label{building-blocks-for-predictive-performance}}

Prediction in the logistic regression is not straightforward. The logistic regression function

\[
p(x) = \frac{e^{\beta_0 + \beta_1 x_1 +\cdots + \beta_kx_k}}{1 + e^{\beta_0 + \beta_1 x_1 + \cdots + \beta_kx_k}}.
\]

predicts the probability of ``success'' but not the status of ``success''. In order to predict the value of \(Y\), we still need to have a cut-off probability to define the predicted ``success'' and ``failure''. How to find the optimal cut-off probability will be addressed later module.

Next, we assume there is a cut-off probability for predicting the original value of the response variable. Most software programs as R and SAS use 0.5 as the cut-off for predicting \(Y\).

\hypertarget{understanding-the-performance-of-medical-diagnostics}{%
\subsection{Understanding the Performance of Medical Diagnostics}\label{understanding-the-performance-of-medical-diagnostics}}

In this section, we define some performance metrics of the logistic regression as a predictive model based on the predictive error. For ease of illustration, we consider a simple model using a diagnostic test result (X = T+ or T-) to predict a disease (Y = D+ or D-) to define these performance metrics where

T+ = positive test result: numerical coding 1

T- = negative test result: numerical coding 0

D+ = diseased: numerical coding 1

D- = disease-free: numerical coding 0

The following metrics measure the predictive performance of the logistic regression model. The first two measures reflect the correct decision of the model and the last two error rates of the logistic regression model.

\begin{itemize}
\item
  \textbf{Positive Predictive Value}: P(Y = D+ \textbar{} X = T+)

  \(PPV =P(Y=1|X=1)= \frac{e^{\beta_0+\beta_1}}{1+e^{\beta_0+\beta_1}}\)
\item
  \textbf{Negative Predictive Value}: P(Y = D- \textbar{} X = T-)

  \(NPV=P(Y=1|X=0)= \frac{e^{\beta_0}}{1+e^{\beta_0}}\)
\item
  \textbf{False Positive Predictive Rate}: P(Y = D- \textbar{} X = T+)

  \(FPPV=P(Y=0|X=1)= \frac{1}{1+e^{\beta_0+\beta_1}}\)
\item
  \textbf{False Positive Predictive Rate}: P(Y = D+ \textbar{} X = T-)

  \(FNPV=P(Y=0|X=0)= \frac{1}{1+e^{\beta_0}}\)
\end{itemize}

The above four conditional probabilities can also be estimated by calculating the corresponding relative frequencies from the following two-way contingency table - also called \textbf{confusion matrix}. For convenience, we call the above four metrics \textbf{prediction performance metrics}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{D1 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"n11"}\NormalTok{, }\StringTok{"n12"}\NormalTok{)}
\NormalTok{D0 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"n21"}\NormalTok{, }\StringTok{"n22"}\NormalTok{)}
\NormalTok{M}\OtherTok{=}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(D1, D0))}
\FunctionTok{names}\NormalTok{(M)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"T+"}\NormalTok{, }\StringTok{"T{-}"}\NormalTok{)}
\FunctionTok{row.names}\NormalTok{(M) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"D+"}\NormalTok{, }\StringTok{"D{-}"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(M)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l}
\hline
  & T+ & T-\\
\hline
D+ & n11 & n12\\
\hline
D- & n21 & n22\\
\hline
\end{tabular}

The above four metrics are used by clinical diagnosis after the test was approved by the FDA since the diagnostic decision is based on the test result.

\hypertarget{performance-metrics-used-in-clinical-trials}{%
\subsection{Performance Metrics Used in Clinical Trials:}\label{performance-metrics-used-in-clinical-trials}}

Now, let's consider the case that a manufacturer conducting a clinical phase II trial and submitting the results for FDA approval. The FDA uses the following metrics in the approval process.

\begin{itemize}
\item
  \textbf{Sensitivity}: P( T+ \textbar{} D+)
\item
  \textbf{Specificity}: P( T- \textbar{} D-)
\item
  \textbf{False Negative Rate}: P( T- \textbar{} D+)
\item
  \textbf{False Positive Rate}: P( T+ \textbar{} D-)
\end{itemize}

The above metrics are well-defined since the disease status of subjects is known in the clinical trial. The estimated values of these metrics can be found in the clinical data. For convenience, we call the above four metrics \textbf{Validation Performance Metrics}.

\hypertarget{remarks}{%
\subsection{Remarks}\label{remarks}}

Here are several remarks on the above two sets of performance metrics.

\begin{itemize}
\item
  The \textbf{prediction performance metrics} are dependent on the choice of the cut-off ``success'' probability. They can be estimated from a fitted logistic regression model.
\item
  The \textbf{validation performance metrics} are defined based on the data with known disease status. A proposed diagnostic test is good if both sensitivity and specificity are high.
\item
  Thinking about the logistic regression model you developed as ``a diagnostic test'' (since it can predict the status of a disease), which sets of metrics you should use to show the goodness of your model? The answer is the set of \textbf{validation performance metrics}.
\item
  \textbf{Sensitivity and Specificity} are the basic building blocks used to define various performance metrics to assess the goodness of the predictive model using the \textbf{testing data} with known response values. This will be one of the major topics in the next module.
\end{itemize}

\hypertarget{case-study}{%
\section{Case Study}\label{case-study}}

In this case study, we still use the diabetes data that was used in the last module.

\hypertarget{data-and-variable-descriptions-1}{%
\section{Data and Variable Descriptions}\label{data-and-variable-descriptions-1}}

There are 9 variables in the data set.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{pregnant}: Number of times pregnant
\item
  \textbf{glucose}: Plasma glucose concentration (glucose tolerance test)
\item
  \textbf{pressure}: Diastolic blood pressure (mm Hg)
\item
  \textbf{triceps}: Triceps skin fold thickness (mm)
\item
  \textbf{insulin}: 2-Hour serum insulin (mu U/ml)
\item
  \textbf{mass}: Body mass index (weight in kg/(height in m)\^{}2)
\item
  \textbf{pedigree}: Diabetes pedigree function
\item
  \textbf{age}: Age (years)
\item
  \textbf{diabetes}: Class variable (test for diabetes)
\end{enumerate}

I load the data from R \textbf{library\{mlbench\}} in the following code. For convenience, I delete all records with missing values and keep only the records with complete records in this case study. The final analytic data set has 392 records. This

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mlbench)}
\FunctionTok{data}\NormalTok{(PimaIndiansDiabetes2)           }\CommentTok{\# load the data to R work{-}space}
\NormalTok{diabetes}\FloatTok{.0} \OtherTok{=}\NormalTok{ PimaIndiansDiabetes2    }\CommentTok{\# make a copy of the data for data cleansing }
\NormalTok{diabetes }\OtherTok{=} \FunctionTok{na.omit}\NormalTok{(diabetes}\FloatTok{.0}\NormalTok{)       }\CommentTok{\# Delete all records with missing components}
\CommentTok{\#head(diabetes)}
\end{Highlighting}
\end{Shaded}

\hypertarget{research-question}{%
\subsection{Research Question}\label{research-question}}

The objective of this case study is to identify the risk factors for diabetes.

\hypertarget{exploratory-analysis}{%
\subsection{Exploratory Analysis}\label{exploratory-analysis}}

We first make the following pairwise scatter plots to inspect the potential issues with predictor variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 载入程辑包：'psych'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:scales':
## 
##     alpha, rescale
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs.panels}\NormalTok{(diabetes[,}\SpecialCharTok{{-}}\DecValTok{9}\NormalTok{], }
             \AttributeTok{method =} \StringTok{"pearson"}\NormalTok{, }\CommentTok{\# correlation method}
             \AttributeTok{hist.col =} \StringTok{"\#00AFBB"}\NormalTok{,}
             \AttributeTok{density =} \ConstantTok{TRUE}\NormalTok{,  }\CommentTok{\# show density plots}
             \AttributeTok{ellipses =} \ConstantTok{TRUE} \CommentTok{\# show correlation ellipses}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-131-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{detach}\NormalTok{(}\StringTok{"package:psych"}\NormalTok{, }\AttributeTok{unload =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

From the correlation matrix plot, we can see several patterns in the predictor variables.

\begin{itemize}
\tightlist
\item
  All predictor variables are unimodal. But \textbf{pregnant} and \textbf{age} are significantly skewed. We next take a close look at the frequency distribution of these two variables.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{hist}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{pregnant, }\AttributeTok{xlab=}\StringTok{"pregnant"}\NormalTok{, }\AttributeTok{main =} \StringTok{""}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{age, }\AttributeTok{xlab =} \StringTok{"age"}\NormalTok{, }\AttributeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-132-1} \end{center}

Based on the above histogram, we discretize \textbf{pregnant} and \textbf{age} in the following.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{preg }\OtherTok{=}\NormalTok{ diabetes}\SpecialCharTok{$}\NormalTok{pregnant}
\NormalTok{grp.preg }\OtherTok{=}\NormalTok{ preg}
\NormalTok{grp.preg[preg }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)] }\OtherTok{=} \StringTok{"4{-}6"}
\NormalTok{grp.preg[preg }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{7}\SpecialCharTok{:}\DecValTok{9}\NormalTok{)] }\OtherTok{=} \StringTok{"7{-}9"}
\NormalTok{grp.preg[preg }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\SpecialCharTok{:}\DecValTok{17}\NormalTok{)] }\OtherTok{=} \StringTok{"10+"}
\DocumentationTok{\#\#}
\NormalTok{age }\OtherTok{=}\NormalTok{ diabetes}\SpecialCharTok{$}\NormalTok{age}
\DocumentationTok{\#\#}
\NormalTok{grp.age }\OtherTok{=}\NormalTok{ age}
\NormalTok{grp.age[age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{21}\SpecialCharTok{:}\DecValTok{24}\NormalTok{)] }\OtherTok{=} \StringTok{"21{-}25"}
\NormalTok{grp.age[age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\SpecialCharTok{:}\DecValTok{30}\NormalTok{)] }\OtherTok{=} \StringTok{"25{-}30"}
\NormalTok{grp.age[age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{31}\SpecialCharTok{:}\DecValTok{40}\NormalTok{)] }\OtherTok{=} \StringTok{"31{-}40"}
\NormalTok{grp.age[age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{41}\SpecialCharTok{:}\DecValTok{50}\NormalTok{)] }\OtherTok{=} \StringTok{"41{-}50"}
\NormalTok{grp.age[age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{51}\SpecialCharTok{:}\DecValTok{99}\NormalTok{)] }\OtherTok{=} \StringTok{"50 +"}
\DocumentationTok{\#\# added to the diabetes data set}
\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{grp.age }\OtherTok{=}\NormalTok{ grp.age}
\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{grp.preg }\OtherTok{=}\NormalTok{ grp.preg}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  A moderate correlation is observed in several pairs of variables: \textbf{age} v.s. \textbf{pregnant}, \textbf{glucose} v.s. \textbf{insulin}, and \textbf{triceps} v.s. \textbf{mass}. We will not drop any of these variables for the moment but will perform an automatic variable selection process to remove potential redundant variables since a few of them will be forced to be included in the final model.
\item
  Since our goal is association analysis, we will not perform variable transformations for the time being.
\item
  It is very common in real-world applications that some of the practically important variables are always included in the final model regardless of their statistical significance. In the diabetes study, three insulin, MBI, and pedigree are considered significant risk factors. We will include these three variables in the final model. This means the smallest model must have these three variables.
\end{itemize}

\hypertarget{building-the-multiple-logistic-regression-model}{%
\subsection{Building the Multiple Logistic Regression Model}\label{building-the-multiple-logistic-regression-model}}

Based on the above exploratory analysis, we first build the full model and the smallest model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{full.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(diabetes }\SpecialCharTok{\textasciitilde{}}\NormalTok{grp.preg}\SpecialCharTok{+}\NormalTok{glucose}\SpecialCharTok{+}\NormalTok{pressure}\SpecialCharTok{+}\NormalTok{triceps}\SpecialCharTok{+}\NormalTok{insulin}\SpecialCharTok{+}\NormalTok{mass}\SpecialCharTok{+}\NormalTok{pedigree}\SpecialCharTok{+}\NormalTok{grp.age, }
          \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),  }\CommentTok{\#  logit(p) = log(p/(1{-}p))!}
          \AttributeTok{data =}\NormalTok{ diabetes)  }
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(full.model)}\SpecialCharTok{$}\NormalTok{coef, }
      \AttributeTok{caption=}\StringTok{"Summary of inferential statistics of the full model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-134}Summary of inferential statistics of the full model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & -8.7503322 & 1.3331796 & -6.5635058 & 0.0000000\\
\hline
grp.preg1 & -0.3977538 & 0.5093087 & -0.7809679 & 0.4348214\\
\hline
grp.preg10+ & 0.3935217 & 0.7902561 & 0.4979673 & 0.6185071\\
\hline
grp.preg2 & -0.2958769 & 0.5571478 & -0.5310564 & 0.5953797\\
\hline
grp.preg3 & 0.3842810 & 0.5700718 & 0.6740923 & 0.5002526\\
\hline
grp.preg4-6 & -0.9140847 & 0.5668123 & -1.6126761 & 0.1068149\\
\hline
grp.preg7-9 & -0.2284477 & 0.6514907 & -0.3506538 & 0.7258481\\
\hline
glucose & 0.0382521 & 0.0060502 & 6.3224381 & 0.0000000\\
\hline
pressure & -0.0072937 & 0.0121999 & -0.5978495 & 0.5499403\\
\hline
triceps & 0.0107744 & 0.0180674 & 0.5963470 & 0.5509435\\
\hline
insulin & -0.0002279 & 0.0013854 & -0.1645109 & 0.8693290\\
\hline
mass & 0.0583354 & 0.0284914 & 2.0474744 & 0.0406115\\
\hline
pedigree & 1.0412541 & 0.4461747 & 2.3337362 & 0.0196095\\
\hline
grp.age25-30 & 1.0829606 & 0.4193260 & 2.5826224 & 0.0098053\\
\hline
grp.age31-40 & 1.5405843 & 0.4909205 & 3.1381545 & 0.0017002\\
\hline
grp.age41-50 & 2.2601500 & 0.6144945 & 3.6780639 & 0.0002350\\
\hline
grp.age50 + & 2.0014940 & 0.7174832 & 2.7896040 & 0.0052773\\
\hline
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reduced.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(diabetes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ insulin }\SpecialCharTok{+}\NormalTok{ mass }\SpecialCharTok{+}\NormalTok{ pedigree, }
          \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),  }\CommentTok{\# logit(p) = log(p/(1{-}p))!}
          \AttributeTok{data =}\NormalTok{ diabetes) }
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(reduced.model)}\SpecialCharTok{$}\NormalTok{coef, }
      \AttributeTok{caption=}\StringTok{"Summary of inferential statistics of the reduced model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-135}Summary of inferential statistics of the reduced model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & -4.3288188 & 0.6463940 & -6.696874 & 0.0000000\\
\hline
insulin & 0.0047984 & 0.0010835 & 4.428585 & 0.0000095\\
\hline
mass & 0.0673268 & 0.0178494 & 3.771943 & 0.0001620\\
\hline
pedigree & 1.0779952 & 0.3601870 & 2.992876 & 0.0027636\\
\hline
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# automatic variable selection}
\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{final.model.forward }\OtherTok{=} \FunctionTok{stepAIC}\NormalTok{(reduced.model, }
                      \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower=}\FunctionTok{formula}\NormalTok{(reduced.model),}\AttributeTok{upper=}\FunctionTok{formula}\NormalTok{(full.model)),}
                      \AttributeTok{direction =} \StringTok{"forward"}\NormalTok{,   }\CommentTok{\# forward selection}
                      \AttributeTok{trace =} \DecValTok{0}   \CommentTok{\# do not show the details}
\NormalTok{                      )}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(final.model.forward)}\SpecialCharTok{$}\NormalTok{coef, }
      \AttributeTok{caption=}\StringTok{"Summary of inferential statistics of the final model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-136}Summary of inferential statistics of the final model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & -9.5340546 & 1.0941037 & -8.7140321 & 0.0000000\\
\hline
insulin & -0.0005617 & 0.0013568 & -0.4140043 & 0.6788710\\
\hline
mass & 0.0703082 & 0.0212351 & 3.3109430 & 0.0009298\\
\hline
pedigree & 1.0567094 & 0.4305469 & 2.4543423 & 0.0141143\\
\hline
glucose & 0.0387018 & 0.0058814 & 6.5804090 & 0.0000000\\
\hline
grp.age25-30 & 1.0012238 & 0.3918271 & 2.5552695 & 0.0106106\\
\hline
grp.age31-40 & 1.3679308 & 0.4163693 & 3.2853782 & 0.0010185\\
\hline
grp.age41-50 & 2.1962925 & 0.4712387 & 4.6606796 & 0.0000032\\
\hline
grp.age50 + & 1.9277491 & 0.5802575 & 3.3222305 & 0.0008930\\
\hline
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Other global goodness{-}of{-}fit}
\NormalTok{global.measure}\OtherTok{=}\ControlFlowTok{function}\NormalTok{(s.logit)\{}
\NormalTok{dev.resid }\OtherTok{=}\NormalTok{ s.logit}\SpecialCharTok{$}\NormalTok{deviance}
\NormalTok{dev.}\FloatTok{0.}\NormalTok{resid }\OtherTok{=}\NormalTok{ s.logit}\SpecialCharTok{$}\NormalTok{null.deviance}
\NormalTok{aic }\OtherTok{=}\NormalTok{ s.logit}\SpecialCharTok{$}\NormalTok{aic}
\NormalTok{goodness }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{Deviance.residual =}\NormalTok{dev.resid, }\AttributeTok{Null.Deviance.Residual =}\NormalTok{ dev.}\FloatTok{0.}\NormalTok{resid,}
      \AttributeTok{AIC =}\NormalTok{ aic)}
\NormalTok{goodness}
\NormalTok{\}}
\NormalTok{goodness}\OtherTok{=}\FunctionTok{rbind}\NormalTok{(}\AttributeTok{full.model =} \FunctionTok{global.measure}\NormalTok{(full.model),}
      \AttributeTok{reduced.model=}\FunctionTok{global.measure}\NormalTok{(reduced.model),}
      \AttributeTok{final.model=}\FunctionTok{global.measure}\NormalTok{(final.model.forward))}
\FunctionTok{row.names}\NormalTok{(goodness) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"full.model"}\NormalTok{, }\StringTok{"reduced.model"}\NormalTok{, }\StringTok{"final.model"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(goodness, }\AttributeTok{caption =}\StringTok{"Comparison of global goodness{-}of{-}fit statistics"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-137}Comparison of global goodness-of-fit statistics}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & Deviance.residual & Null.Deviance.Residual & AIC\\
\hline
full.model & 324.9106 & 498.0978 & 358.9106\\
\hline
reduced.model & 434.7276 & 498.0978 & 442.7276\\
\hline
final.model & 334.2005 & 498.0978 & 352.2005\\
\hline
\end{tabular}
\end{table}

\hypertarget{final-model-1}{%
\subsection{Final Model}\label{final-model-1}}

In the exploratory analysis, we observed three pairs of variables are linearly correlated. After automatic variable selection, triceps and age were dropped out from the final model. Both insulin and glucose are still in the model. Although insulin is statistically insignificant, we still include it in the model since it is clinically important.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Odds ratio}
\NormalTok{model.coef.stats }\OtherTok{=} \FunctionTok{summary}\NormalTok{(final.model.forward)}\SpecialCharTok{$}\NormalTok{coef}
\NormalTok{odds.ratio }\OtherTok{=} \FunctionTok{exp}\NormalTok{(}\FunctionTok{coef}\NormalTok{(final.model.forward))}
\NormalTok{out.stats }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(model.coef.stats, }\AttributeTok{odds.ratio =}\NormalTok{ odds.ratio)                 }
\FunctionTok{kable}\NormalTok{(out.stats,}\AttributeTok{caption =} \StringTok{"Summary Stats with Odds Ratios"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-138}Summary Stats with Odds Ratios}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|) & odds.ratio\\
\hline
(Intercept) & -9.5340546 & 1.0941037 & -8.7140321 & 0.0000000 & 0.0000723\\
\hline
insulin & -0.0005617 & 0.0013568 & -0.4140043 & 0.6788710 & 0.9994384\\
\hline
mass & 0.0703082 & 0.0212351 & 3.3109430 & 0.0009298 & 1.0728388\\
\hline
pedigree & 1.0567094 & 0.4305469 & 2.4543423 & 0.0141143 & 2.8768887\\
\hline
glucose & 0.0387018 & 0.0058814 & 6.5804090 & 0.0000000 & 1.0394605\\
\hline
grp.age25-30 & 1.0012238 & 0.3918271 & 2.5552695 & 0.0106106 & 2.7216105\\
\hline
grp.age31-40 & 1.3679308 & 0.4163693 & 3.2853782 & 0.0010185 & 3.9272159\\
\hline
grp.age41-50 & 2.1962925 & 0.4712387 & 4.6606796 & 0.0000032 & 8.9916151\\
\hline
grp.age50 + & 1.9277491 & 0.5802575 & 3.3222305 & 0.0008930 & 6.8740202\\
\hline
\end{tabular}
\end{table}

The interpretation of the odds ratios is similar to the case of simple logistic regression. The group-age variable \textbf{grp.age} has five categories. The baseline category is aged 21-24. We can see from the above table inferential table that the odds of getting diabetes increase as age increases. For example, the odds ratio associated with the age group 31-39 is 3.927 meaning that, given the same level of insulin, BMI, pedigree, and glucose, the odds of being diabetic in the age group of 31-40 is almost 4 times of that in the baseline group aged 21-24. But the same ratio becomes nine times when comparing the age group 41-50 with the baseline group of age 21-24.

\hypertarget{summary-and-conclusion}{%
\subsection{Summary and Conclusion}\label{summary-and-conclusion}}

The case study focused on the association analysis between a set of potential risk factors for diabetes. The initial data set has 8 numerical and categorical variables.

After exploratory analysis, we decide to re-group two sparse discrete variables \textbf{pregnant} and \textbf{age}, and then define dummy variables for the associated variables. These new group variables were used in the model search process.

Since \textbf{insulin}, \textbf{BMI}, and \textbf{pedigree} are considered to be major contributors to the development of diabetes, we include three risk factors in the final model regardless of the statistical significance.

After automatic variable selection, we obtain the final model with 4 factors, BMI, pedigree, glucose, age (with 4 dummy variables), and insulin (that is not statistically significant but clinically important).

Diabetes prediction or classification is another important practical issue. We will address this practical question in the next module.

\hypertarget{analysis-assignment-1}{%
\section{Analysis Assignment}\label{analysis-assignment-1}}

This assignment focuses on multiple logistic regression modeling using the same data set you used in the previous week. To be more specific, the data set has to meet the following requirements:

\begin{itemize}
\item
  The response variable must be binary. It could be made by dichotomizing a numerical variable or regrouping a categorical variable.
\item
  At least two continuous predictor variables
\item
  At least two categorical predictor variables
\item
  The sample size should be at least 15 times the total number of numerical variables and \emph{dummy} variables.
\end{itemize}

\textbf{Components of the analysis report}

The report should contain the same components as I included in the case study in this week's class note. Please keep in mind that the interpretation of results is VERY important.

\begin{itemize}
\item
  Description of your data set and variables
\item
  Research questions
\item
  Data management and variable inspection

  \begin{itemize}
  \item
    variable creation based on existing variables
  \item
    variable transformation
  \item
    variable discretization
  \item
    handling sparse categorical variables
  \end{itemize}
\item
  model building process

  \begin{itemize}
  \item
    candidate models
  \item
    manual variable selection
  \item
    automatic variable selection
  \item
    final model identification
  \item
    summarize the inferential statistics in the final model.
  \end{itemize}
\item
  Conclusion and discussion
\end{itemize}

\textbf{Remarks}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  This assignment focuses only on the association analysis.
\item
  Convert the regression coefficients in the final to odds ratio and then provide practical interpretation.
\item
  The global goodness-of-fit measures (deviance, AIC, etc) in all candidate models should be reported during model selection.
\end{enumerate}

\hypertarget{predictive-modeling-with-logistic-regression}{%
\chapter{Predictive Modeling with Logistic Regression}\label{predictive-modeling-with-logistic-regression}}

The logistic regression model as a member of the family of the generalized linear regression model has the following form.

Let \(Y\) be the binary response variable and \(\{x_1, x_2, \cdots, x_n \}\) be the set of predictor variables. If \(Y\) takes on either 1 or 0, the multiple logistic regression model is then defined as

\[
\frac{E[Y]}{1-E[Y]} = \beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k
\]

The success probability function

\[
p(x_1, x_2, \cdots, x_k)=P(Y=1|x_1, x_2, \cdots, x_k) =\frac{\exp(\beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k)}{1+\exp(\beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k)}
\]

If \(Y\) takes on character values, R uses chooses the alphabetically higher value to model the above probability. For example, if \(Y\) = ``diseased'' or ``disease-free'', by default, the logistic regression will be defined as

\[
p(x_1, x_2, \cdots, x_k)=P(Y= \text{disease-free}|x_1, x_2, \cdots, x_k) =\frac{\exp(\beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k)}{1+\exp(\beta_0 + \beta_1x_1 + \beta_2 x_2 + \cdots + \beta_kx_k)}
\]

We can also redefine the factor level of the response variable to model the probability of the desired category.

In the previous module, we introduced the strategies for searching the final model using both manual and automatic variable selection approaches using likelihood-based performance metrics. However, in predictive modeling, the classical model search strategies might not work satisfactorily. It is a common practice to use data-driven and algorithm-based approaches to assess the predictive performance of a predictive model or an algorithm.

In the following sections, we assume that a set of several candidate models/algorithms have already been developed. The candidate models and algorithms include logistic regression models and other algorithms such as tree-based algorithms, neural nets, support vector machines, etc. The objective is to use various methods to choose the one with the best predictive power to implement in real-world applications.

\hypertarget{cross-validation-in-predictive-modeling}{%
\section{Cross-Validation in Predictive Modeling}\label{cross-validation-in-predictive-modeling}}

In classical statistical modeling, we evaluate the predictive performance of a model using the large sample theory developed based on the likelihood. However, in algorithm-based predictive modeling, we don't assume the distribution of the population or the underlying population is uncertain. In other words, there is no general theory that can derive the predictive performance. Therefore, data-driven methods are used to define the goodness of the model. The key is to hold up a portion of the sample as ``unseen'' observations to test the actual performance of the predictive models and algorithms.

\hypertarget{the-logic-of-the-three-way-split}{%
\subsection{The Logic of the Three-way Split}\label{the-logic-of-the-three-way-split}}

The general idea is to randomly partition the data into several subsets for different purposes. One of them will be used to build the model, one is used to validate the model, and one is used as ``unseen real-world'' data to report the ``actual'' performance of the final model.

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{img08/w08-three3WaySplit} 

}

\caption{Three-way splitting mechanism}\label{fig:unnamed-chunk-139}
\end{figure}

With this three-way split, the \textbf{model selection} and the \textbf{true error rate computation} can be carried out simultaneously. One important observation is that the error rate estimate of the final model on validation data is, in general, underestimated since the validation set is used to select the final model. Therefore, a third independent part of the data, the \textbf{test data}, is needed to report the actual prediction performance.

\textbf{After the performance of the final model was evaluated on the test set, all estimated parameters of the model based on the training data \emph{must not} be changed any further}.

The three-way splitting method may face practical challenges such as the insufficiency of the data and variation of the error estimate based on a single-step process of model development. To overcome these potential challenges, researchers and practitioners developed resampling-based cross-validation (CV) methods to combine training, validating, and testing in an iterative process. The following subsection outlines this

\hypertarget{cross-validation-cv}{%
\subsection{Cross-validation (CV)}\label{cross-validation-cv}}

Among the methods available for estimating prediction error, the most widely used is cross-validation. Contrary to most people who thought that CV was developed by machine learning researchers, it was developed by statisticians. The idea was initially formulated in the 1930s and formally published in statistics literature in the 1970s. Maybe due to computational constraints, the cross-validation methods have not been used by the statistics community. Recently, machine learning researchers dug this old but gold algorithm out for assessing the performance of predictive modeling and algorithms.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{img08/w08-One-step-vs-CV} 

}

\caption{Three-way data splitting versus cross-validation}\label{fig:unnamed-chunk-140}
\end{figure}

Essentially cross-validation includes techniques to split the sample into multiple training and test data sets. To obtain multiple training and testing subsets, we use various random sub-sampling methods to perform K data splits of the entire sample. Different random splits constitute different cross-validation methods. The most commonly used cross-validation method is k-fold cross-validation.

The following are steps for choosing the final working model from a set of candidate models using the cross-validation method.

\begin{itemize}
\item
  \textbf{Step 1}: Split the entire data into a training set and a testing set.
\item
  \textbf{Step 2}: Perform the cross-validation in the training set and don't touch the test data.

  \begin{itemize}
  \tightlist
  \item
    \emph{step 2.1}. Randomly split the training set into k subsets with equal size, say,\(F_1\), \(F_2\), \(\cdots\), \(F_k\).
  \item
    \emph{step 2.2}. hold up \(F_i\) and combine \(T_i =\{\)\(\cdots\), \(F_{i-1}\), \(F_{i+1}\), \(\cdots\) \(\}\). Fit all candidate models to \(T_i\) and then use the fitted model to predict the response using the testing set \(F_i\), for \(i=1, 2, \cdots, k\).\\
  \item
    \emph{step 2.3}. Since the actual response values are available in the test set \(F_i\), we can compare the predicted response value with the true response values to calculate the predictive error for each candidate model in each of the k-rounds.
  \end{itemize}
\item
  \textbf{Step 3}: Calculate the average of the predictive errors for each candidate model. The one with the smallest average predictive error is the winner.
\item
  \textbf{Step 4}: Fit the winner to the entire training set \(\{F_1, F_2, \cdots, F_k\}\) to obtain the final working model.
\item
  \textbf{Step 5}: Report the \emph{actual} predictive error using the testing data set that has not been used in the above cross-validation process.
\end{itemize}

The above process is summarized in the following figure.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img08/w08-CV-process} 

}

\caption{The workflow of the cross-validation process}\label{fig:unnamed-chunk-141}
\end{figure}

\textbf{Remarks} on the cross-validation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The proportions of the training and testing sets are determined by modelers. 75\%:25\% is a commonly used ratio.
\item
  The number of folds used in the cross-validation is the \textbf{hyper-parameter} or \textbf{tuning-parameter}. The 10-fold cross-validation is commonly used. The reason 10-fold is commonly used is that when \(k \ge 10\), the predictive errors in the training and testing become stable.
\end{enumerate}

\hypertarget{predictive-performance-measures}{%
\section{Predictive Performance Measures}\label{predictive-performance-measures}}

Most of the predictive performance measures are defined based on the predictive errors in logistic modeling.

\hypertarget{error-based-measures}{%
\subsection{Error-based Measures}\label{error-based-measures}}

We introduced two sets of conditional probabilities using clinical language in the previous module. As an analogy, we can think about the logistic predictive model to be a \textbf{diagnostics test} for predicting a specific disease. Note that what the logistic model predicted is the probability of having a \textbf{disease}. In order to predict the value ( T+ = ``predicted disease'' or T- = ``predicted no-disease'') of the binary response, we need to use a cut-off probability to define ``T+'' and ``T-''. In most software programs, 0.5 was used. If the success probability is greater than 0.5, the predicted value of the response will be ``T+'', otherwise, the predicted response value will be ``T-''.

Since each of the hold-up validating set has the true values of the response, we then can use corresponding predicted values based on a selected cut-off probability to make a two-way table in the following form

\begin{longtable}[]{@{}lcr@{}}
\toprule\noalign{}
& Actual pos & Actual neg \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Predict pos} & a (TP) & b (FP) \\
\textbf{Predict neg} & c (FN) & d (TN) \\
\end{longtable}

The above two-way contingency table is called the \textbf{confusion matrix} which can be used to estimate the following four performance measures introduced in the previous module.

\begin{itemize}
\item
  \textbf{Sensitivity}: \[TPR = \widehat{P( T+ | D+)} = TP/(TP+FN)\]
\item
  \textbf{Specificity}: \[TNR = \widehat{P( T- | D-)}=TN/(TN+FP)\]
\item
  \textbf{False Negative Rate}: \[FNR = \widehat{P( T- | D+)} = FN/(TP+FN)\]
\item
  \textbf{False Positive Rate}: \[\widehat{P( T+ | D-)}=FP/(FP+TN)\]
\end{itemize}

Ideally, a good binary predictive model such as the logistic regression model should have a high sensitivity and specificity and a low false negative rate and false-positive rate (FPR, also called false alarm rate).

The overall accuracy is defined by

\[
\text{accuracy} = (TP + TN)/(TP+TN+FP+FN)
\]

and the predictive error (PE) is defined by

\[
PE = (FP + FN)/(TP+FP+TN+FN).
\]

The above PE was used in the iterations of the process of cross-validation for a binary predictive model. In the logistic regression model, the confusion matrix is dependent on the choice of the cut-off probability. In other words, the values of the above 6 measures are dependent on the choice of the cut-off probability. Therefore, the above performance metrics are called \textbf{local performance} measures.

\textbf{Remarks}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The default choice of 0.5 in most software programs may not be the optimal choice.
\item
  The ``disease'' probability is a continuous variable on {[}0,1{]}, we can search the optimal cut-off probability by plotting the cut-off probability against the accuracy and then find the optimal cut-off from the plot.
\end{enumerate}

\hypertarget{measuring-global-performance---roc-curve}{%
\section{Measuring Global Performance - ROC Curve}\label{measuring-global-performance---roc-curve}}

In many practical applications, we may have different candidate models that behaved differently from different perspectives. For example, (1). how to report the performance of a medical diagnostic test if it is dependent on the age of the patient? (2). If there are two tests and both are dependent on the age of patients, how to compare the performance of the two tests? (3). sometimes the costs of false positive and false negative rates are very different in a specific application, how to minimize the cost due to errors? These questions can be addressed using the Receiver Operating Characteristics (ROC) analysis.

\hypertarget{roc-curve}{%
\subsection{ROC Curve}\label{roc-curve}}

An ROC curve is the plot of sensitivity (i.e., TPR = True Positive Rate) against (1-specificity) = False Positive Rate (FPR). Drawing the ROC curve is straightforward. We only need to choose a sequence of cut-off probability and then calculate the corresponding TPR and FPR. Then we can plot these points to get the ROC curve which is similar to the following figure.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img08/w08-roc} 

}

\caption{An illustrative ROC curve}\label{fig:unnamed-chunk-142}
\end{figure}

Using the same set of cut-off probabilities, different candidate models have different ROC curves. The area under the curve (AUC) reflects the \textbf{global goodness of the model}. See the following illustrative curve.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img08/w08-roc-2} 

}

\caption{Illustrative comparison of multiple ROC curves associated with the corresponding candidate models}\label{fig:unnamed-chunk-143}
\end{figure}

With the ROC curve, we can answer the questions in the opening paragraph of this section.

\hypertarget{area-under-the-curve-auc}{%
\subsection{Area Under The Curve (AUC)}\label{area-under-the-curve-auc}}

If two or more ROC curves intersect at least one point, we may want to report the area under the curves (AUC) to compare the global performance between the two corresponding models. See the illustrative example below.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img08/w08-Equal-AUC-ROC} 

}

\caption{Using ROC for model selection.}\label{fig:unnamed-chunk-144}
\end{figure}

There are several R packages that have functions to calculate the AUC. We are going use R functions \texttt{roc()} and \texttt{auc()} in package \texttt{pROC} to calculate the AUC in the case study.

\hypertarget{case-study---diabetes-prediction-with-logistic-regression}{%
\section{Case Study - Diabetes Prediction with Logistic Regression}\label{case-study---diabetes-prediction-with-logistic-regression}}

In this case study, we still use the diabetes data that was used in the last module.

\hypertarget{data-and-variable-descriptions-2}{%
\subsection{Data and Variable Descriptions}\label{data-and-variable-descriptions-2}}

There are 9 variables in the data set.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{pregnant}: Number of times pregnant
\item
  \textbf{glucose}: Plasma glucose concentration (glucose tolerance test)
\item
  \textbf{pressure}: Diastolic blood pressure (mm Hg)
\item
  \textbf{triceps}: Triceps skinfold thickness (mm)
\item
  \textbf{insulin}: 2-Hour serum insulin (mu U/ml)
\item
  \textbf{mass}: Body mass index (weight in kg/(height in m)\^{}2)
\item
  \textbf{pedigree}: Diabetes pedigree function
\item
  \textbf{age}: Age (years)
\item
  \textbf{diabetes}: Class variable (test for diabetes)
\end{enumerate}

I load the data from R \textbf{library\{mlbench\}} in the following code. For convenience, I delete all records with missing values and keep only the records with complete records in this case study. The final analytic data set has 392 records. This

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mlbench)}
\FunctionTok{data}\NormalTok{(PimaIndiansDiabetes2)           }\CommentTok{\# load the data to R work{-}space}
\NormalTok{diabetes}\FloatTok{.0} \OtherTok{=}\NormalTok{ PimaIndiansDiabetes2    }\CommentTok{\# make a copy of the data for data cleansing }
\NormalTok{diabetes }\OtherTok{=} \FunctionTok{na.omit}\NormalTok{(diabetes}\FloatTok{.0}\NormalTok{)       }\CommentTok{\# Delete all records with missing components}
\CommentTok{\#head(diabetes)}
\end{Highlighting}
\end{Shaded}

\hypertarget{research-question-1}{%
\subsection{Research Question}\label{research-question-1}}

The objective of this case study is to build a logistic regression model to predict diabetes using various risk factors associated with the individual patient.

\hypertarget{exploratory-analysis-1}{%
\subsection{Exploratory Analysis}\label{exploratory-analysis-1}}

We first make the following pairwise scatter plots to inspect the potential issues with predictor variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 载入程辑包：'psych'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:scales':
## 
##     alpha, rescale
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs.panels}\NormalTok{(diabetes[,}\SpecialCharTok{{-}}\DecValTok{9}\NormalTok{], }
             \AttributeTok{method =} \StringTok{"pearson"}\NormalTok{, }\CommentTok{\# correlation method}
             \AttributeTok{hist.col =} \StringTok{"\#00AFBB"}\NormalTok{,}
             \AttributeTok{density =} \ConstantTok{TRUE}\NormalTok{,  }\CommentTok{\# show density plots}
             \AttributeTok{ellipses =} \ConstantTok{TRUE} \CommentTok{\# show correlation ellipses}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-146-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{detach}\NormalTok{(}\StringTok{"package:psych"}\NormalTok{, }\AttributeTok{unload =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

From the correlation matrix plot, we can see several patterns in the predictor variables.

\begin{itemize}
\tightlist
\item
  All predictor variables are unimodal. But \textbf{pregnant} and \textbf{age} are significantly skewed. we discretize \textbf{pregnant} and \textbf{age} in the following.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{preg }\OtherTok{=}\NormalTok{ diabetes}\SpecialCharTok{$}\NormalTok{pregnant}
\NormalTok{grp.preg }\OtherTok{=}\NormalTok{ preg}
\NormalTok{grp.preg[preg }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)] }\OtherTok{=} \StringTok{"4{-}6"}
\NormalTok{grp.preg[preg }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{7}\SpecialCharTok{:}\DecValTok{9}\NormalTok{)] }\OtherTok{=} \StringTok{"7{-}9"}
\NormalTok{grp.preg[preg }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\SpecialCharTok{:}\DecValTok{17}\NormalTok{)] }\OtherTok{=} \StringTok{"10+"}
\DocumentationTok{\#\#}
\NormalTok{age }\OtherTok{=}\NormalTok{ diabetes}\SpecialCharTok{$}\NormalTok{age}
\DocumentationTok{\#\#}
\NormalTok{grp.age }\OtherTok{=}\NormalTok{ age}
\NormalTok{grp.age[age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{21}\SpecialCharTok{:}\DecValTok{24}\NormalTok{)] }\OtherTok{=} \StringTok{"21{-}25"}
\NormalTok{grp.age[age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\SpecialCharTok{:}\DecValTok{30}\NormalTok{)] }\OtherTok{=} \StringTok{"25{-}30"}
\NormalTok{grp.age[age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{31}\SpecialCharTok{:}\DecValTok{40}\NormalTok{)] }\OtherTok{=} \StringTok{"31{-}40"}
\NormalTok{grp.age[age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{41}\SpecialCharTok{:}\DecValTok{50}\NormalTok{)] }\OtherTok{=} \StringTok{"41{-}50"}
\NormalTok{grp.age[age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{51}\SpecialCharTok{:}\DecValTok{99}\NormalTok{)] }\OtherTok{=} \StringTok{"50 +"}
\DocumentationTok{\#\# added to the diabetes data set}
\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{grp.age }\OtherTok{=}\NormalTok{ grp.age}
\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{grp.preg }\OtherTok{=}\NormalTok{ grp.preg}
\end{Highlighting}
\end{Shaded}

A moderate correlation is observed in several pairs of variables: \textbf{age} v.s. \textbf{pregnant}, \textbf{glucose} v.s. \textbf{insulin}, and \textbf{triceps} v.s. \textbf{mass}. We will not drop any of these variables. We will standardize these variables. Part of the correlation may be removed after they are standardized.

\hypertarget{standizing-numerical-predictor-variables}{%
\subsection{Standizing Numerical Predictor Variables}\label{standizing-numerical-predictor-variables}}

Since this is a predictive model, we don't worry about the interpretation of the coefficients. The objective is to identify a model that has the best predictive performance.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# standardizing numerical variables}
\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{sd.glucose }\OtherTok{=}\NormalTok{ (diabetes}\SpecialCharTok{$}\NormalTok{glucose}\SpecialCharTok{{-}}\FunctionTok{mean}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{glucose))}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{glucose)}
\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{sd.pressure }\OtherTok{=}\NormalTok{ (diabetes}\SpecialCharTok{$}\NormalTok{pressure}\SpecialCharTok{{-}}\FunctionTok{mean}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{pressure))}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{pressure)}
\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{sd.triceps }\OtherTok{=}\NormalTok{ (diabetes}\SpecialCharTok{$}\NormalTok{triceps}\SpecialCharTok{{-}}\FunctionTok{mean}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{triceps))}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{triceps)}
\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{sd.insulin }\OtherTok{=}\NormalTok{ (diabetes}\SpecialCharTok{$}\NormalTok{insulin}\SpecialCharTok{{-}}\FunctionTok{mean}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{insulin))}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{insulin)}
\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{sd.mass }\OtherTok{=}\NormalTok{ (diabetes}\SpecialCharTok{$}\NormalTok{mass}\SpecialCharTok{{-}}\FunctionTok{mean}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{mass))}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{mass)}
\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{sd.pedigree }\OtherTok{=}\NormalTok{ (diabetes}\SpecialCharTok{$}\NormalTok{pedigree}\SpecialCharTok{{-}}\FunctionTok{mean}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{pedigree))}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{pedigree)}
\DocumentationTok{\#\# drop the original variables except for the response variable}
\NormalTok{sd.diabetes }\OtherTok{=}\NormalTok{ diabetes[, }\SpecialCharTok{{-}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\textbf{Remark}: If the final model is used for real-time prediction, we need to write a separate function to perform all variable transformations and modifications before feeding it to the model for prediction.

\hypertarget{data-split---training-and-testing-data}{%
\subsection{Data Split - Training and Testing Data}\label{data-split---training-and-testing-data}}

We \textbf{randomly} split the data into two subsets. 70\% of the data will be used as training data. We will use the training data to search the candidate models, validate them and identify the final model using the cross-validation method. The 30\% of the hold-up sample will be used for assessing the performance of the final model.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# splitting data: 80\% training and 20\% testing}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(sd.diabetes)[}\DecValTok{1}\NormalTok{]}
\NormalTok{train.n }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FloatTok{0.8}\SpecialCharTok{*}\NormalTok{n)}
\NormalTok{train.id }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, train.n, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\DocumentationTok{\#\# training and testing data sets}
\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ sd.diabetes[train.id, ]}
\NormalTok{test }\OtherTok{\textless{}{-}}\NormalTok{ sd.diabetes[}\SpecialCharTok{{-}}\NormalTok{train.id, ]}
\end{Highlighting}
\end{Shaded}

\hypertarget{best-model-identification}{%
\subsection{Best Model Identification}\label{best-model-identification}}

In the previous module, we introduced full and reduced models to set up the scope for searching for the final model. In this case study, we use the full, reduced, and final models obtained based on the step-wise variable selection as the three candidate models.

For illustration, we use 0.5 as the common cut-off for all three models to define the predicted. In a real application, \textbf{we may want to find the optimal cut-off for each candidate model in the cross-validation process}.

\hypertarget{cross-validation-for-model-identification}{%
\subsubsection{Cross-Validation for Model Identification}\label{cross-validation-for-model-identification}}

Since our training data is relatively small, I will use 5-fold cross-validation to ensure the validation data set has enough diabetes cases.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{library}\NormalTok{(knitr)}
\DocumentationTok{\#\# 5{-}fold cross{-}validation}
\NormalTok{k}\OtherTok{=}\DecValTok{5}
\DocumentationTok{\#\# floor() function must be used to avoid producing NA in the subsequent results}
\NormalTok{fold.size }\OtherTok{=} \FunctionTok{floor}\NormalTok{(}\FunctionTok{dim}\NormalTok{(train)[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{k)}
\DocumentationTok{\#\# PE vectors for candidate models}
\NormalTok{PE1 }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{5}\NormalTok{)}
\NormalTok{PE2 }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{5}\NormalTok{)}
\NormalTok{PE3 }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{5}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{k)\{}
  \DocumentationTok{\#\# Training and testing folds}
\NormalTok{  valid.id }\OtherTok{=}\NormalTok{ (fold.size}\SpecialCharTok{*}\NormalTok{(i}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(fold.size}\SpecialCharTok{*}\NormalTok{i)}
\NormalTok{  valid }\OtherTok{=}\NormalTok{ train[valid.id, ]}
\NormalTok{  train.dat }\OtherTok{=}\NormalTok{ train[}\SpecialCharTok{{-}}\NormalTok{valid.id,]}
  \DocumentationTok{\#\#  full model}
\NormalTok{  candidate01 }\OtherTok{=} \FunctionTok{glm}\NormalTok{(diabetes }\SpecialCharTok{\textasciitilde{}}\NormalTok{grp.preg }\SpecialCharTok{+}\NormalTok{ sd.glucose }\SpecialCharTok{+}\NormalTok{sd.pressure}\SpecialCharTok{+}\NormalTok{ sd.triceps }\SpecialCharTok{+}\NormalTok{ sd.insulin }\SpecialCharTok{+} 
\NormalTok{                    sd.mass }\SpecialCharTok{+}\NormalTok{ sd.pedigree }\SpecialCharTok{+}\NormalTok{ grp.age, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),  }
                    \AttributeTok{data =}\NormalTok{ train.dat)  }
\DocumentationTok{\#\# reduced model}
\NormalTok{  candidate03 }\OtherTok{=} \FunctionTok{glm}\NormalTok{(diabetes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sd.insulin }\SpecialCharTok{+}\NormalTok{ sd.mass }\SpecialCharTok{+}\NormalTok{ sd.pedigree, }
                    \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),  }
                    \AttributeTok{data =}\NormalTok{ train.dat) }
\DocumentationTok{\#\# }
\NormalTok{   candidate02 }\OtherTok{=} \FunctionTok{stepAIC}\NormalTok{(candidate01, }
                      \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower=}\FunctionTok{formula}\NormalTok{(candidate03),}\AttributeTok{upper=}\FunctionTok{formula}\NormalTok{(candidate01)),}
                      \AttributeTok{direction =} \StringTok{"forward"}\NormalTok{,   }\CommentTok{\# forward selection}
                      \AttributeTok{trace =} \DecValTok{0}                \CommentTok{\# do not show the details}
\NormalTok{                      )}
  \DocumentationTok{\#\#  predicted probabilities of each candidate model}
\NormalTok{   pred01 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(candidate01, }\AttributeTok{newdata =}\NormalTok{ valid, }\AttributeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{   pred02 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(candidate02, }\AttributeTok{newdata =}\NormalTok{ valid, }\AttributeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{   pred03 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(candidate03, }\AttributeTok{newdata =}\NormalTok{ valid, }\AttributeTok{type=}\StringTok{"response"}\NormalTok{)}
   
\NormalTok{   pre.outcome01 }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{as.vector}\NormalTok{(pred01) }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{, }\StringTok{"pos"}\NormalTok{, }\StringTok{"neg"}\NormalTok{)}
\NormalTok{   pre.outcome02 }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{as.vector}\NormalTok{(pred02) }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{, }\StringTok{"pos"}\NormalTok{, }\StringTok{"neg"}\NormalTok{)}
\NormalTok{   pre.outcome03 }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{as.vector}\NormalTok{(pred03) }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{, }\StringTok{"pos"}\NormalTok{, }\StringTok{"neg"}\NormalTok{)}
   
\NormalTok{   PE1[i] }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pre.outcome01 }\SpecialCharTok{==}\NormalTok{ valid}\SpecialCharTok{$}\NormalTok{diabetes )}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(pred01)}
\NormalTok{   PE2[i] }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pre.outcome02 }\SpecialCharTok{==}\NormalTok{ valid}\SpecialCharTok{$}\NormalTok{diabetes )}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(pred02)}
\NormalTok{   PE3[i] }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pre.outcome02 }\SpecialCharTok{==}\NormalTok{ valid}\SpecialCharTok{$}\NormalTok{diabetes )}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(pred03)}
\NormalTok{\}}
\NormalTok{avg.pe }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{PE1 =} \FunctionTok{mean}\NormalTok{(PE1), }\AttributeTok{PE2 =} \FunctionTok{mean}\NormalTok{(PE2), }\AttributeTok{PE3 =} \FunctionTok{mean}\NormalTok{(PE3))}
\FunctionTok{kable}\NormalTok{(avg.pe, }\AttributeTok{caption =} \StringTok{"Average of prediction errors of candidate models"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-150}Average of prediction errors of candidate models}
\centering
\begin{tabular}[t]{r|r|r}
\hline
PE1 & PE2 & PE3\\
\hline
0.7903226 & 0.7903226 & 0.7903226\\
\hline
\end{tabular}
\end{table}

The average predictive errors show that candidate models 1 and 2 have the same predictive error. Since model 2 is simpler than model 1, we choose model 2 as the final predictive model. \textbf{This selection of the final model is based on the cut-off probability 0.5}.

\hypertarget{final-model-reporting}{%
\subsubsection{Final Model Reporting}\label{final-model-reporting}}

The previous cross-validation procedure identified the best model with pre-selected cut-off 0.5. The actual accuracy to be report to the client MUST be based on the withheld \textbf{test data}. Therefore, the actual accuracy of the final model is given by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred02 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(candidate02, }\AttributeTok{newdata =}\NormalTok{ test, }\AttributeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{pred02.outcome }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{as.vector}\NormalTok{(pred02)}\SpecialCharTok{\textgreater{}}\FloatTok{0.5}\NormalTok{, }\StringTok{"pos"}\NormalTok{, }\StringTok{"neg"}\NormalTok{)}

\NormalTok{accuracy }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred02.outcome }\SpecialCharTok{==}\NormalTok{ test}\SpecialCharTok{$}\NormalTok{diabetes)}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(pred02)}
\FunctionTok{kable}\NormalTok{(accuracy, }\AttributeTok{caption=}\StringTok{"The actual accuracy of the final model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-151}The actual accuracy of the final model}
\centering
\begin{tabular}[t]{r}
\hline
x\\
\hline
0.8076923\\
\hline
\end{tabular}
\end{table}

Therefore, the final model has an accuracy rate given in the above table.

\textbf{Remark}: Since we used a random split method to define the training and testing data when re-running the code, the performance metrics will be slightly different.

\hypertarget{roc-analysis---global-performance}{%
\subsection{ROC Analysis - Global Performance}\label{roc-analysis---global-performance}}

The ROC curve as a visual tool is used to compare the \textbf{global performance} of binary predictive models. Since it is used for the purpose of model selection, we need to construct ROC curves based on the training data. Next, we will construct ROC curves and calculate the corresponding AUCs using package \texttt{pROC}.

We first estimate the TPR (true positive rate, sensitivity) and FPR (false positive rate, 1 - specificity) at each cut-off probability for each of the three candidate models using the following R function.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# A function to extract false positive and false negative rates}
\NormalTok{TPR.FPR}\OtherTok{=}\ControlFlowTok{function}\NormalTok{(pred)\{}
\NormalTok{  prob.seq }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{length=}\DecValTok{50}\NormalTok{)  }\CommentTok{\# 50 equally spaced cut{-}off probabilities}
\NormalTok{  pn}\OtherTok{=}\FunctionTok{length}\NormalTok{(prob.seq)}
\NormalTok{  true.lab}\OtherTok{=}\FunctionTok{as.vector}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{diabetes)}
\NormalTok{  TPR }\OtherTok{=} \ConstantTok{NULL}
\NormalTok{  FPR }\OtherTok{=} \ConstantTok{NULL}
  \DocumentationTok{\#\#}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{pn)\{}
\NormalTok{   pred.lab }\OtherTok{=} \FunctionTok{as.vector}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(pred }\SpecialCharTok{\textgreater{}}\NormalTok{prob.seq[i],}\StringTok{"pos"}\NormalTok{, }\StringTok{"neg"}\NormalTok{))}
\NormalTok{   TPR[i] }\OtherTok{=} \FunctionTok{length}\NormalTok{(}\FunctionTok{which}\NormalTok{(true.lab}\SpecialCharTok{==}\StringTok{"pos"} \SpecialCharTok{\&}\NormalTok{ pred.lab}\SpecialCharTok{==}\StringTok{"pos"}\NormalTok{))}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(}\FunctionTok{which}\NormalTok{(true.lab}\SpecialCharTok{==}\StringTok{"pos"}\NormalTok{))}
\NormalTok{   FPR[i] }\OtherTok{=} \FunctionTok{length}\NormalTok{(}\FunctionTok{which}\NormalTok{(true.lab}\SpecialCharTok{==}\StringTok{"neg"} \SpecialCharTok{\&}\NormalTok{ pred.lab}\SpecialCharTok{==}\StringTok{"pos"}\NormalTok{))}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(}\FunctionTok{which}\NormalTok{(true.lab}\SpecialCharTok{==}\StringTok{"neg"}\NormalTok{))}
\NormalTok{  \}}
 \FunctionTok{cbind}\NormalTok{(}\AttributeTok{FPR =}\NormalTok{ FPR, }\AttributeTok{TPR =}\NormalTok{ TPR)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The ROC curves of the three candidate models are given below.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# we need a function to calculate the AUC.}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"pROC"}\NormalTok{)) \{}
   \FunctionTok{install.packages}\NormalTok{(}\StringTok{"pROC"}\NormalTok{)}
   \FunctionTok{library}\NormalTok{(pROC)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: 程辑包'pROC'是用R版本4.2.3 来建造的
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#  candidate models}
\DocumentationTok{\#\#  full model}
\NormalTok{candidate01 }\OtherTok{=} \FunctionTok{glm}\NormalTok{(diabetes }\SpecialCharTok{\textasciitilde{}}\NormalTok{grp.preg }\SpecialCharTok{+}\NormalTok{ sd.glucose }\SpecialCharTok{+}\NormalTok{sd.pressure}\SpecialCharTok{+}\NormalTok{ sd.triceps }\SpecialCharTok{+}\NormalTok{ sd.insulin }\SpecialCharTok{+} 
\NormalTok{                    sd.mass }\SpecialCharTok{+}\NormalTok{ sd.pedigree }\SpecialCharTok{+}\NormalTok{ grp.age, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),  }
                    \AttributeTok{data =}\NormalTok{ train)  }
\DocumentationTok{\#\# reduced model}
\NormalTok{candidate03 }\OtherTok{=} \FunctionTok{glm}\NormalTok{(diabetes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sd.insulin }\SpecialCharTok{+}\NormalTok{ sd.mass }\SpecialCharTok{+}\NormalTok{ sd.pedigree, }
                    \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),  }
                    \AttributeTok{data =}\NormalTok{ train) }
\DocumentationTok{\#\# }
\NormalTok{candidate02 }\OtherTok{=} \FunctionTok{stepAIC}\NormalTok{(candidate03, }
                      \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower=}\FunctionTok{formula}\NormalTok{(candidate03),}\AttributeTok{upper=}\FunctionTok{formula}\NormalTok{(candidate01)),}
                      \AttributeTok{direction =} \StringTok{"forward"}\NormalTok{,   }\CommentTok{\# forward selection}
                      \AttributeTok{trace =} \DecValTok{0}                \CommentTok{\# do not show the details}
\NormalTok{                      )}
\DocumentationTok{\#\#  predicted probabilities}
\NormalTok{pred01 }\OtherTok{=} \FunctionTok{predict.glm}\NormalTok{(candidate01, }\AttributeTok{newdata =}\NormalTok{ train, }\AttributeTok{type=}\StringTok{"response"}\NormalTok{) }
\NormalTok{pred02 }\OtherTok{=} \FunctionTok{predict.glm}\NormalTok{(candidate02, }\AttributeTok{newdata =}\NormalTok{ train, }\AttributeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{pred03 }\OtherTok{=} \FunctionTok{predict.glm}\NormalTok{(candidate03, }\AttributeTok{newdata =}\NormalTok{ train, }\AttributeTok{type=}\StringTok{"response"}\NormalTok{)}
\DocumentationTok{\#\#\#\#}
\DocumentationTok{\#\# ROC curve}
 \FunctionTok{plot}\NormalTok{(}\FunctionTok{TPR.FPR}\NormalTok{(pred01)[,}\DecValTok{1}\NormalTok{], }\FunctionTok{TPR.FPR}\NormalTok{(pred01)[,}\DecValTok{2}\NormalTok{], }
      \AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{col=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
      \AttributeTok{xlab =} \StringTok{"FPR: 1 {-} specificity"}\NormalTok{,}
      \AttributeTok{ylab =}\StringTok{"TPR: sensitivity"}\NormalTok{,}
      \AttributeTok{main =} \StringTok{"ROC curves of the three candidate models"}\NormalTok{,}
      \AttributeTok{cex.main =} \FloatTok{0.8}\NormalTok{,}
      \AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{)}
 \FunctionTok{lines}\NormalTok{(}\FunctionTok{TPR.FPR}\NormalTok{(pred02)[,}\DecValTok{1}\NormalTok{], }\FunctionTok{TPR.FPR}\NormalTok{(pred02)[,}\DecValTok{2}\NormalTok{],  }\AttributeTok{col=}\DecValTok{3}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)}
 \FunctionTok{lines}\NormalTok{(}\FunctionTok{TPR.FPR}\NormalTok{(pred03)[,}\DecValTok{1}\NormalTok{], }\FunctionTok{TPR.FPR}\NormalTok{(pred03)[,}\DecValTok{2}\NormalTok{],  }\AttributeTok{col=}\DecValTok{4}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{)    }

  \DocumentationTok{\#\#}
\NormalTok{  category }\OtherTok{=}\NormalTok{ train}\SpecialCharTok{$}\NormalTok{diabetes }\SpecialCharTok{==} \StringTok{"pos"}
\NormalTok{  ROCobj01 }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(category, }\FunctionTok{as.vector}\NormalTok{(pred01))}
\NormalTok{  ROCobj02 }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(category, }\FunctionTok{as.vector}\NormalTok{(pred02))}
\NormalTok{  ROCobj03 }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(category, }\FunctionTok{as.vector}\NormalTok{(pred03))}
\NormalTok{  AUC01 }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(ROCobj01),}\DecValTok{4}\NormalTok{)}
\NormalTok{  AUC02 }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(ROCobj02),}\DecValTok{4}\NormalTok{)}
\NormalTok{  AUC03 }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(ROCobj03),}\DecValTok{4}\NormalTok{)}
  \DocumentationTok{\#\#}
  \FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Full model: AUC = "}\NormalTok{,AUC01), }
                         \FunctionTok{paste}\NormalTok{(}\StringTok{"Stepwise model: AUC ="}\NormalTok{,AUC02),}
                         \FunctionTok{paste}\NormalTok{(}\StringTok{"reduced model: AUC ="}\NormalTok{, AUC03)),}
        \AttributeTok{col=}\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-153-1} \end{center}

We can see from the ROC curve that full model and the stepwise model are better than the reduced model. However, the full model and the stepwise model have similar ROC curve and AUCs. Since stepwise model is simpler than the full model, therefore, the final model to report to the client is the stepwise model.

The accuracy measure of the stepwise model based one the test data has been reported in the earlier section.

\hypertarget{summary-and-conclusion-1}{%
\subsection{Summary and Conclusion}\label{summary-and-conclusion-1}}

The case study focused on predicting diabetes. For illustrative purposes, we used three models as candidates and use both cross-validation and ROC curve to select the final working model. Both cross-validation and ROC curve yielded the same result.

\hypertarget{analysis-assignment-2}{%
\section{Analysis Assignment}\label{analysis-assignment-2}}

This assignment focuses on binary predictive modeling using the logistic regression model. I used the same data set that was used in the previous few weeks to build a logistic predictive model for predicting the occurrence of diabetes.

You are expected to use the same data set you used in the three of your assignment to build a predictive logistic regression model.

The write-up of your assignment should be the same as my case study. To be more specific, you are expected to use my case study as a template to complete this assignment. The following are the major components I expected you to include in your report.

\begin{itemize}
\item
  Introduction - description of what you plan to do in the analysis
\item
  Description of data and variables

  \begin{itemize}
  \item
    information on the data collection process.
  \item
    list of variable names and \textbf{definitions}.
  \end{itemize}
\item
  Research question(s) - what is the objective of the analysis
\item
  Variable transformation and discretization

  \begin{itemize}
  \item
    list the numerical variables you standardize
  \item
    list of the variable you discretize
  \end{itemize}
\item
  Data split - the proportions of data for training and testing sets
\item
  Candidate models - you can the candidate model you used in the previous assignment on the multiple logistic regression model.
\item
  The final model selection

  \begin{itemize}
  \item
    Cross-validation method
  \item
    ROC approach (\textbf{This is optional})
  \end{itemize}
\end{itemize}

\hypertarget{poison-regression-modeling}{%
\chapter{Poison Regression Modeling}\label{poison-regression-modeling}}

We have studied the normal-based linear and binary logistic regression models dependent on the types of random response variables.

\hypertarget{linear-regression-models}{%
\section{Linear Regression Models}\label{linear-regression-models}}

The primary regression models are normal linear models. The basic distributional assumption is that the residuals follow a normal distribution with mean zero and constant variance. The explanatory variables (also called predictor variables) are assumed to be uncorrelated with the response variable. Of course, the functional form of the explanatory variables must be correctly specified. Furthermore, the predictor variables are assumed to be non-random. This means that the response variable is a normal random variable - a special continuous random variable.

The regression coefficients are estimated by the least square method - also the least square estimation (LSE). When making inferences about the LSE, we still assume the residuals are normally distributed in order to construct confidence intervals of the regression coefficients and test the significance of the regression coefficient as well.

However, many continuous variables in the real world are not normally distributed, for example, a system's lifetime in reliability engineering, toxic concentrations in underground water, survival times of cancer patients who received surgery, the waiting time of a customer at a service desk, etc. These random variables are not normal.

\hypertarget{binary-logistic-regression-model}{%
\section{Binary Logistic Regression Model}\label{binary-logistic-regression-model}}

Contrary to the linear regression model that requires the response variable to be a continuous normal random variable, in the logistic regression model, the response variable is assumed to be a Bernoulli random variable that takes on only two distinct values such as ``diseased'' vs ``disease-free'', ``success'' vs ``failure'', etc.

The actual regression function in the logistic regression is the probability of ``success'' not the value of the response variable ``success''. The model was constructed with a special structure. The estimation of the regression coefficients is based on the likelihood theory.

The interpretation of the logistic regression model is also different from that of the linear regression model due to the special structure of the logistic regression. The regression coefficients measure how the corresponding explanatory variable impacts the log odds of success.

The resulting logistic regression model can be used for association analysis and prediction as well. The use of predictive modeling is one of the most important classification algorithms in data science. This module will focus on the discrete response variable which represents the number of occurrences of some event. Here are some examples.

\begin{itemize}
\tightlist
\item
  The number of sunspots over the years.
\item
  the number of positive COVID-19 cases in a period of time.
\item
  the number of the COVID-19 death counts.
\item
  the number of people walking into an Emergency Room per hour.
\end{itemize}

\hypertarget{poisson-regression-models}{%
\section{Poisson Regression Models}\label{poisson-regression-models}}

The Poisson regression model assumes the random response variable to be a frequency count or a rate of a specific event such as COVID-19 positivity rates, COVID-19 death mortality, etc. As in the linear and logistic regression models, we also assume that predictor variables are non-random.

The family of logistic regression models assumes that the response variable follows a binomial distribution while Poisson regression models assume that the response variable has a Poisson distribution.

\hypertarget{assumptions-of-the-poisson-regression-model}{%
\subsection{Assumptions of the Poisson Regression Model}\label{assumptions-of-the-poisson-regression-model}}

The basic assumptions of Poisson regression are

\begin{itemize}
\item
  \textbf{Poisson Response}: The response variable is a count per unit of time or space, described by a Poisson distribution.
\item
  \textbf{Independence}: The observations must be independent of one another.
\item
  \textbf{Mean is equal to variance}: By definition, the mean of a Poisson random variable must be equal to its variance.
\item
  \textbf{Linearity}: The log of the mean rate, \(\log( \lambda)\), must be a linear function of \(x\).
\end{itemize}

\hypertarget{structure-of-poisson-regression-model-for-counts}{%
\subsection{Structure of Poisson Regression Model for Counts}\label{structure-of-poisson-regression-model-for-counts}}

Let \(Y\) be the response variable that takes on frequency counts as values and \(X\) be the set of predictor variables such as demographics and social determinants. Further, let \(\mu=E[Y]\) be the mean of the response variable. The Poisson regression model is defined in the following analytic expression.

\[
\log(\mu) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p,
\] where \(\beta_0, \beta_1, \cdots, \beta_p\) are coefficients of the Poisson regression model. The interpretation of the regression coefficient \(\beta_i\) is as follows

\begin{itemize}
\item
  \(\beta_0\) = the baseline logarithm of the mean of \(Y\), \(\log(\mu)\), when all predictor variables \(x_i = 0\), for \(i = 1, 2, \cdots, p\). As usual, we are not interested in the inference of the intercept parameter.
\item
  \(\beta_i\) = is the change of the log mean due to one unit increases in \(x_i\) with all other \(x_j\) being fixed, for \(j\ne i\).
\end{itemize}

To be more specific, let \(\mu(x_i) = E[Y|(\cdots,x_i,\cdots)]\) be the mean counts with variables \(\cdots, x_{i-1}, x_{i+1}, \cdots\) being fixed except for \(x_i\). To look at how \(x_i\) impacts the value of \(E[Y]\), we increase \(x_i\) by one unit and fix all other predictor variables.

\[
\log \mu(x_i) =  \beta_0 + \cdots + \beta_{i-1} x_{i-1}+ \beta_{i} x_{i} + \beta_{i+1} x_{i+1} + \cdots + \beta_p x_p
\]

After increasing \(x_i\) by one unit, the corresponding log mean is given by

\[
\log \mu(x_i+1), = \beta_0 + \cdots + \beta_{i-1} x_{i-1}+ \beta_{i} (x_{i}+1) + \beta_{i+1} x_{i+1} + \cdots + \beta_p x_p
\] Therefore,

\[
\beta_i = \log\mu(x_i+1)  - \log\mu(x_i)
\]

\begin{itemize}
\item
  If \(\beta_i = 0\), then \(\log\mu(x_i+1) = \log\mu(x_i)\). This implies that \(x_i\) does not impact the mean of \(Y\), equivalently, \(Y\) and \(X_i\) are not associated with each other.
\item
  If \(\beta_i > 0\), then \(\log\mu(x_i+1) > \log\mu(x_i)\). This implies that \(\mu(x_i+1) > \mu(x_i)\), equivalently, \(Y\) and \(X_i\) are positively associated with each other.
\item
  Similarly, if \(\beta_i < 0\), then \(Y\) and \(X_i\) are negatively associated with each other.
\end{itemize}

Because the Poisson distribution is usually used to model rare events such as diseases and anomalies and the regression coefficients \(\beta_i\) can be expressed as \(\beta_i = \log (\mu(x_i+1)/\mu(x_i))\), \(\beta_i\) is called \textbf{relative risk}, sometimes also called \textbf{risk ratio} or \textbf{log risk ratio}.

\hypertarget{poisson-models-for-rates}{%
\subsection{Poisson Models for Rates}\label{poisson-models-for-rates}}

The Poisson log-linear regression model for the expected rate of the occurrence of the event is defined by

\[
\log(\mu/t) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p
\]

This can be re-expressed as

\[
\log(\mu)=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p+\log(t)
\]

The term \(\log(t)\) is referred to as an offset. It is an adjustment term and a group of observations may have the same offset, or each individual may have a different value of t. \(\log(t)\) is an observation and it will change the value of estimated counts:

\[
\mu=\exp[\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p+\log(t)]
= t\exp(\beta_0)\exp(\beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p)
\]

This means that the mean count is proportional to t.

Note that the interpretation of parameter estimates \(\beta_0\) and \(\beta_1, \beta_2, \cdots, \beta_p\) will stay the same as for the model of counts; you just need to multiply the expected counts by t.

\hypertarget{estimation-of-regression-coefficients-and-goodness-of-fit}{%
\subsection{Estimation of Regression Coefficients and Goodness-of-fit}\label{estimation-of-regression-coefficients-and-goodness-of-fit}}

Unlike linear regression in which we have assumptions about the residuals. The estimated residuals can be used to test the assumptions about the distribution. In GLM, the goodness-of-fit is much more complex than the normal-based regression modeling. But we can mimic residuals in the linear regression modeling to define a similar quantity called \textbf{deviance residual} based on the likelihood of the model. We will give this definition in the next module.

The estimation of the regression coefficients is based on the maximum likelihood estimation (MLE) which requires numerical solutions. In the Poisson distribution, the mean and variance are equal (\(E[Y] = var[Y]\)). Failing to meet this assumption results in the issue of \textbf{dispersion}, a common violation of the Poisson regression. We will discuss this issue and the relevant remedies in the next module.

We will not go into detail about how to estimate the regression coefficients and perform model diagnostics in this module. Instead, we will focus on data analysis, in particular, the interpretation of regression coefficients.

\hypertarget{data-set-layout}{%
\subsection{Data Set Layout}\label{data-set-layout}}

The data set required for the Poisson regression model in R should have the following layout.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
ID(optional)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(x_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(x_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(x_k\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(y\) (counts)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
total (counts, optional)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & \(x_{11}\) & \(x_{21}\) & \ldots{} & \(x_{k1}\) & \(y_1\) & \(t_1\) \\
2 & \(x_{12}\) & \(x_{22}\) & \ldots{} & \(x_{k2}\) & \(y_2\) & \(t_2\) \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} \\
n & \(x_{1n}\) & \(x_{2n}\) & \ldots{} & \(x_{kn}\) & \(y_n\) & \(t_n\) \\
\end{longtable}

As usual, if there are categorical variables (with numerical coding), we need to introduce dummy variables to capture the unequal effects on the response across the categories of the variables.

\hfill\break

\hypertarget{case-study-modeling-lung-cancer-rates-in-four-cities-of-denmark---part-i}{%
\section{Case Study: Modeling Lung Cancer Rates in Four Cities of Denmark - Part I}\label{case-study-modeling-lung-cancer-rates-in-four-cities-of-denmark---part-i}}

The World Health Organisation (WHO) statistics suggests that Denmark has the highest cancer rates in the world, with about 326 people out of every 100,000 developing cancer each year. The country is known to have a good record of diagnosing cancer, but also has high rates of smoking among women and high levels of alcohol consumption.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"img09/DenmarkCitiesMap.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{img09/DenmarkCitiesMap} \end{center}

In this case study, we use a data set that summarized the lung cancer incident counts (cases) per age group for four Danish cities from 1968 to 1971. The primary random response variable is lung cancer cases. The predictor variables are the age group and the total population size of the neighboring cities.

The data set was built in the R library \{ISwR\}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{library}\NormalTok{(ISwR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: 程辑包'ISwR'是用R版本4.2.3 来建造的
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#eba1977 = read.csv("eba1977.csv")}
\FunctionTok{data}\NormalTok{(eba1977)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(eba1977), }\AttributeTok{caption =} \StringTok{"First few records in the data set"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-155}First few records in the data set}
\centering
\begin{tabular}[t]{l|l|r|r}
\hline
city & age & pop & cases\\
\hline
Fredericia & 40-54 & 3059 & 11\\
\hline
Horsens & 40-54 & 2879 & 13\\
\hline
Kolding & 40-54 & 3142 & 4\\
\hline
Vejle & 40-54 & 2520 & 5\\
\hline
Fredericia & 55-59 & 800 & 11\\
\hline
Horsens & 55-59 & 1083 & 6\\
\hline
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# check the values of the variables in the data set}
\end{Highlighting}
\end{Shaded}

\hypertarget{poisson-regression-on-cancer-counts}{%
\subsection{Poisson Regression on Cancer Counts}\label{poisson-regression-on-cancer-counts}}

We first build a Poisson frequency regression model and ignore the population size of each city in the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.freq }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(cases }\SpecialCharTok{\textasciitilde{}}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ age, }\AttributeTok{family =}\NormalTok{ poisson, }\AttributeTok{data =}\NormalTok{ eba1977)}
\DocumentationTok{\#\#}
\NormalTok{pois.count.coef }\OtherTok{=} \FunctionTok{summary}\NormalTok{(model.freq)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(pois.count.coef, }\AttributeTok{caption =} \StringTok{"The Poisson regression model for the counts of lung cancer cases versus the geographical locations and the age group."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-156}The Poisson regression model for the counts of lung cancer cases versus the geographical locations and the age group.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & 2.2437446 & 0.2036265 & 11.0189233 & 0.0000000\\
\hline
cityHorsens & -0.0984401 & 0.1812909 & -0.5429952 & 0.5871331\\
\hline
cityKolding & -0.2270575 & 0.1877041 & -1.2096561 & 0.2264109\\
\hline
cityVejle & -0.2270575 & 0.1877041 & -1.2096561 & 0.2264109\\
\hline
age55-59 & -0.0307717 & 0.2480988 & -0.1240298 & 0.9012916\\
\hline
age60-64 & 0.2646926 & 0.2314278 & 1.1437369 & 0.2527328\\
\hline
age65-69 & 0.3101549 & 0.2291839 & 1.3533017 & 0.1759593\\
\hline
age70-74 & 0.1923719 & 0.2351660 & 0.8180261 & 0.4133423\\
\hline
age75+ & -0.0625204 & 0.2501222 & -0.2499593 & 0.8026188\\
\hline
\end{tabular}
\end{table}

The above inferential table about the regression coefficients indicates both city and age are insignificant. This means, if we look at cancer count across the age group and city, there is no statistical evidence to support the potential discrepancy across the age groups and cities. However, this does not imply that the model is meaningless from the practical perspective since statistical significance is not equivalent the clinical importance. Moreover, the sample size could impact the statistical significance of some of the variables.

The other way to look at the model is the appropriateness model. The cancer counts are dependent on the population size. Ignoring the population size implies the information in the sample was not effectively used. In the next subsection, we model the cancer rates that involve the population size.

The other way to look at the model is goodness of the model. The cancer counts are dependent on the population size. Ignoring the population size implies the information in the sample was not effectively used. In the next subsection, we model the cancer rates that involve the population size.

Since it's reasonable to assume that the expected count of lung cancer incidents is proportional to the population size, we would prefer to model the rate of incidents per capita. However, for the purposed of illustration, we will fit the Poisson regression model with both counts and rate of cancer rates.

\hypertarget{poisson-regression-on-rates}{%
\subsection{Poisson Regression on Rates}\label{poisson-regression-on-rates}}

The following model assesses the potential relationship between cancer death rates and age. This is the primary interest of the model. We also want to adjust the relationship be the potential neighboring cities.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.rates }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(cases }\SpecialCharTok{\textasciitilde{}}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ age, }\AttributeTok{offset =} \FunctionTok{log}\NormalTok{(pop), }
                   \AttributeTok{family =}\NormalTok{ poisson, }\AttributeTok{data =}\NormalTok{ eba1977)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(model.rates)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =} \StringTok{"Poisson regression on the rate of the the cancer rate in the four Danish cities adjusted by age."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-157}Poisson regression on the rate of the the cancer rate in the four Danish cities adjusted by age.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & -5.6320645 & 0.2002545 & -28.124529 & 0.0000000\\
\hline
cityHorsens & -0.3300600 & 0.1815033 & -1.818479 & 0.0689909\\
\hline
cityKolding & -0.3715462 & 0.1878063 & -1.978348 & 0.0478895\\
\hline
cityVejle & -0.2723177 & 0.1878534 & -1.449629 & 0.1471620\\
\hline
age55-59 & 1.1010140 & 0.2482858 & 4.434463 & 0.0000092\\
\hline
age60-64 & 1.5186123 & 0.2316376 & 6.555985 & 0.0000000\\
\hline
age65-69 & 1.7677062 & 0.2294395 & 7.704455 & 0.0000000\\
\hline
age70-74 & 1.8568633 & 0.2353230 & 7.890701 & 0.0000000\\
\hline
age75+ & 1.4196534 & 0.2502707 & 5.672472 & 0.0000000\\
\hline
\end{tabular}
\end{table}

The above table indicates that the log of cancer rate is not identical across the age groups and among the four cities. To be more specific, the log rates of Fredericia (baseline city) were higher than in the other three cities. The youngest age group (45-55) has the lowest log rate. The regression coefficients represent the change of log rate between the associate age group and the reference age group. The same interpretation applies to the change in log rate among the cities.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.rates }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(cases }\SpecialCharTok{\textasciitilde{}}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ age, }\AttributeTok{offset =} \FunctionTok{log}\NormalTok{(pop), }
                   \AttributeTok{family =}\NormalTok{ quasipoisson, }\AttributeTok{data =}\NormalTok{ eba1977)}
\FunctionTok{summary}\NormalTok{(model.rates)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = cases ~ city + age, family = quasipoisson, data = eba1977, 
##     offset = log(pop))
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.63573  -0.67296  -0.03436   0.37258   1.85267  
## 
## Coefficients:
##             Estimate Std. Error t value          Pr(>|t|)    
## (Intercept)  -5.6321     0.2456 -22.932 0.000000000000431 ***
## cityHorsens  -0.3301     0.2226  -1.483           0.15884    
## cityKolding  -0.3715     0.2303  -1.613           0.12756    
## cityVejle    -0.2723     0.2304  -1.182           0.25561    
## age55-59      1.1010     0.3045   3.616           0.00254 ** 
## age60-64      1.5186     0.2841   5.346 0.000081665456528 ***
## age65-69      1.7677     0.2814   6.282 0.000014695563904 ***
## age70-74      1.8569     0.2886   6.434 0.000011253999281 ***
## age75+        1.4197     0.3069   4.625           0.00033 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for quasipoisson family taken to be 1.504109)
## 
##     Null deviance: 129.908  on 23  degrees of freedom
## Residual deviance:  23.447  on 15  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(model.rates)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =} \StringTok{"Poisson regression on the rate of the  the cancer rate in the four Danish cities adjusted by age."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-158}Poisson regression on the rate of the  the cancer rate in the four Danish cities adjusted by age.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & -5.6320645 & 0.2455964 & -22.932196 & 0.0000000\\
\hline
cityHorsens & -0.3300600 & 0.2225995 & -1.482753 & 0.1588444\\
\hline
cityKolding & -0.3715462 & 0.2303296 & -1.613107 & 0.1275571\\
\hline
cityVejle & -0.2723177 & 0.2303873 & -1.181999 & 0.2556057\\
\hline
age55-59 & 1.1010140 & 0.3045029 & 3.615775 & 0.0025421\\
\hline
age60-64 & 1.5186123 & 0.2840852 & 5.345623 & 0.0000817\\
\hline
age65-69 & 1.7677062 & 0.2813894 & 6.282063 & 0.0000147\\
\hline
age70-74 & 1.8568633 & 0.2886051 & 6.433924 & 0.0000113\\
\hline
age75+ & 1.4196534 & 0.3069372 & 4.625224 & 0.0003301\\
\hline
\end{tabular}
\end{table}

The intercept represents the \textbf{baseline log-cancer rate} ( of baseline \emph{age group 44-55} in the baseline city \emph{Fredericia}). The actual rate is \(\exp(-5.6321) \approx 0.36\%\) which is close the recently reported rate of the country by WHO. The intercept \(-0.3301\) is the difference of the log-rates between baseline city Fredericia and the city of Horsens at any given age group, to be more specific, \(\log(R_{\text{Horsen}}) - \log(R_{\text{Fredericia}}) = -0.3301\) which is equivalent to
\[
\log \left( \frac{R_{\text{Horsen}}}{R_{\text{Fredericia}}} \right) = -0.3301 ~~~\Rightarrow~~~\frac{R_{\text{Horsen}}}{R_{\text{Fredericia}}} = e^{-0.3301} \approx 0.7188518.
\]
This means, with fixed age groups, the cancer rate in Horsens is about \(28\%\) lower than that in Fredericia. Next, we look at the coefficient \(1.4197\) associated with age group \(\text{75+}\). For any given city,

\[
\log \left(\frac{R_{\text{age75+}}}{R_{\text{age45-54}}} \right) = 1.4197~~~\Rightarrow~~~\frac{R_{\text{age75+}}}{R_{\text{age45-54}}} = e^{1.41971} \approx 4.135921. 
\]

This implies that the cancer rate in age group 75+ is 4.14 times that of the baseline age group of 45-54.

\hypertarget{some-graphical-comparison}{%
\subsection{Some Graphical Comparison}\label{some-graphical-comparison}}

The inferential tables of the Poisson regression models in the previous sections give numerical information about the potential discrepancy across the age group and among the cities. But it is not intuitive. Next, we create a graphic to visualize the relationship between cancer rate and age across cities.

First of all, every city has a trend line that reflects the relationship between the cancer rate and the age. We next find the rates of combinations of city and age-group based on the following working rate model.
\[
\text{log-rate} = -5.6321 -0.3301 \times \text{cityHorsens} -0.3715 \times \text{cityKolding} -0.2723 \times \text{cityVejle} + 1.1010 \times \text{age55-59}
\]

\[
+ 1.5186 \times \text{age60-64} + 1.7677 \times  \text{age65-69} + 1.8569 \times \text{age70-74} + 1.4197 \times \text{age75+}
\]
Or equivalently, we can write rate model as
\[
rate =\exp(-5.6321 -0.3301 \times \text{cityHorsens} -0.3715 \times \text{cityKolding} -0.2723 \times \text{cityVejle} + 1.1010 \times \text{age55-59}) 
\]

\[
\times \exp( 1.5186 \times \text{age60-64} + 1.7677 \times  \text{age65-69} + 1.8569 \times \text{age70-74} + 1.4197 \times \text{age75+})
\]

Next, we made a table cancer rates of combinations of city and age group.

The following calculation is based on the regression equation with coefficients given in above table 3. Note that all variables in the model are indicator variables. Each of these indicator variables takes only two possible values: 0 and 1.

For example, \(\exp(-5.632)\) gives the cancer rate of the baseline city, Fredericia, and the baseline age group {[}45-54{]}. \(\exp(-5.632+1.101)\) gives the cancer rate of baseline city, Fredericia, and age group {[}55-59{]}. Following the same pattern, you can find the cancer rate for each combination of the city and age group.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fredericia}
\NormalTok{Fredericia }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632+1.101}\NormalTok{),   }
               \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632+1.52}\NormalTok{),}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632+1.77}\NormalTok{),}
               \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632+1.86}\NormalTok{),}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632+1.42}\NormalTok{))}
\CommentTok{\# Horsens}
\NormalTok{Horsens }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331+1.101}\NormalTok{),   }
            \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331+1.52}\NormalTok{),}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331+1.77}\NormalTok{),}
            \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331+1.86}\NormalTok{),}
            \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331+1.42}\NormalTok{))}
\CommentTok{\# Kolding}
\NormalTok{Kolding}\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372+1.101}\NormalTok{),   }
           \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372+1.52}\NormalTok{),}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372+1.77}\NormalTok{),}
           \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372+1.86}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372+1.42}\NormalTok{))}
\CommentTok{\# Vejle}
\NormalTok{Vejle }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272+1.101}\NormalTok{),   }
          \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272+1.52}\NormalTok{),}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272+1.77}\NormalTok{),}
          \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272+1.86}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272+1.42}\NormalTok{))}
\NormalTok{minmax }\OtherTok{=} \FunctionTok{range}\NormalTok{(}\FunctionTok{c}\NormalTok{(Fredericia,Horsens,Kolding,Vejle))}
\DocumentationTok{\#\#\#\#}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{,Fredericia, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{lty =}\DecValTok{1}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }
               \AttributeTok{ylab=}\StringTok{"Cancer Rate"}\NormalTok{, }\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{6}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.03}\NormalTok{), }\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{ )}
\FunctionTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{labels=}\FunctionTok{c}\NormalTok{(}\StringTok{"[45{-}54]"}\NormalTok{,}\StringTok{"[55,59]"}\NormalTok{,}\StringTok{"[60,64]"}\NormalTok{,}\StringTok{"[65,69]"}\NormalTok{,}\StringTok{"[70,74]"}\NormalTok{,}\StringTok{"75+"}\NormalTok{), }
            \AttributeTok{at =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{,Fredericia, }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Horsens, }\AttributeTok{lty =}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Horsens, }\AttributeTok{pch=}\DecValTok{20}\NormalTok{, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Kolding, }\AttributeTok{lty =}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"purple"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Kolding, }\AttributeTok{pch=}\DecValTok{21}\NormalTok{, }\AttributeTok{col=}\StringTok{"purple"}\NormalTok{)}
\DocumentationTok{\#\#\#}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Vejle, }\AttributeTok{lty =}\DecValTok{4}\NormalTok{, }\AttributeTok{col=}\StringTok{"mediumvioletred"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Vejle, }\AttributeTok{pch=}\DecValTok{22}\NormalTok{, }\AttributeTok{col=}\StringTok{"mediumvioletred"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Fredericia"}\NormalTok{,}\StringTok{"Horsens"}\NormalTok{, }\StringTok{"Kolding"}\NormalTok{, }\StringTok{"Vejle"}\NormalTok{ ),}
                  \AttributeTok{pch=}\DecValTok{19}\SpecialCharTok{:}\DecValTok{22}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,  }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{, }
        \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"purple"}\NormalTok{, }\StringTok{"mediumvioletred"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-160-1.pdf}

\hypertarget{discussions-and-conclusions}{%
\subsection{Discussions and Conclusions}\label{discussions-and-conclusions}}

Several conclusions we can draw from the output of the regression models.

The regression model based on the cancer count is not appropriate since the information on the population size is a key variable in study the cancer distribution. Simply including the population size in the regression model will relduce the significance of age. See the following output of the fitted Poisson regression model of count adjusted by population size.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.freq.pop }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(cases }\SpecialCharTok{\textasciitilde{}}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(pop), }\AttributeTok{family =}\NormalTok{ poisson, }
                      \AttributeTok{data =}\NormalTok{ eba1977)}
\DocumentationTok{\#\#}
\NormalTok{pois.count.coef.pop }\OtherTok{=} \FunctionTok{summary}\NormalTok{(model.freq.pop)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(pois.count.coef.pop, }\AttributeTok{caption =} \StringTok{"The Poisson regression model for the counts of lung cancer cases versus the geographical locations, population size, and age group."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-161}The Poisson regression model for the counts of lung cancer cases versus the geographical locations, population size, and age group.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & 11.7495934 & 8.8151328 & 1.3328890 & 0.1825682\\
\hline
cityHorsens & 0.1832573 & 0.3192679 & 0.5739922 & 0.5659731\\
\hline
cityKolding & -0.0483001 & 0.2519622 & -0.1916957 & 0.8479806\\
\hline
cityVejle & -0.1679335 & 0.1964757 & -0.8547289 & 0.3927012\\
\hline
age55-59 & -1.3842350 & 1.2728775 & -1.0874849 & 0.2768226\\
\hline
age60-64 & -1.2366489 & 1.4049520 & -0.8802073 & 0.3787470\\
\hline
age65-69 & -1.4377681 & 1.6310051 & -0.8815228 & 0.3780349\\
\hline
age70-74 & -1.8048920 & 1.8607922 & -0.9699589 & 0.3320670\\
\hline
age75+ & -1.8383162 & 1.6587773 & -1.1082357 & 0.2677600\\
\hline
log(pop) & -1.2095837 & 1.1227191 & -1.0773698 & 0.2813151\\
\hline
\end{tabular}
\end{table}

We can see from the above output the adding population size to the model

The cancer rate in Fredericia is significantly higher than in the other three cities. It seems that there is no significant difference between Horsens, Kolding, and Vejle. The reason why Fredericia has a higher cancer rate needs further investigation with additional information.

There is a curve linear relationship between age and the cancer rate. The cancer rate increases as age increase. However, the rate starts decreasing after 75. This pattern is consistent with the clinical studies since lung cancer patients were mostly diagnosed between 65-70. It is rare to see lung cancer patients aged under 45.

The last statistical observation is that there is no interaction effect between the age groups and the geographic locations. The rate curves are ``parallel''.

This is only a small data set with limited information. All conclusions in this report are only based on the given data set.

\hypertarget{concluding-remarks-2}{%
\section{Concluding Remarks}\label{concluding-remarks-2}}

This note briefly outlines the regular Poisson regression model for fitting frequency data. The Poisson regression model has a simple structure and is easy to interpret but has a relatively strong assumption - variance is equal to the mean.

If this assumption is violated, we can use negative binomial regression as an alternative. The other potential issue is the data has excess zeros, then we can consider zero-inflated Poisson or zero-inflated negative binomial regression models.

For this week's assignment, you will model the daily counts (and proportion) of cyclists who entered and left the four bridges in New York City. The data set will be provided in the document of the assignment instruction.

\hypertarget{analysis-assignment-3}{%
\section{Analysis Assignment}\label{analysis-assignment-3}}

\hypertarget{data-description-2}{%
\subsection{Data Description}\label{data-description-2}}

The daily total of bike counts was conducted monthly on the Brooklyn Bridge, Manhattan Bridge, Williamsburg Bridge, and Queensboro Bridge. To keep count of cyclists entering and leaving Queens, Manhattan, and Brooklyn via the East River Bridges. The Traffic Information Management System (TIMS) collects the count data. Each record represents the total number of cyclists per 24 hours at Brooklyn Bridge, Manhattan Bridge, Williamsburg Bridge, and Queensboro Bridge.

\hypertarget{data-formats-and-loading}{%
\subsection{Data Formats and Loading}\label{data-formats-and-loading}}

To save you time in finding a data set for Poisson regression, I created several subsets that contain the relevant information you need for this week's assignment. The data was saved in Excel format with multiple tabs. Please find the tab with your last name and then copy-and-paste your data file to a new Excel sheet and save it as a CSV format file or simply copy-and-paste to Notepad to create a TXT format file so you can read the file to R. You can also read the Excel file directly to R using appropriate R functions in relevant R libraries.

Here is the link to the Excel data set:\href{https://github.com/pengdsci/datasets/raw/main/w09-AssignDataSet.xlsx}{NYC cyclist data set (AssignDataSet.xlsx)}.

\hypertarget{assignment-instructions}{%
\subsection{Assignment Instructions}\label{assignment-instructions}}

Your analysis and report should be similar to section 3 of my class note. PLEASE TELL STORIES BEHIND ALL R OUTPUTS (tables and figures) YOU GENERATED IN THE ANALYSIS. You can earn half of the credit if you only generate relevant outputs correctly but with no good storytelling. \emph{The model diagnostic is not required in this assignment but will be required in the next assignment}.

The following components must be included in your analysis report.

\begin{itemize}
\tightlist
\item
  Your description of the data and the variables

  \begin{itemize}
  \tightlist
  \item
    data collection
  \item
    variable names and definitions. Keep in mind that the variable \textbf{Date} is the observation ID.
  \end{itemize}
\item
  What is the research/practical question?

  \begin{itemize}
  \tightlist
  \item
    Explicit definition of the questions
  \item
    what is the response variable that captures the information to address the research/practical questions
  \end{itemize}
\item
  What statistical model will be used in the analysis

  \begin{itemize}
  \tightlist
  \item
    Assumptions and conditions - give a brief description of the models
  \item
    Build a Poison regression model on the counts only.

    \begin{itemize}
    \tightlist
    \item
      Use the p-values to perform variable selection - keep only significant variables in the final model
    \item
      Interpret the regression coefficients of the Poison regression model
    \item
      Explain/describe the steps of your analysis, motivation, and findings.
    \end{itemize}
  \item
    Build a Poison regression on the proportions (rates) of cyclists entering and leaving the bridge in your data.

    \begin{itemize}
    \tightlist
    \item
      Use the p-values to select the significant variables
    \item
      Interpret the regression coefficients of the Poison rate model
    \item
      Explain/describe the steps of your analysis, motivation, and findings.
    \end{itemize}
  \end{itemize}
\item
  Summarize the findings of the above two models.
\end{itemize}

\hypertarget{dispersed-poisson-regression}{%
\chapter{Dispersed Poisson Regression}\label{dispersed-poisson-regression}}

In the last module, we introduced the basics of Poisson regression on counts and rates. Sometimes Poisson regression may not work well since the variance of the response may not be equal to the mean. In this module, we will look into the issue of potential \textbf{dispersion} and other relevant issues and find an alternative count and rate regression model.

The general structure of the Poisson regression model is given by

\[
\log \mu(x) = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p
\]

There are several assumptions for the Poisson regression model. The most important one is that the mean and variance of the response variable are equal. The other assumption is the linear relationship between the explanatory variables and the log mean of the response. It is not straightforward to detect the potential violation of these assumptions. In the next section, we define some metrics based on \emph{residuals} based on the hypothetical model and the observed data.

The learning objectives of this module are (1) to develop measures to detect the violation of the assumptions of the Poisson regression, (2) to define a measure to estimate the dispersion, (3) to introduce the quasi-likelihood Poisson regression model to make robust inference of the regression coefficients.

\hypertarget{residuals-of-poisson-regression}{%
\section{Residuals of Poisson Regression}\label{residuals-of-poisson-regression}}

In linear regression, we can use the residual plots to check the potential violation of the model assumption since the residuals are normally distributed with zero mean and constant standard deviation if the model is appropriate. In Poisson regression, we can mimic the way of defining the \emph{kind of residuals} as we did in linear regression. Under the large sample assumptions, these residuals are approximately normally distributed if the underlying hypothetical model is appropriate. Using this large sample property, we can define some metrics to detect the potential violation of the model assumptions.

Recall that the residual of \(i\)-th observation under a model defined by

\[ e_i = y_i - \hat{\mu}_i.\]

where \(\hat{\mu}_i\) is the fitted value based on the hypothetical model \(\log(\mu) = \hat{\beta}_0 + \hat{\beta}_1x_1 + \cdots + \hat{\beta}_p x_p\). The regression coefficients can be estimated using least squares and likelihood methods.

Next, I am going to use a portion of NYC cyclist data (you will use similar data for this week's assignment). I will use this data set as an example to explain the concepts and models discussed in this module.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cyclist }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"dat/w10{-}NYCcyclistData.csv"}\NormalTok{)}
\NormalTok{cyclist}\SpecialCharTok{$}\NormalTok{log.Brooklynbridge }\OtherTok{=} \FunctionTok{log}\NormalTok{(cyclist}\SpecialCharTok{$}\NormalTok{BrooklynBridge)}
\NormalTok{m0.loglinear }\OtherTok{=} \FunctionTok{lm}\NormalTok{(log.Brooklynbridge }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Day }\SpecialCharTok{+}\NormalTok{ HighTemp }\SpecialCharTok{+}\NormalTok{ LowTemp }\SpecialCharTok{+}\NormalTok{ Precipitation, }
                  \AttributeTok{data =}\NormalTok{ cyclist)}
\NormalTok{m1.Poisson }\OtherTok{=} \FunctionTok{glm}\NormalTok{(BrooklynBridge }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Day }\SpecialCharTok{+}\NormalTok{ HighTemp }\SpecialCharTok{+}\NormalTok{ LowTemp }\SpecialCharTok{+}\NormalTok{ Precipitation, }
                 \AttributeTok{family =} \FunctionTok{poisson}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{), }\AttributeTok{offset =} \FunctionTok{log}\NormalTok{(Total),  }\AttributeTok{data =}\NormalTok{ cyclist)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Least Square Estimate (LSE) of} \(\beta\)'s
\end{itemize}

In this method, we need to take the logarithm of the observed count as in the data table as shown in the following table.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0649}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1299}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1299}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0649}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1299}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2987}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(x_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(x_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(x_k\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(y\) (counts)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
log-count {[}\(\log(y)\){]}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & \(x_{11}\) & \(x_{21}\) & \ldots{} & \(x_{k1}\) & \(y_1\) & \(\log(y_1)\) \\
2 & \(x_{12}\) & \(x_{22}\) & \ldots{} & \(x_{k2}\) & \(y_2\) & \(\log(y_2)\) \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} \\
n & \(x_{1n}\) & \(x_{2n}\) & \ldots{} & \(x_{kn}\) & \(y_n\) & \(\log(y_n)\) \\
\end{longtable}

We can use the log of the observed count and values of the explanatory variables to find the least square estimates (LSE) of the regression coefficients, denoted by \(\tilde{\beta}_0, \tilde{\beta}_1, \cdots, \tilde{\beta}_p\), of the Poisson regression model. Then the \(i\)-th fitted value \(\hat{\mu}_i = \exp(\tilde{\beta}_0 + \tilde{\beta}_1 x_1 + \cdots + \tilde{\beta}_p x_p)\).

\begin{itemize}
\tightlist
\item
  \textbf{Maximum Likelihood Estimate (MLE) of} \(\beta\)'s
\end{itemize}

Since the response variable is assumed to have a Poisson with its mean \(\mu_j = \exp(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj})\). The kernel of the log-likelihood of observing the data set is defined in the following

\[
l(\beta_0, \beta_1, \cdots, \beta_p) \propto  \sum_{j=1}^n \left[y_j(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj})-\exp(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj}) \right]
\]

The MLE of the \(\beta\)'s, denoted by \(\hat{\beta}_0, \hat{\beta}_1, \cdots, \hat{\beta}_p\), maximizes the above log-likelihood through solving the following \textbf{score equations},

\[
\left\{
\begin{array}{ccccc}
\frac{\partial l(\beta_0, \beta_1, \cdots, \beta_p)}{\partial \alpha_0} & = & \frac{\partial}{\partial \alpha_0}\sum_{j=1}^n \left[y_j(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj})-\exp(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj}) \right] & = & 0 \\
\frac{\partial l(\beta_0, \beta_1, \cdots, \beta_p)}{\partial \alpha_1} & = & \frac{\partial}{\partial \alpha_1}\sum_{j=1}^n \left[y_j(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj})-\exp(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj}) \right]& = & 0 \\
\cdots & \cdots &\cdots & \cdots &\cdots\\
\frac{\partial l(\beta_0, \beta_1, \cdots, \beta_p)}{\partial \alpha_{p}} & = & \frac{\partial}{\partial \alpha_p}\sum_{j=1}^n \left[y_j(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj})-\exp(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj}) \right]& = & 0  \\
\end{array}
\right.
\]

With the MLE, we can find the fitted value by \(\hat{\mu}_i = \exp(\hat{\beta}_0 + \hat{\beta}_1 x_1 + \cdots + \hat{\beta}_p x_p)\).

\hypertarget{pearson-residuals}{%
\subsection{Pearson Residuals}\label{pearson-residuals}}

The Pearson residual of \(i\)-th observation is defined to be

\[
\text{Pearson.Residual}_i = \frac{y_i-\hat{\mu}_i}{\sqrt{\hat{\mu}_i}}
\]

Pearson residuals are the standardized value of the observed \(y_i\) with the assumption that \(Y_i\) is a Poisson normal random variable. Under a large sample assumption, we would expect that residuals are approximately normally distributed. We can use this property to assess the appropriateness of the Poisson regression model. Equivalently, the square of the Pearson residual is a chi-square distribution with one degree of freedom.

Next, we extract the residuals (\(y_i-\hat{\mu}_i\)) direct from the linear regression model and then divided by the square root of \(\hat{\mu}_i\), fitted values of \(y_i\), to find the Pearson residuals.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{resid.loglin }\OtherTok{=}\NormalTok{ m0.loglinear}\SpecialCharTok{$}\NormalTok{residuals}
\NormalTok{fitted.loglin }\OtherTok{=}\NormalTok{ m0.loglinear}\SpecialCharTok{$}\NormalTok{fitted.values}
\NormalTok{pearson.resid }\OtherTok{=}\NormalTok{ resid.loglin}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(fitted.loglin)}
\FunctionTok{qqnorm}\NormalTok{(pearson.resid, }\AttributeTok{main =} \StringTok{"Normal Q{-}Q plot of Pearson }\SpecialCharTok{\textbackslash{}n}\StringTok{  Residuals of Poisson Regression"}\NormalTok{)}
\FunctionTok{qqline}\NormalTok{(pearson.resid)}
\DocumentationTok{\#\#}
\NormalTok{seq.bound}\OtherTok{=}\FunctionTok{seq}\NormalTok{(}\FunctionTok{range}\NormalTok{(pearson.resid)[}\DecValTok{1}\NormalTok{], }\FunctionTok{range}\NormalTok{(pearson.resid)[}\DecValTok{2}\NormalTok{], }\AttributeTok{length=}\DecValTok{10}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(pearson.resid, }\AttributeTok{breaks =}\NormalTok{ seq.bound, }
     \AttributeTok{main =} \StringTok{"Pearson Residuals of Poisson"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-163-1} \end{center}

Both the Q-Q plot and histogram indicate that the distribution of Pearson residuals is skewed. There is a discrepancy between frequency distribution and normal distribution. Since the Pearson residuals are derived based on the least square algorithm, they don't have good distributional properties to develop a test.

\hypertarget{deviance-residuals}{%
\subsection{Deviance Residuals}\label{deviance-residuals}}

Deviance residuals of Poisson regression are defined based on the likelihood method in the following

\[
\text{Deviance.Residual}_i=\text{sign}(y_i - \hat{\mu}_i)\sqrt{2\left[y_i\log(y_i/\hat{\mu}_i) -(y_i-\hat{\mu}_i)\right]}
\]

where

\[
\text{sign}(x) = \begin{cases} 
           1    & \text{if } x \geq 0 \\
           - 1  & \text{if } x < 0   \\
           0    & \text{if } x =0
          \end{cases}
\]

Since the deviance residuals based on Poisson regression are defined based on the likelihood, there are asymptotically normally distributed. We can use this asymptotic property of normal distribution to assess the appropriateness of the Poisson regression.

\hfill\break

\hypertarget{numerical-example}{%
\subsection{Numerical Example}\label{numerical-example}}

The residual deviance of Poisson (or other generalized linear models) is defined to be the sum of all deviance residuals: \(\text{deviance} =\sum_i(\text{deviance.residual}_i)^2\). In the output of \textbf{glm()} in R, the \textbf{null deviance residual} (corresponding to the model with no explanatory variable in it) and \textbf{deviance residual} (corresponding to the fitted model) are reported with the corresponding degrees of freedom.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"img10/w10{-}glmPoisOutput.jpg"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{img10/w10-glmPoisOutput} 

}

\caption{R glm() output of Poisson regression model}\label{fig:unnamed-chunk-164}
\end{figure}

We can see from the Poisson regression output in R that only deviance residuals and the corresponding descriptive statistics are reported (the five-number-summary of deviance residuals, null deviance, and deviance with corresponding degrees of freedoms). This is because the inference of Poisson regression in \textbf{glm()} is based on the likelihood theory.

We can also extract the deviance residuals from \textbf{glm()} object and make a Q-Q plot in the following

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\FunctionTok{qqnorm}\NormalTok{(m1.Poisson}\SpecialCharTok{$}\NormalTok{residuals, }
       \AttributeTok{main =} \StringTok{"Normal Q{-}Q plot of Deviance }\SpecialCharTok{\textbackslash{}n}\StringTok{  Residuals of Poisson Regression"}\NormalTok{)}
\FunctionTok{qqline}\NormalTok{(m1.Poisson}\SpecialCharTok{$}\NormalTok{residuals)}
\NormalTok{resid.m1 }\OtherTok{=}\NormalTok{ m1.Poisson}\SpecialCharTok{$}\NormalTok{residuals}
\NormalTok{seq.bound}\OtherTok{=}\FunctionTok{seq}\NormalTok{(}\FunctionTok{range}\NormalTok{(resid.m1)[}\DecValTok{1}\NormalTok{], }\FunctionTok{range}\NormalTok{(resid.m1)[}\DecValTok{2}\NormalTok{], }\AttributeTok{length=}\DecValTok{9}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(m1.Poisson}\SpecialCharTok{$}\NormalTok{residuals, }\AttributeTok{breaks =}\NormalTok{ seq.bound, }
     \AttributeTok{main =} \StringTok{"Deviance Residuals of Poisson"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-165-1} \end{center}

Both Q-Q plot and histogram indicate that the distribution of the deviance residuals is slightly different from a normal distribution. If the model is correctly specified, the sum of squared residuals \(\sum_i(\text{Deviance.Residual}_i)^2\) is distributed as \(\chi^2_{n-p}\). The deviance and the degrees of freedom of the deviance are given in the output of the \textbf{glm()} (see the output given in the above figure).

For example, we next extract the deviance and degrees of freedom from the output and perform a chi-square test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{deviance.resid }\OtherTok{=}\NormalTok{ m1.Poisson}\SpecialCharTok{$}\NormalTok{deviance}
\NormalTok{deviance.df }\OtherTok{=}\NormalTok{ m1.Poisson}\SpecialCharTok{$}\NormalTok{df.residual}
\CommentTok{\# p{-}value of chi{-}square test}
\NormalTok{p.value }\OtherTok{=} \DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{pchisq}\NormalTok{(deviance.resid, deviance.df)}
\NormalTok{pval }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{p.value =}\NormalTok{ p.value)}
\FunctionTok{kable}\NormalTok{(pval, }\AttributeTok{caption=}\StringTok{"The p{-}value of deviance chi{-}square test"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-166}The p-value of deviance chi-square test}
\centering
\begin{tabular}[t]{r}
\hline
p.value\\
\hline
0\\
\hline
\end{tabular}
\end{table}

The p-value is almost equal to zero. The assumption of the Poisson regression model was violated.

\hypertarget{goodness-of-fit}{%
\subsection{Goodness-of-fit}\label{goodness-of-fit}}

The deviance has an asymptotic \(\chi^2_{n-p}\) distribution if the model is correct. If the p-value calculated based on the deviance from \(\chi^2_{n-p}\) is less than the significance level, we claim the model has a poor fit (also lack-of-fit, badness-of-fit). There could be different reasons that cause the poor fit. For example, (1) data issues such as outliers, (2) functional form of the explanatory variables (non-linear relationship between the log of the mean of the response), (3) missing some important explanatory variable in the data set, (4) dispersion issue, etc.

The deviance residual can be used naturally to \textbf{compare hierarchical models} by defining the likelihood ratio chi-square tests.

The dispersion issue will be detailed in the next section.

\hfill\break

\hypertarget{dispersion-and-dispersed-poisson-regression-model}{%
\section{Dispersion and Dispersed Poisson Regression Model}\label{dispersion-and-dispersed-poisson-regression-model}}

The issue of \textbf{Over-dispersion} in Poisson regression is common. It indicates that the variance is bigger than the mean.

\hypertarget{definition-of-dispersion}{%
\subsection{Definition of Dispersion}\label{definition-of-dispersion}}

To detect over-dispersion (i.e., the violation of the assumption in Poisson regression), we define the following dispersion parameter

\[
\hat{\phi} = \frac{\sum_i(\text{Pearson.Residual}_i)^2}{n-p} ,
\]

where \(p\) is the number of regression coefficients. Note that \(\sum_i(\text{Pearson.Residual}_i)^2\) has a \(\chi_{n-1}^2\) if the Poisson assumption is correct. Since the expectation of a chi-square distribution is equal to the degrees of freedom, this means that the \textbf{estimated dispersion parameter}, \(\hat{\phi}\), should be around 1 if the Poisson assumption is correct. Therefore, the estimated dispersion parameter can be used to detect potential dispersion issues.

\begin{itemize}
\tightlist
\item
  \textbf{Impact of Dispersion}
\end{itemize}

Over-dispersion means the assumptions of the Poisson model (or other models in the exponential family) are not met, therefore, the p-values in the output of \textbf{glm()} with the regular \emph{log link} in the \emph{poisson family} are not reliable. We should use p-values in the output to perform significant tests and use them for variable selection.

\hfill\break

\hypertarget{quasi-poisson-regression-model}{%
\subsection{Quasi-Poisson Regression Model}\label{quasi-poisson-regression-model}}

We can make an adjustment to the Poisson variance by adding a dispersion parameter. In other words, while for Poisson data \(\bar{Y} = s^2_Y\), the quasi-Poisson allows for \(\bar{Y} = \phi \cdot s^2_Y\), and estimates the over-dispersion parameter \(\phi\) (or under-dispersion, if \(\phi < 1\)). The estimated \(\phi\) is given earlier.

The parameters of the Poisson regression model are estimated based on the following \textbf{adjusted score} equations.

\[
\left\{
\begin{array}{ccccc}
 \frac{\partial}{\partial \alpha_0} \frac{\sum_{j=1}^n \left[y_j(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj})-\exp(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj}) \right]}{\phi} & = & 0 \\
 \frac{\partial}{\partial \alpha_1}\frac{\sum_{j=1}^n \left[y_j(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj})-\exp(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj}) \right] }{\phi}& = & 0 \\
\cdots & \cdots &\cdots \\
 \frac{\partial}{\partial \alpha_p}\frac{\sum_{j=1}^n \left[y_j(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj})-\exp(\beta_0 + \beta_1 x_{1j} + \cdots + \beta_p x_{pj}) \right] }{\phi}& = & 0  \\
\end{array}
\right.
\]

Note that the above \textbf{score equations} are \textbf{not} derived from the loglikelihood function of the regular Poisson regression model. Unlike the likelihood-based goodness of fit measures such as \emph{deviance}, \emph{AIC}, and \emph{SBC} measures should not be used in inference.

Apparently, the estimated regression coefficients are identical since the scale parameter \(\phi\) will not impact the solution to the linear system. However, the variance \(\text{V}(\mathbf{Y}) = \phi \text{V}(\hat{\mu})\). Thus, we now have a parameter that allows the variance to be larger or smaller than the mean by a multiplicative factor \(\phi\). Hence, it will affect the inference of QMLE of the regression coefficients

\[
\hat{\mathbf{\beta}} \to \mathbf{N}[\mathbf{\beta},~ \phi(\mathbf{X}^T\mathbf{WX})^{-1}]
\]
That means the standard errors of \(\hat{\beta}_i\) (\(i = 0,1, 2, \cdots, k\)) are different from the MLE in the regular Poisson regression. Because of this, the reported p-values are also different from those of the output of the regular Poisson regression model.

\hfill\break

\hypertarget{numerical-example-1}{%
\subsection{Numerical Example}\label{numerical-example-1}}

Next, we use \textbf{glm()} to fit the quasi-Poisson model and compare its output with that of the regular Poisson regression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2.quasi.pois }\OtherTok{=} \FunctionTok{glm}\NormalTok{(BrooklynBridge }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Day }\SpecialCharTok{+}\NormalTok{ HighTemp }\SpecialCharTok{+}\NormalTok{ LowTemp }\SpecialCharTok{+}\NormalTok{ Precipitation, }
                    \AttributeTok{family =}\NormalTok{ quasipoisson, }\AttributeTok{offset =} \FunctionTok{log}\NormalTok{(Total),  }\AttributeTok{data =}\NormalTok{ cyclist)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"img10/w10{-}QuasiPoisOutput.jpg"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=15.76in]{img10/w10-QuasiPoisOutput} 

}

\caption{R glm() output of quasi-Poisson regression model}\label{fig:unnamed-chunk-168}
\end{figure}

We can see from the output of the quasi-likelihood-based Poisson regression that the dispersion parameter is \(\hat{\phi} = 5.420292\). Since the dispersion parameter is significantly different from 1, the p-values in the output of the Poisson regression model are not reliable. The main effect is the substantially larger errors for the estimates (the point estimates do not change), and hence potentially changed the significance of explanatory variables.

We can manually compute the corrected standard errors in the quasi-Poisson model by adjusting the standard error from the Poisson standard errors using relation \(SE_Q(\hat{\beta})=SE(\hat{\beta})\times \sqrt{\hat{\phi}}\). For example, considering the standard error of \(\hat{\beta}_1\) (associated with dummy variable \textbf{DayMonday}), \(SE(\hat{\beta}_1) = 0.0134153\), in the output of the regular Poisson regression model. The corresponding corrected standard error in the quasi-Poisson model is given by \(\sqrt{5.420292}\times 0.0134153 = 0.03123286\), which is the same as the one reported in the quasi-Poisson model.

\hypertarget{summary-and-concluding-remarks}{%
\subsection{Summary and Concluding Remarks}\label{summary-and-concluding-remarks}}

We have introduced regular Poisson and quasi-Poisson regression modes in this and previous notes. The two models use the same formulation, but different estimations are used. The regular Poisson model is based on the likelihood estimation, all statistics reported in the out are valid. However, the quasi-Poisson model is \textbf{not} based on the standard maximum likelihood estimation, and not all reported statistics can be used for inference.

\begin{itemize}
\tightlist
\item
  Regular Poisson Model

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Assume \(Y\) to be a Poisson random variable.
  \item
    link function is \(\log(\cdot)\) , the model is defined to be \(\log(\mu) =\beta_0 + \sum_{i=1}^k \beta_ix_i\)
  \item
    Score equation: first-order partial derivative of the log-likelihood function
  \item
    Parameter estimation - MLE via Fisher scoring algorithm
  \item
    Regression coefficient is log risk ratio.
  \item
    Pearson and Deviance residuals are defined for model diagnosis.
  \item
    All likelihood-based statistics such as \emph{R-square, AIC, and SBC} are valid and can be used as usual.
  \end{enumerate}
\item
  Quasi-Poisson Model:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Assume \(Y\) to be a Poisson random variable.
  \item
    link function is \(\log(\cdot)\) , the model is defined to be \(\log(\mu) =\beta_0 + \sum_{i=1}^k \beta_ix_i\)
  \item
    Score equation: first order partial derivative of the scaled log-likelihood function (\emph{quasi-likelihood}).
  \item
    Parameter estimation - MLE via Fisher scoring algorithm
  \item
    Regression coefficient is log risk ratio.
  \item
    Pearson and Deviance residuals are defined for model diagnosis.
  \item
    All likelihood based statistics such as \emph{R-square, AIC, SBC} are \textbf{theoretically invalid}.
  \end{enumerate}
\item
  Which Model Should Be Used?

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    In predictive modeling, both models will yield the same results.
  \item
    In the association analysis, the regular Poisson model should be used when dispersion is not an issue (i.e., \(\phi\) is close to 1.). However, the quasi-Poisson should be used when \(\phi\) is significantly different from 1.
  \item
    When there is no dispersion, could we simply use the quasi-Poisson in the association analysis? The answer is No.~The reason is that the additional approximation was used to adjust the estimation of the standard error and the approximation also add rounding errors to the result. From the computational perspective, it uses more system resources.
  \end{enumerate}
\end{itemize}

\hfill\break

\hypertarget{case-study-i-denmark-cities-lung-cancer-rates}{%
\section{Case Study I: Denmark Cities Lung Cancer Rates}\label{case-study-i-denmark-cities-lung-cancer-rates}}

This is a complete analysis of the case study of Denmark Cities Ling Cancer rate we started in the previous module.

\hypertarget{introduction-1}{%
\subsection{Introduction}\label{introduction-1}}

The World Health Organisation (WHO) statistics suggest that Denmark has the highest cancer rates in the world, with about 326 people out of every 100,000 developing cancer each year. The country is known to have a good record of diagnosing cancer but also has high rates of smoking among women and high levels of alcohol consumption.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"img10/DenmarkCitiesMap.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{img10/DenmarkCitiesMap} \end{center}

In this case study, we use a data set that summarized the lung cancer incident counts (cases) per age group for four Danish cities from 1968 to 1971. The primary random response variable is lung cancer cases. The predictor variables are the age group and the total population size of the neighboring cities.

The data set was built in the R library \{ISwR\}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{data}\NormalTok{(eba1977)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(eba1977), }\AttributeTok{caption =} \StringTok{"First few records in the data set"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-170}First few records in the data set}
\centering
\begin{tabular}[t]{l|l|r|r}
\hline
city & age & pop & cases\\
\hline
Fredericia & 40-54 & 3059 & 11\\
\hline
Horsens & 40-54 & 2879 & 13\\
\hline
Kolding & 40-54 & 3142 & 4\\
\hline
Vejle & 40-54 & 2520 & 5\\
\hline
Fredericia & 55-59 & 800 & 11\\
\hline
Horsens & 55-59 & 1083 & 6\\
\hline
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# check the values of the variables in the data set}
\end{Highlighting}
\end{Shaded}

Since it's reasonable to assume that the expected count of lung cancer incidents is proportional to the population size, we would prefer to model the rate of incidents per capita. However, for the purpose of illustration, we will fit the Poisson regression model with both counts and rate of cancer rates.

\hypertarget{poisson-regression-on-cancer-counts-1}{%
\subsection{Poisson Regression on Cancer Counts}\label{poisson-regression-on-cancer-counts-1}}

We first build a Poisson frequency regression model and ignore the population size of each city in the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.freq }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(cases }\SpecialCharTok{\textasciitilde{}}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ age, }\AttributeTok{family =} \FunctionTok{poisson}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{), }\AttributeTok{data =}\NormalTok{ eba1977)}
\DocumentationTok{\#\#}
\NormalTok{pois.count.coef }\OtherTok{=} \FunctionTok{summary}\NormalTok{(model.freq)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(pois.count.coef, }\AttributeTok{caption =} \StringTok{"The Poisson regression model for the counts of lung }
\StringTok{      cancer cases versus the geographical locations and the age group."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-171}The Poisson regression model for the counts of lung 
      cancer cases versus the geographical locations and the age group.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & 2.2437446 & 0.2036265 & 11.0189233 & 0.0000000\\
\hline
cityHorsens & -0.0984401 & 0.1812909 & -0.5429952 & 0.5871331\\
\hline
cityKolding & -0.2270575 & 0.1877041 & -1.2096561 & 0.2264109\\
\hline
cityVejle & -0.2270575 & 0.1877041 & -1.2096561 & 0.2264109\\
\hline
age55-59 & -0.0307717 & 0.2480988 & -0.1240298 & 0.9012916\\
\hline
age60-64 & 0.2646926 & 0.2314278 & 1.1437369 & 0.2527328\\
\hline
age65-69 & 0.3101549 & 0.2291839 & 1.3533017 & 0.1759593\\
\hline
age70-74 & 0.1923719 & 0.2351660 & 0.8180261 & 0.4133423\\
\hline
age75+ & -0.0625204 & 0.2501222 & -0.2499593 & 0.8026188\\
\hline
\end{tabular}
\end{table}

The above inferential table about the regression coefficients indicates both city and age are insignificant. This means, if we look at cancer count across the age group and city, there is no statistical evidence to support the potential discrepancy across the age groups and cities. However, this does not imply that the model is meaningless from the practical perspective since statistical significance is not equivalent the clinical importance. Moreover, the sample size could impact the statistical significance of some of the variables.

The other way to look at the model is the appropriateness model. The cancer counts are dependent on the population size. Ignoring the population size implies the information in the sample was not effectively used. In the next subsection, we model the cancer rates that involve the population size.

The other way to look at the model is goodness of the model. The cancer counts are dependent on the population size. Ignoring the population size implies the information in the sample was not effectively used. In the next subsection, we model the cancer rates that involve the population size.

\hypertarget{poisson-regression-on-rates-1}{%
\subsection{Poisson Regression on Rates}\label{poisson-regression-on-rates-1}}

The following model assesses the potential relationship between cancer death rates and age. This is the primary interest of the model. We also want to adjust the relationship be the potential neighboring cities.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.rates }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(cases }\SpecialCharTok{\textasciitilde{}}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ age, }\AttributeTok{offset =} \FunctionTok{log}\NormalTok{(pop), }
                   \AttributeTok{family =} \FunctionTok{poisson}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{), }\AttributeTok{data =}\NormalTok{ eba1977)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(model.rates)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =} \StringTok{"Poisson regression on the rate of the }
\StringTok{      the cancer rate in the four Danish cities adjusted by age."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-172}Poisson regression on the rate of the 
      the cancer rate in the four Danish cities adjusted by age.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & -5.6320645 & 0.2002545 & -28.124529 & 0.0000000\\
\hline
cityHorsens & -0.3300600 & 0.1815033 & -1.818479 & 0.0689909\\
\hline
cityKolding & -0.3715462 & 0.1878063 & -1.978348 & 0.0478895\\
\hline
cityVejle & -0.2723177 & 0.1878534 & -1.449629 & 0.1471620\\
\hline
age55-59 & 1.1010140 & 0.2482858 & 4.434463 & 0.0000092\\
\hline
age60-64 & 1.5186123 & 0.2316376 & 6.555985 & 0.0000000\\
\hline
age65-69 & 1.7677062 & 0.2294395 & 7.704455 & 0.0000000\\
\hline
age70-74 & 1.8568633 & 0.2353230 & 7.890701 & 0.0000000\\
\hline
age75+ & 1.4196534 & 0.2502707 & 5.672472 & 0.0000000\\
\hline
\end{tabular}
\end{table}

The above table indicates that the log of cancer rate is not identical across the age groups and among the four cities. To be more specific, the log rates of Fredericia (baseline city) were higher than in the other three cities. The youngest age group (45-55) has the lowest log rate. The regression coefficients represent the change of log rate between the associate age group and the reference age group. The same interpretation applies to the change in log rate among the cities.

\hfill\break

\hypertarget{quasi-poisson-rate-model}{%
\subsection{Quasi-Poisson Rate Model}\label{quasi-poisson-rate-model}}

The above two Poison models assume that there is no dispersion issue in the model. The quasi-Poisson through \texttt{glm()} returns the dispersion coefficient.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quasimodel.rates }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(cases }\SpecialCharTok{\textasciitilde{}}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ age, }\AttributeTok{offset =} \FunctionTok{log}\NormalTok{(pop), }
                   \AttributeTok{family =}\NormalTok{ quasipoisson, }\AttributeTok{data =}\NormalTok{ eba1977)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(model.rates)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =} \StringTok{"Quasi{-}Poisson regression on the rate of the cancer rate in the four Danish cities adjusted by age."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-173}Quasi-Poisson regression on the rate of the cancer rate in the four Danish cities adjusted by age.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & -5.6320645 & 0.2002545 & -28.124529 & 0.0000000\\
\hline
cityHorsens & -0.3300600 & 0.1815033 & -1.818479 & 0.0689909\\
\hline
cityKolding & -0.3715462 & 0.1878063 & -1.978348 & 0.0478895\\
\hline
cityVejle & -0.2723177 & 0.1878534 & -1.449629 & 0.1471620\\
\hline
age55-59 & 1.1010140 & 0.2482858 & 4.434463 & 0.0000092\\
\hline
age60-64 & 1.5186123 & 0.2316376 & 6.555985 & 0.0000000\\
\hline
age65-69 & 1.7677062 & 0.2294395 & 7.704455 & 0.0000000\\
\hline
age70-74 & 1.8568633 & 0.2353230 & 7.890701 & 0.0000000\\
\hline
age75+ & 1.4196534 & 0.2502707 & 5.672472 & 0.0000000\\
\hline
\end{tabular}
\end{table}

The dispersion index can be extracted from the quasi-Poisson object with the following code

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ydif}\OtherTok{=}\NormalTok{eba1977}\SpecialCharTok{$}\NormalTok{cases}\SpecialCharTok{{-}}\FunctionTok{exp}\NormalTok{(model.rates}\SpecialCharTok{$}\NormalTok{linear.predictors)  }\CommentTok{\# diff between y and yhat}
\NormalTok{prsd }\OtherTok{=}\NormalTok{ ydif}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{exp}\NormalTok{(model.rates}\SpecialCharTok{$}\NormalTok{linear.predictors))   }\CommentTok{\# Pearson residuals}
\NormalTok{phi }\OtherTok{=} \FunctionTok{sum}\NormalTok{(prsd}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{/}\DecValTok{15}          \CommentTok{\# Dispersion index: 24{-}9 = 15  }
\FunctionTok{kable}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{Dispersion =}\NormalTok{ phi))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r}
\hline
Dispersion\\
\hline
1.504109\\
\hline
\end{tabular}

\hfill\break

\hypertarget{final-working-model}{%
\subsection{Final Working Model}\label{final-working-model}}

The dispersion index is 1.56. It is slightly dispersed. We stay with the regular Poisson regression model.

The intercept represents the \textbf{baseline log-cancer rate} ( of baseline \emph{age group 44-55} in the baseline city \emph{Fredericia}). The actual rate is \(\exp(-5.6321) \approx 0.36\%\) which is close to the recently reported rate of the country by WHO. The slope \(-0.3301\) is the difference of the log-rates between baseline city Fredericia and the city of Horsens at any given age group, to be more specific, \(\log(R_{\text{Horsen}}) - \log(R_{\text{Fredericia}}) = -0.3301\) which is equivalent to

\[
\log \left( \frac{R_{\text{Horsen}}}{R_{\text{Fredericia}}} \right) = -0.3301 ~~~\Rightarrow~~~\frac{R_{\text{Horsen}}}{R_{\text{Fredericia}}} = e^{-0.3301} \approx 0.7188518.
\]

This means, with fixed age groups, the cancer rate in Horsens is about \(28\%\) lower than that in Fredericia. Next, we look at the coefficient \(1.4197\) associated with age group \(\text{75+}\). For any given city,

\[
\log \left(\frac{R_{\text{age75+}}}{R_{\text{age45-54}}} \right) = 1.4197~~~\Rightarrow~~~\frac{R_{\text{age75+}}}{R_{\text{age45-54}}} = e^{1.41971} \approx 4.135921. 
\]

This implies that the cancer rate in the age group 75+ is 4.14 times that of the baseline age group of 45-54.

\hypertarget{some-visual-comparisons}{%
\subsection{Some Visual Comparisons}\label{some-visual-comparisons}}

The inferential tables of the Poisson regression models in the previous sections give numerical information about the potential discrepancy across the age group and among the cities. Next, we create a graph to visualize the relationship between cancer rate and age across cities.

First of all, every city has a trend line that reflects the relationship between the cancer rate and age. We next find the rates of combinations of city and age group based on the following working rate model.
\[
\text{log-rate} = -5.6321 -0.3301 \times \text{cityHorsens} -0.3715 \times \text{cityKolding} -0.2723 \times \text{cityVejle} + 1.1010 \times \text{age55-59} \\ + 1.5186 \times \text{age60-64} + 1.7677 \times  \text{age65-69} + 1.8569 \times \text{age70-74} + 1.4197 \times \text{age75+}
\]

Or equivalently, we can write the rate model as

\[
rate =\exp(-5.6321 -0.3301 \times \text{cityHorsens} -0.3715 \times \text{cityKolding} -0.2723 \times \text{cityVejle} + 1.1010 \times \text{age55-59}) \\ \times \exp( 1.5186 \times \text{age60-64} + 1.7677 \times  \text{age65-69} + 1.8569 \times \text{age70-74} + 1.4197 \times \text{age75+})
\]

To make the visual representation of the output, we tabulate cancer rates of the corresponding combinations of city and age group in the following calculation based on the regression equation with coefficients given in above table 3. Note that all variables in the model are indicator variables. Each of these indicator variables takes only two possible values: 0 and 1.

For example, \(\exp(-5.632)\) gives the cancer rate of the baseline city, Fredericia, and the baseline age group {[}45-54{]}. \(\exp(-5.632+1.101)\) gives the cancer rate of baseline city, Fredericia, and age group {[}55-59{]}. Following the same pattern, we can find the cancer rate for each combination of the city and age group.

The following table calculates the \textbf{estimated} cancer rates of cities Fredericia and Horsens across age groups. The rates for other cities can be similarly calculated.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2361}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3611}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4028}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fredericia's Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Horsens' Rates
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
{[}40 - 49{]} & \(\exp(-5.632)\) & \(\exp(-5.632-0.331)\) \\
{[}55 - 59{]} & \(\exp(-5.632+1.101)\) & \(\exp(-5.632-0.331+1.101)\) \\
{[}60 - 64{]} & \(\exp(-5.632+1.52)\) & \(\exp(-5.632-0.331+1.52)\) \\
{[}65 - 69{]} & \(\exp(-5.632+1.77)\) & \(\exp(-5.632-0.331+1.77)\) \\
{[}70 - 74{]} & \(\exp(-5.632+1.86)\) & \(\exp(-5.632-0.331+1.86)\) \\
75+ & \(\exp(-5.632+1.42)\) & \(\exp(-5.632-0.331+1.42)\) \\
\end{longtable}

We use age as the horizontal axis and the estimated cancer rates (in the above table) as the vertical axis to make the trend lines for each of the four cities using the following code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fredericia}
\NormalTok{Fredericia }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632+1.101}\NormalTok{),   }
               \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632+1.52}\NormalTok{),}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632+1.77}\NormalTok{),}
               \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632+1.86}\NormalTok{),}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632+1.42}\NormalTok{))}
\CommentTok{\# Horsens}
\NormalTok{Horsens }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331+1.101}\NormalTok{),   }
            \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331+1.52}\NormalTok{),}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331+1.77}\NormalTok{),}
            \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331+1.86}\NormalTok{),}
            \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.331+1.42}\NormalTok{))}
\CommentTok{\# Kolding}
\NormalTok{Kolding}\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372+1.101}\NormalTok{),   }
           \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372+1.52}\NormalTok{),}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372+1.77}\NormalTok{),}
           \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372+1.86}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.372+1.42}\NormalTok{))}
\CommentTok{\# Vejle}
\NormalTok{Vejle }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272+1.101}\NormalTok{),   }
          \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272+1.52}\NormalTok{),}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272+1.77}\NormalTok{),}
          \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272+1.86}\NormalTok{), }\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{5.632{-}0.272+1.42}\NormalTok{))}
\NormalTok{minmax }\OtherTok{=} \FunctionTok{range}\NormalTok{(}\FunctionTok{c}\NormalTok{(Fredericia,Horsens,Kolding,Vejle))}
\DocumentationTok{\#\#\#\#}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{,Fredericia, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{lty =}\DecValTok{1}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }
               \AttributeTok{ylab=}\StringTok{"Cancer Rate"}\NormalTok{, }\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{6}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.03}\NormalTok{), }\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{ )}
\FunctionTok{title}\NormalTok{(}\StringTok{"The Trend Line of Cancer Rates"}\NormalTok{)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{labels=}\FunctionTok{c}\NormalTok{(}\StringTok{"[45{-}54]"}\NormalTok{,}\StringTok{"[55,59]"}\NormalTok{,}\StringTok{"[60,64]"}\NormalTok{,}\StringTok{"[65,69]"}\NormalTok{,}\StringTok{"[70,74]"}\NormalTok{,}\StringTok{"75+"}\NormalTok{), }
            \AttributeTok{at =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{,Fredericia, }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Horsens, }\AttributeTok{lty =}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Horsens, }\AttributeTok{pch=}\DecValTok{20}\NormalTok{, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Kolding, }\AttributeTok{lty =}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"purple"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Kolding, }\AttributeTok{pch=}\DecValTok{21}\NormalTok{, }\AttributeTok{col=}\StringTok{"purple"}\NormalTok{)}
\DocumentationTok{\#\#\#}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Vejle, }\AttributeTok{lty =}\DecValTok{4}\NormalTok{, }\AttributeTok{col=}\StringTok{"mediumvioletred"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, Vejle, }\AttributeTok{pch=}\DecValTok{22}\NormalTok{, }\AttributeTok{col=}\StringTok{"mediumvioletred"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Fredericia"}\NormalTok{,}\StringTok{"Horsens"}\NormalTok{, }\StringTok{"Kolding"}\NormalTok{, }\StringTok{"Vejle"}\NormalTok{ ),}
                  \AttributeTok{pch=}\DecValTok{19}\SpecialCharTok{:}\DecValTok{22}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,  }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{, }
        \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"purple"}\NormalTok{, }\StringTok{"mediumvioletred"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-176-1.pdf}

\hypertarget{discussions-and-conclusions-1}{%
\subsection{Discussions and Conclusions}\label{discussions-and-conclusions-1}}

Several conclusions we can draw from the output of the regression models.

The regression model based on the cancer count is not appropriate since the information on the population size is a key variable in the study of cancer distribution. Simply including the population size in the regression model will reduce the statistical significance of age. See the following output of the fitted Poisson regression model of count adjusted by population size.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.freq.pop }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(cases }\SpecialCharTok{\textasciitilde{}}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(pop), }\AttributeTok{family =} \FunctionTok{poisson}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{), }
                      \AttributeTok{data =}\NormalTok{ eba1977)}
\DocumentationTok{\#\#}
\NormalTok{pois.count.coef.pop }\OtherTok{=} \FunctionTok{summary}\NormalTok{(model.freq.pop)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(pois.count.coef.pop, }\AttributeTok{caption =} \StringTok{"The Poisson regression model for }
\StringTok{         the counts of lung cancer cases versus the geographical locations, }
\StringTok{         population size, and age group."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-177}The Poisson regression model for 
         the counts of lung cancer cases versus the geographical locations, 
         population size, and age group.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & 11.7495934 & 8.8151328 & 1.3328890 & 0.1825682\\
\hline
cityHorsens & 0.1832573 & 0.3192679 & 0.5739922 & 0.5659731\\
\hline
cityKolding & -0.0483001 & 0.2519622 & -0.1916957 & 0.8479806\\
\hline
cityVejle & -0.1679335 & 0.1964757 & -0.8547289 & 0.3927012\\
\hline
age55-59 & -1.3842350 & 1.2728775 & -1.0874849 & 0.2768226\\
\hline
age60-64 & -1.2366489 & 1.4049520 & -0.8802073 & 0.3787470\\
\hline
age65-69 & -1.4377681 & 1.6310051 & -0.8815228 & 0.3780349\\
\hline
age70-74 & -1.8048920 & 1.8607922 & -0.9699589 & 0.3320670\\
\hline
age75+ & -1.8383162 & 1.6587773 & -1.1082357 & 0.2677600\\
\hline
log(pop) & -1.2095837 & 1.1227191 & -1.0773698 & 0.2813151\\
\hline
\end{tabular}
\end{table}

We can see from the above output the adding population size to the model

The cancer rate in Fredericia is significantly higher than in the other three cities. It seems that there is no significant difference between Horsens, Kolding, and Vejle. The reason why Fredericia has a higher cancer rate needs further investigation with additional information.

There is a curve linear relationship between age and the cancer rate. The cancer rate increases as age increase. However, the rate starts decreasing after 75. This pattern is consistent with the clinical studies since lung cancer patients were mostly diagnosed between 65-70. It is rare to see lung cancer patients aged under 45.

The last statistical observation is that there is no interaction effect between the age groups and the geographic locations. The rate curves are ``parallel''.

This is only a small data set with limited information. All conclusions in this report are only based on the given data set.

\hypertarget{case-study-ii---ph.d.-and-mentors-productivity}{%
\section{Case Study II - Ph.D.~and Mentor's Productivity}\label{case-study-ii---ph.d.-and-mentors-productivity}}

In this case study, we use data from Long (1990) on the number of publications produced by Ph.D.~biochemists to illustrate the application of Poisson models. The variables in the data set are listed below.

\hypertarget{variable-description}{%
\subsection{Variable Description}\label{variable-description}}

\begin{itemize}
\tightlist
\item
  articles: integer. articles in the last three years of Ph.D.
\item
  gender: factor. coded one for females.
\item
  married: factor. coded one if married.
\item
  kids: integer. the number of children under age six.
\item
  prestige: numeric.the prestige of Ph.D.~program
\item
  mentor: integer. articles by the mentor in last three years
\end{itemize}

\hypertarget{research-question-2}{%
\subsection{Research Question}\label{research-question-2}}

We want to assess how factors affect the number of articles published in the last three years in the Ph.D.~programs.

\hfill\break

\hypertarget{data-preparation-and-variable-processing}{%
\subsection{Data Preparation and Variable Processing}\label{data-preparation-and-variable-processing}}

Variable \textbf{kids} is a discrete variable. We create a frequency table of \textbf{kids} and found that 16 of 915 Ph.D.~students had 3 kids. After additional exploratory analysis. We decide to dichotomize \textbf{kids} and redefine a new variable under the name \textbf{newkids}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{phd}\OtherTok{=}\FunctionTok{read.table}\NormalTok{(}\StringTok{"img10/w10{-}ph{-}data.txt"}\NormalTok{,}\AttributeTok{skip=}\DecValTok{10}\NormalTok{, }\AttributeTok{header=}\ConstantTok{TRUE}\NormalTok{ )[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] }\CommentTok{\# drop the ID variable}
\NormalTok{id}\FloatTok{.3} \OtherTok{=} \FunctionTok{which}\NormalTok{(phd}\SpecialCharTok{$}\NormalTok{kids }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}
\NormalTok{newkids }\OtherTok{=}\NormalTok{ phd}\SpecialCharTok{$}\NormalTok{kids}
\NormalTok{newkids[id}\FloatTok{.3}\NormalTok{] }\OtherTok{=} \DecValTok{1}
\NormalTok{phd}\SpecialCharTok{$}\NormalTok{newkids }\OtherTok{=}\NormalTok{ newkids}
\end{Highlighting}
\end{Shaded}

\hypertarget{poison-regression-modeling-1}{%
\subsection{Poison Regression Modeling}\label{poison-regression-modeling-1}}

We build both the regular Poisson and Quasi-Poisson regression models and extract the dispersion parameter to decide which model should be used as working model.

\hypertarget{section}{%
\section{}\label{section}}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# phd=read.table("w10{-}ph{-}data.txt",skip=10, header=TRUE )[,{-}1] \# drop the ID variable}
\DocumentationTok{\#\# Regular Poison Model}
\NormalTok{pois.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(newkids) }\SpecialCharTok{+}\NormalTok{ prestige }\SpecialCharTok{+}\NormalTok{ mentor, }
                 \AttributeTok{family =} \FunctionTok{poisson}\NormalTok{(}\AttributeTok{link=}\StringTok{"log"}\NormalTok{), }\AttributeTok{data =}\NormalTok{phd)  }
\DocumentationTok{\#\# Quasi Poisson or dispersed Poisson model}
\NormalTok{quasi.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(newkids) }\SpecialCharTok{+}\NormalTok{ prestige }\SpecialCharTok{+}\NormalTok{ mentor, }
                  \AttributeTok{family =}\NormalTok{ quasipoisson, }\AttributeTok{data =}\NormalTok{phd)}
\DocumentationTok{\#\# Extracting dispersion parameter}
\NormalTok{SE.q }\OtherTok{=} \FunctionTok{summary}\NormalTok{(quasi.model)}\SpecialCharTok{$}\NormalTok{coef[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{SE }\OtherTok{=} \FunctionTok{summary}\NormalTok{(pois.model)}\SpecialCharTok{$}\NormalTok{coef[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{dispersion }\OtherTok{=}\NormalTok{ (SE.q}\SpecialCharTok{/}\NormalTok{SE)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{disp }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{dispersion =}\NormalTok{ dispersion)}
\FunctionTok{kable}\NormalTok{(disp, }\AttributeTok{caption=}\StringTok{"Dispersion parameter"}\NormalTok{, }\AttributeTok{align =} \StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-179}Dispersion parameter}
\centering
\begin{tabular}[t]{c}
\hline
dispersion\\
\hline
1.841565\\
\hline
\end{tabular}
\end{table}

The dispersion parameter 1.829006 indicates that the Poisson model is inappropriate. We need to correct the dispersion issue. The quasi-likelihood-based Poisson model is one option.

Next, we summarize the inferential statistics about the regression coefficients in the following table

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SE.quasi.pois }\OtherTok{=} \FunctionTok{summary}\NormalTok{(quasi.model)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(SE.quasi.pois, }\AttributeTok{caption =} \StringTok{"Summary statistics of quasi{-}poisson regression model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-180}Summary statistics of quasi-poisson regression model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 0.4579404 & 0.1290407 & 3.5488055 & 0.0004068\\
\hline
genderWomen & -0.2179247 & 0.0742536 & -2.9348721 & 0.0034208\\
\hline
marriedSingle & -0.1516973 & 0.0855315 & -1.7735846 & 0.0764666\\
\hline
factor(newkids)1 & -0.2495633 & 0.0859576 & -2.9033304 & 0.0037815\\
\hline
prestige & 0.0102754 & 0.0359069 & 0.2861675 & 0.7748150\\
\hline
mentor & 0.0258173 & 0.0027397 & 9.4233384 & 0.0000000\\
\hline
\end{tabular}
\end{table}

In the above quasi-Poisson regression, variable prestige is insignificant (p-value = 0.72). The p-value for testing the significance of the variable married is 0.079. We refit the quasi-Poisson model by dropping \textbf{prestige} and \textbf{married}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quasi.model}\FloatTok{.02} \OtherTok{=} \FunctionTok{glm}\NormalTok{(article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(newkids) }\SpecialCharTok{+}\NormalTok{ mentor, }
                     \AttributeTok{family =}\NormalTok{ quasipoisson, }\AttributeTok{data =}\NormalTok{phd)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(quasi.model}\FloatTok{.02}\NormalTok{)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =} \StringTok{"Inferential statistics of }
\StringTok{the Poisson regression coefficients  in the final working model."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-181}Inferential statistics of 
the Poisson regression coefficients  in the final working model.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 0.4238445 & 0.0645972 & 6.561345 & 0.0000000\\
\hline
genderWomen & -0.2332786 & 0.0738848 & -3.157329 & 0.0016446\\
\hline
factor(newkids)1 & -0.1796153 & 0.0767849 & -2.339200 & 0.0195404\\
\hline
mentor & 0.0257762 & 0.0026586 & 9.695432 & 0.0000000\\
\hline
\end{tabular}
\end{table}

The above model will be used as the final model. The interpretation of the regression coefficient of the Poisson model is not as straightforward as that in the linear regression models since the response variable in the model is at a log-scale.

For example, the coefficient associated with gender is -0.233. This is the estimated Poisson regression coefficient comparing females to males, given the other variables are held constant in the model. The difference in the logs of expected publications is expected to be 0.2332786 units lower for females compared to males while holding the other variables constant in the model. This is still not easy to understand for the general audience.

\hypertarget{some-visual-comparisons-1}{%
\subsection{Some Visual Comparisons}\label{some-visual-comparisons-1}}

Next, we make a visualization to show how the explanatory variables in the final working model affect the \textbf{actual} number of publications of doctoral students.

To this end, we classify all Ph.D.~students in the following four groups defined by \textbf{gender} and \textbf{status} of having at least one child:

\textbf{phd.m0} = male and had no child

\textbf{phd.m1} = male and had at least one child

\textbf{phd.f0} = female and had no child

\textbf{phd.f1} = female and had at least one child

Next, We exponentiate the log-count of publications of Ph.D.~students to the actual number of publications and then make graphs to show the relationship between doctoral students and their mentors in terms of the number of publications in each of the groups defined above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mentors }\OtherTok{=}  \FunctionTok{range}\NormalTok{(phd}\SpecialCharTok{$}\NormalTok{mentor)[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{:}\FunctionTok{range}\NormalTok{(phd}\SpecialCharTok{$}\NormalTok{mentor)[}\DecValTok{2}\NormalTok{]}
\NormalTok{phd.m0 }\OtherTok{=} \FloatTok{0.42384447} \SpecialCharTok{+} \FloatTok{0.02577624}\SpecialCharTok{*}\NormalTok{mentors}
\NormalTok{phd.m1 }\OtherTok{=} \FloatTok{0.42384447} \SpecialCharTok{{-}} \FloatTok{0.17961531} \SpecialCharTok{+} \FloatTok{0.02577624}\SpecialCharTok{*}\NormalTok{mentors}
\NormalTok{phd.f0 }\OtherTok{=} \FloatTok{0.42384447} \SpecialCharTok{{-}}\FloatTok{0.23327860} \SpecialCharTok{+} \FloatTok{0.02577624}\SpecialCharTok{*}\NormalTok{mentors}
\NormalTok{phd.f1 }\OtherTok{=} \FloatTok{0.42384447} \SpecialCharTok{{-}}\FloatTok{0.23327860} \SpecialCharTok{{-}}\FloatTok{0.17961531} \SpecialCharTok{+} \FloatTok{0.02577624}\SpecialCharTok{*}\NormalTok{mentors}
\DocumentationTok{\#\#}
\FunctionTok{plot}\NormalTok{(mentors, }\FunctionTok{exp}\NormalTok{(phd.m0), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{),}
     \AttributeTok{type =} \StringTok{"l"}\NormalTok{,}
     \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
     \AttributeTok{lty =} \DecValTok{1}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"PhD\textquotesingle{}s Publications"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Mentor\textquotesingle{}s publications"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Factors That Impact Student\textquotesingle{}s Publications"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(mentors, }\FunctionTok{exp}\NormalTok{(phd.m1), }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(mentors, }\FunctionTok{exp}\NormalTok{(phd.f0), }\AttributeTok{col =} \StringTok{"darkorchid"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{3}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(mentors, }\FunctionTok{exp}\NormalTok{(phd.f1), }\AttributeTok{col =} \StringTok{"firebrick"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{4}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"male with no child"}\NormalTok{, }\StringTok{"male with at least one child"}\NormalTok{, }
                    \StringTok{"female with no child"}\NormalTok{, }\StringTok{"female with at least one child"}\NormalTok{), }
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"darkorchid"}\NormalTok{, }\StringTok{"firebrick"}\NormalTok{),  }\AttributeTok{lty=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-182-1} \end{center}

We can see the relationship between the number of publications of doctoral students and other factors.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  the number of publications of doctoral students is positively associated with their mentor publication.
\item
  Male doctoral students with no kid published more articles than those who had at least one kid. Female doctoral students also have the same pattern.
\item
  Overall, male doctoral students published more than female students.
\end{enumerate}

\hypertarget{conclusions}{%
\subsection{Conclusions}\label{conclusions}}

The Poisson regression model is used for modeling counts/rates-based data sets. If the model is appropriate, its results are explainable and comparable and backed by statistical theory.

If Poisson regression is not appropriate, we can consider other models depending on the situation. The complex alternatives to the Poisson regression model that can be considered are negative binomial regression, zero-inflated regression models, random-forest-based regression models, and neural-network-based regression models. The last two models are ``black-box'' models because of the lack of interpretability.

\hypertarget{analysis-assignment-4}{%
\section{Analysis Assignment}\label{analysis-assignment-4}}

This week's assignment is to revise your analysis in Week \#9 by adding a new section to include a quasi-Poisson model to the report. In this new analysis, we modify the predictor variables in the following ways.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Instead of using two variables \textbf{HightTemp} and \textbf{LowTemp} in the model, we will use the new variable \textbf{AvgTemp = (HighTemp + LowTemp)/2}.
\item
  Discretize \textbf{Precipitation} using the following definition: if \textbf{Precipitation} = 0, then NewPrecip = 0; if \textbf{Precipitation} \textgreater{} 0, then NewPrecip = 1.
\end{enumerate}

The dispersed Poisson regression model will have three predictor variables: \textbf{Day}, \textbf{AvgTemp}, and \textbf{NewPrecip}.

Here are the steps for building Poisson (quasi-Poisson) model (similar to the two case studies):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Fit a quasi-Poisson regression model on the counts of cyclists who entered and left the Bridge in your data set.
\item
  Report the value of the estimated dispersion parameter and based on the value determine whether the regular Poisson model or the quasi-Poisson should be used as the final model. The two models have the same estimated coefficients by different p-values.
\item
  Make a visualization to show the relationship between the number of cyclists who entered and left the bridge and the related predictor variables.
\end{enumerate}

\hypertarget{concepts-of-time-series-forecasting}{%
\chapter{Concepts of Time Series Forecasting}\label{concepts-of-time-series-forecasting}}

\begin{verbatim}
## 载入需要的程辑包：forecast
\end{verbatim}

\begin{verbatim}
## Warning: 程辑包'forecast'是用R版本4.2.3 来建造的
\end{verbatim}

\begin{verbatim}
## Registered S3 method overwritten by 'quantmod':
##   method            from
##   as.zoo.data.frame zoo
\end{verbatim}

A univariate time series is a sequence of data values with certain random behaviors that occur in successive order over some period of time. A time series can be constructed by any data that is measured over time at evenly-spaced intervals. Some examples are historical stock prices, earnings, GDP, rainfalls, university enrollments, daily counts of ER patients, annual death counts due to traffic accidents in a country, inflation rates, or other sequences of financial or economic data that can be analyzed as a time series.

A forecast is a prediction of some future events. Forecasting problems can be found in many applied areas such as business and industry, economics, finance, environmental sciences, social sciences, political sciences, etc., and can be categorized into

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Short-term forecasting is when a prediction is only made for a few periods ahead (hours, days, weeks).
\item
  Medium-term in which one or two years of forecasting will be made.
\item
  Long-term forecasting in which predictions for several years ahead will be made.
\end{enumerate}

We have discussed different regression models such as linear models and logistic regression models. We also discussed predictions using the fitted regression models. In practice, if the data are collected over time, then the aforementioned regression models are not valid for this type of data since the data values are serially correlated. New models that address time series are needed.

For time-series data, the typical scatter plot of the data values versus the time point at which the data values are observed is shown below.

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-Figure01} \end{center}

Many business applications of forecasting utilize daily, weekly, monthly, quarterly, or annual data, but any reporting interval may be used.

The data may be instantaneous, such as the viscosity of a chemical product at the point in time where it is measured; it may be cumulative, such as the total sales of a product during the month; or it may be a statistic that in some way reflects the activity of the variable during the period, such as the daily closing price of a specific stock on the New York Stock Exchange.

\begin{itemize}
\item
  \textbf{Methods of forecasting}: There are two major methods for forecasting:

  \begin{itemize}
  \item
    Quantitative forecasting methods that make formal use of historical data to build a mathematical/statistical model to project into the future.
  \item
    Qualitative forecasting methods are used when little data are available (new product introduction). In this case, expert opinion is often used.
  \end{itemize}
\end{itemize}

There are two modes for analyzing time series: frequency domain and time domain approaches. We restrict our discussion within the time domain to avoid more advanced mathematical tools in this class.

\begin{itemize}
\item
  \textbf{Statistical forecasting models}:

  \begin{itemize}
  \item
    Regression methods.
  \item
    Smoothing methods.
  \item
    Formal time series analysis methods - ARIMA Box-Jenkins methodology.
  \item
    Dynamic regression methods.
  \end{itemize}
\item
  \textbf{Some terminology}
\item
  Point forecast or point estimate - a single predicted value (no information on the variation of the predicted value)
\item
  Prediction interval - interval prediction, information on accuracy is available.
\item
  Forecast horizon or lead time - based on the fitted model, the number of periods specified to be predicted.
\end{itemize}

The forecast horizon is the length of time into the future for which forecasts are to be prepared.

\hypertarget{types-of-time-series-data}{%
\section{Types of Time Series Data}\label{types-of-time-series-data}}

\hypertarget{uncorrelated-data-constant-process-model-random-time-series}{%
\subsection{Uncorrelated data, constant process model: random time series}\label{uncorrelated-data-constant-process-model-random-time-series}}

Random time series is the result of many influences that act independently to yield nonsystematic and non-repeating patterns about some average value.

A purely random series has a constant mean and no systematic patterns. Simply averaging the models is often the best and the most accurate way to forecast them.

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-Figure02} \end{center}

A special uncorrelated process: \textbf{white noise process}, \(x_t \to_{i.i.d} N(0, \sigma^2)\). That is, a sequence \(\{x_t\}\), (\(i = 1, 2, \cdots,\)) is a white noise process if each value in the sequence has a) zero-mean; b) constant conditional variance; and c) is uncorrelated with all other realizations.

\hypertarget{autocorrelated-data}{%
\subsection{Autocorrelated data}\label{autocorrelated-data}}

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-Figure03} \end{center}

The correlation measures the degree of dependence or association between two variables.

The term auto-correlation means that the value of a series in one time period is related to the value itself in previous periods.

\hypertarget{trend-data}{%
\subsection{Trend Data}\label{trend-data}}

A trend is a general increase or decrease in a time series that lasts for approximately seven or more periods. it can be a period-to-period increase or decrease that follows a straight line - this kind of pattern is called a linear trend.

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-Figure04} \end{center}

Trends are not necessarily linear because there are a large number of nonlinear causal influences that yield nonlinear series.

\hypertarget{seasonal-data}{%
\subsection{Seasonal Data}\label{seasonal-data}}

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-Figure05} \end{center}

The seasonal series result from events that are periodic and recurrent (e.g., monthly, quarterly, changes recurring each year). The common seasonal influences are climate, human habits, holidays, repeating promotions, and so on.

\hypertarget{nonstationary-data}{%
\subsection{Nonstationary Data}\label{nonstationary-data}}

A stationary time series is one whose properties do not depend on the time at which the series is observed.

More formally, a strictly stationary stochastic process is one where given \(t_1, \cdots, t_{\mathcal{l}}\) the joint statistical distribution of \(X_{t_1} , \cdots ,X_{t_{\mathcal{l}}}\) is NOT the same as the joint statistical distribution of \(X_{t_1 + \tau} , . . . ,X_{t_{\mathcal{l}}+\tau}\) for all \(\mathcal{l}\) and \(\tau\).

Thus, time series with trends, or with seasonality, are not stationary --- the trend and seasonality will affect the value of the time series at different times.

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-Figure06} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-Figure07} \end{center}

However, a time series with cyclic behavior (but with no trend or seasonality) is stationary.

\hypertarget{atypical-event-outliers}{%
\subsection{Atypical Event (Outliers)}\label{atypical-event-outliers}}

The analysis of past data can be made very complex when the included values are not typical of the past or future.

These non-typical values are called outliers, which are very large or small observations that are not indicative of repeating past or future patterns. Outliers occur because of unusual events.

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-Figure08} \end{center}

\hypertarget{some-technical-terms-and-baseline-forecasting-models}{%
\section{Some Technical Terms and Baseline Forecasting Models}\label{some-technical-terms-and-baseline-forecasting-models}}

There are many forecasting models for different time series models. Some of them are extremely simple and surprisingly effective. In this module, we introduce some naive methods as baseline methods to understand some basic concepts and technical terms before introducing formal forecasting models.

\hypertarget{some-definitions}{%
\subsection{Some Definitions}\label{some-definitions}}

Notation of a time series data: suppose there are \(T\) periods of data available with \(T\) the most recent period. The observed data values will be denoted by \(y_t\) for \(t = 1, 2, \cdots, T\).

\textbf{Forecast v.s. fitted values}: in time series analysis, we distinguish between forecast (predicted) values from the fitted value:

\begin{itemize}
\item
  \textbf{Forecast value}: the value was produced at some previous period, say, \(t - \tau\) (the predictive model on which the forecasting will be made should be built using the data up to \(t - \tau\)).
\item
  \textbf{Fitted value}: The values produced by the fitted model at historical time points.
\end{itemize}

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-ConceptsDefinition} \end{center}

\textbf{Seasonal/Cycle Frequency} is used to define time series objects for forecasting models in R library \textbf{\{forecast\}}. In the R function \textbf{ts()}, the argument \textbf{frequency} needs to be specified.

The ``\textbf{frequency}'' is the number of observations \textbf{\color{red}per seasonal cycle}. When using the ts() function in R, the following choices should be used.

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
Data & frequency \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Annual & 1 \\
Quarterly & 4 \\
Monthly & 12 \\
Weekly & 52 \\
\end{longtable}

Actually, there are not 52 weeks in a year, but 365.25/7 = 52.18 on average. But most functions that use \textbf{ts} objects require integer frequency.

Once the frequency of observations is smaller than a week, then there is usually more than one way of handling the frequency. For example, data \emph{\color{red}observed every minute} might have an \textbf{hourly seasonality} (frequency=60), a \textbf{daily seasonality} (frequency=24x60=1440), a \textbf{weekly seasonality} (frequency=24x60x7=10080), and an \textbf{annual seasonality} (frequency=24x60x365.25=525960). If we want to use a \textbf{ts} object, then we need to decide which of these is the most important.

\hypertarget{baseline-methods}{%
\subsection{Baseline Methods}\label{baseline-methods}}

\textbf{Working Data Set}: The Nile dataset was selected to expand knowledge of time series analysis. The Nile dataset contains 100 annual readings of the Nile River at Aswan from the years 1871 to 1970 (Anonymous, n.d.-a). Time series analysis and modeling are concerned with data elements that are ordered in a temporal fashion. The Nile data set is analyzed using various time series approaches and the results are discussed.

\begin{verbatim}
1120, 1160, 963, 1210, 1160, 1160, 813, 1230, 1370, 1140, 995, 935, 1110, 994, 1020, 960,
1180, 799, 958, 1140, 1100, 1210, 1150, 1250, 1260, 1220, 1030, 1100, 774, 840, 874, 694,
940, 833, 701, 916, 692, 1020, 1050, 969, 831, 726, 456, 824, 702, 1120, 1100, 832, 764, 
821, 768, 845, 864, 862, 698, 845, 744, 796, 1040, 759, 781, 865, 845, 944, 984, 897, 822,
1010, 771, 676, 649, 846, 812, 742, 801, 1040, 860, 874, 848, 890, 744, 749, 838, 1050, 
918, 986, 797, 923, 975, 815, 1020, 906, 901, 1170, 912, 746, 919, 718, 714, 740
\end{verbatim}

When fitting time series models in R, most of the R functions for the time series require a time series object which has a special structure and other features. For a given numeric vector, we can use the R function \textbf{ts()} in the library \textbf{forecast}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nile.vec }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1120}\NormalTok{, }\DecValTok{1160}\NormalTok{, }\DecValTok{963}\NormalTok{, }\DecValTok{1210}\NormalTok{, }\DecValTok{1160}\NormalTok{, }\DecValTok{1160}\NormalTok{, }\DecValTok{813}\NormalTok{, }\DecValTok{1230}\NormalTok{, }\DecValTok{1370}\NormalTok{, }\DecValTok{1140}\NormalTok{, }\DecValTok{995}\NormalTok{, }\DecValTok{935}\NormalTok{, }\DecValTok{1110}\NormalTok{, }\DecValTok{994}\NormalTok{,}
\DecValTok{1020}\NormalTok{, }\DecValTok{960}\NormalTok{, }\DecValTok{1180}\NormalTok{, }\DecValTok{799}\NormalTok{, }\DecValTok{958}\NormalTok{, }\DecValTok{1140}\NormalTok{, }\DecValTok{1100}\NormalTok{, }\DecValTok{1210}\NormalTok{, }\DecValTok{1150}\NormalTok{, }\DecValTok{1250}\NormalTok{, }\DecValTok{1260}\NormalTok{, }\DecValTok{1220}\NormalTok{, }\DecValTok{1030}\NormalTok{, }\DecValTok{1100}\NormalTok{, }\DecValTok{774}\NormalTok{, }\DecValTok{840}\NormalTok{,}
\DecValTok{874}\NormalTok{, }\DecValTok{694}\NormalTok{, }\DecValTok{940}\NormalTok{, }\DecValTok{833}\NormalTok{, }\DecValTok{701}\NormalTok{, }\DecValTok{916}\NormalTok{, }\DecValTok{692}\NormalTok{, }\DecValTok{1020}\NormalTok{, }\DecValTok{1050}\NormalTok{, }\DecValTok{969}\NormalTok{, }\DecValTok{831}\NormalTok{, }\DecValTok{726}\NormalTok{, }\DecValTok{456}\NormalTok{, }\DecValTok{824}\NormalTok{, }\DecValTok{702}\NormalTok{, }\DecValTok{1120}\NormalTok{, }\DecValTok{1100}\NormalTok{, }\DecValTok{832}\NormalTok{,}
\DecValTok{764}\NormalTok{, }\DecValTok{821}\NormalTok{, }\DecValTok{768}\NormalTok{, }\DecValTok{845}\NormalTok{, }\DecValTok{864}\NormalTok{, }\DecValTok{862}\NormalTok{, }\DecValTok{698}\NormalTok{, }\DecValTok{845}\NormalTok{, }\DecValTok{744}\NormalTok{, }\DecValTok{796}\NormalTok{, }\DecValTok{1040}\NormalTok{, }\DecValTok{759}\NormalTok{, }\DecValTok{781}\NormalTok{, }\DecValTok{865}\NormalTok{, }\DecValTok{845}\NormalTok{, }\DecValTok{944}\NormalTok{, }\DecValTok{984}\NormalTok{, }\DecValTok{897}\NormalTok{,}
\DecValTok{822}\NormalTok{, }\DecValTok{1010}\NormalTok{, }\DecValTok{771}\NormalTok{, }\DecValTok{676}\NormalTok{, }\DecValTok{649}\NormalTok{, }\DecValTok{846}\NormalTok{, }\DecValTok{812}\NormalTok{, }\DecValTok{742}\NormalTok{, }\DecValTok{801}\NormalTok{, }\DecValTok{1040}\NormalTok{, }\DecValTok{860}\NormalTok{, }\DecValTok{874}\NormalTok{, }\DecValTok{848}\NormalTok{, }\DecValTok{890}\NormalTok{, }\DecValTok{744}\NormalTok{, }\DecValTok{749}\NormalTok{, }\DecValTok{838}\NormalTok{, }\DecValTok{1050}\NormalTok{,}
\DecValTok{918}\NormalTok{, }\DecValTok{986}\NormalTok{, }\DecValTok{797}\NormalTok{, }\DecValTok{923}\NormalTok{, }\DecValTok{975}\NormalTok{, }\DecValTok{815}\NormalTok{, }\DecValTok{1020}\NormalTok{, }\DecValTok{906}\NormalTok{, }\DecValTok{901}\NormalTok{, }\DecValTok{1170}\NormalTok{, }\DecValTok{912}\NormalTok{, }\DecValTok{746}\NormalTok{, }\DecValTok{919}\NormalTok{, }\DecValTok{718}\NormalTok{, }\DecValTok{714}\NormalTok{, }\DecValTok{740}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{nile.ts }\OtherTok{=} \FunctionTok{ts}\NormalTok{(nile.vec, }
             \AttributeTok{start =} \DecValTok{1871}\NormalTok{, }
             \AttributeTok{end =} \DecValTok{1970}\NormalTok{, }
             \AttributeTok{frequency =}\DecValTok{1}  \CommentTok{\# one observation per year. }
                           \CommentTok{\# if there is a weekly seasonal pattern,}
                           \CommentTok{\# then the frequency = 52}
\NormalTok{             )}
\NormalTok{ nile.ts}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time Series:
## Start = 1871 
## End = 1970 
## Frequency = 1 
##   [1] 1120 1160  963 1210 1160 1160  813 1230 1370 1140  995  935 1110  994 1020
##  [16]  960 1180  799  958 1140 1100 1210 1150 1250 1260 1220 1030 1100  774  840
##  [31]  874  694  940  833  701  916  692 1020 1050  969  831  726  456  824  702
##  [46] 1120 1100  832  764  821  768  845  864  862  698  845  744  796 1040  759
##  [61]  781  865  845  944  984  897  822 1010  771  676  649  846  812  742  801
##  [76] 1040  860  874  848  890  744  749  838 1050  918  986  797  923  975  815
##  [91] 1020  906  901 1170  912  746  919  718  714  740
\end{verbatim}

This data set is included in library\{forecast\} as a time series object. We can view the data values in this time series by typing \emph{Nile} to view the time series object.

\textbf{Moving Average Method}

The moving average method forecasts all future values to be equal to the average (or ``mean'') of the historical data. If we let the historical data be denoted by \(\{y_1, y_2, \cdots, y_T \}\), then we can write the forecasts as

\[
\hat{y}_{T+h|T} = (y_1+y_2+\cdots+y_T)/T
\]

\(\hat{y}_{T+h|T}\) is the estimated value of \(y_{T+h|T}\) based on \(\{y_1, y_2, \cdots, y_T \}\).

\textbf{Example 1}. We only use the basic moving average on the Nile River data. The following plot shows the pattern of the time series.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nile.fcast }\OtherTok{\textless{}{-}} \FunctionTok{meanf}\NormalTok{(Nile, }\AttributeTok{h=}\DecValTok{10}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(nile.fcast)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-194-1.pdf}

The forecasted values for 10 future periods (h = 10 is called the forecast horizon) are given below

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(nile.fcast)}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Point Forecast    Lo 80   Hi 80    Lo 95    Hi 95
## 1971         919.35 699.9303 1138.77 581.8912 1256.809
## 1972         919.35 699.9303 1138.77 581.8912 1256.809
## 1973         919.35 699.9303 1138.77 581.8912 1256.809
## 1974         919.35 699.9303 1138.77 581.8912 1256.809
## 1975         919.35 699.9303 1138.77 581.8912 1256.809
## 1976         919.35 699.9303 1138.77 581.8912 1256.809
## 1977         919.35 699.9303 1138.77 581.8912 1256.809
## 1978         919.35 699.9303 1138.77 581.8912 1256.809
## 1979         919.35 699.9303 1138.77 581.8912 1256.809
## 1980         919.35 699.9303 1138.77 581.8912 1256.809
\end{verbatim}

\textbf{Naive method}

For naive forecasts, we simply set all forecasts to be the value of the last observation. That is,

\[
 \hat{y}_{T+h|T} = y_T
 \]

The naive forecast method works really well on the time series data generated from a process in which the next value in the sequence is a modification of the previous value in the sequence.

\textbf{Example 2}. We will use the naive forecast method on the Nile River data.

The forecasted values for the three future periods and the forecasted intervals are plotted in the following figure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{naive.nile }\OtherTok{=} \FunctionTok{naive}\NormalTok{(Nile, }\AttributeTok{h=}\DecValTok{3}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(naive.nile)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-196-1} \end{center}

The corresponding values and confidence intervals are given in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(naive.nile, }\AttributeTok{caption =} \StringTok{"Naive forecasting method on Nile River data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-197}Naive forecasting method on Nile River data}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
  & Point Forecast & Lo 80 & Hi 80 & Lo 95 & Hi 95\\
\hline
1971 & 740 & 525.5648 & 954.4352 & 412.0497 & 1067.950\\
\hline
1972 & 740 & 436.7429 & 1043.2571 & 276.2083 & 1203.792\\
\hline
1973 & 740 & 368.5874 & 1111.4126 & 171.9735 & 1308.027\\
\hline
\end{tabular}
\end{table}

\hypertarget{seasonal-naive-method}{%
\subsection{Seasonal Naive Method}\label{seasonal-naive-method}}

The values of seasonal time series data are influenced by seasonal factors (e.g., the quarter of the year, the month, or the day of the week). seasonal time series are also called periodic time series.

If a data set has a seasonal pattern such as weekly and monthly patterns, the \textbf{moving average} and \textbf{naive} methods work poorly. We need a method to capture the seasonal pattern. The \textbf{seasonal naive method} is modified from the \textbf{naive method} and has the following explicit forecasting function

\[
\hat{y}_{T+h|T} = y_{T+h-m(k+1)}
\]

where \(m =\) the seasonal period and \(k\) is the integer part of \((h-1)/m\) (i.e., the number of complete years in the forecast period before time \(T + h\)). The forecasting formula looks complex, but it essentially says that, for example, with monthly data, the seasonal naive method forecasts for all future February values are equal to the last observed February value.

\textbf{Example 3} Nottingham monthly average temperature - the following data set is a collection of average monthly temperatures in Nottingham from 1920-1939. Since this is monthly data, when we define the time series object with \textbf{ts()}, the option \textbf{frequency} should be set to 12.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nottem }\OtherTok{=}\FunctionTok{c}\NormalTok{(}\FloatTok{40.6}\NormalTok{,}\FloatTok{40.8}\NormalTok{,}\FloatTok{44.4}\NormalTok{,}\FloatTok{46.7}\NormalTok{,}\FloatTok{54.1}\NormalTok{,}\FloatTok{58.5}\NormalTok{,}\FloatTok{57.7}\NormalTok{,}\FloatTok{56.4}\NormalTok{,}\FloatTok{54.3}\NormalTok{,}\FloatTok{50.5}\NormalTok{,}\FloatTok{42.9}\NormalTok{,}\FloatTok{39.8}\NormalTok{,}\FloatTok{44.2}\NormalTok{,}\FloatTok{39.8}\NormalTok{,}\FloatTok{45.1}\NormalTok{,}\FloatTok{47.0}\NormalTok{,}
          \FloatTok{54.1}\NormalTok{,}\FloatTok{58.7}\NormalTok{,}\FloatTok{66.3}\NormalTok{,}\FloatTok{59.9}\NormalTok{,}\FloatTok{57.0}\NormalTok{,}\FloatTok{54.2}\NormalTok{,}\FloatTok{39.7}\NormalTok{,}\FloatTok{42.8}\NormalTok{,}\FloatTok{37.5}\NormalTok{,}\FloatTok{38.7}\NormalTok{,}\FloatTok{39.5}\NormalTok{,}\FloatTok{42.1}\NormalTok{,}\FloatTok{55.7}\NormalTok{,}\FloatTok{57.8}\NormalTok{,}\FloatTok{56.8}\NormalTok{,}\FloatTok{54.3}\NormalTok{,}
          \FloatTok{54.3}\NormalTok{,}\FloatTok{47.1}\NormalTok{,}\FloatTok{41.8}\NormalTok{,}\FloatTok{41.7}\NormalTok{,}\FloatTok{41.8}\NormalTok{,}\FloatTok{40.1}\NormalTok{,}\FloatTok{42.9}\NormalTok{,}\FloatTok{45.8}\NormalTok{,}\FloatTok{49.2}\NormalTok{,}\FloatTok{52.7}\NormalTok{,}\FloatTok{64.2}\NormalTok{,}\FloatTok{59.6}\NormalTok{,}\FloatTok{54.4}\NormalTok{,}\FloatTok{49.2}\NormalTok{,}\FloatTok{36.3}\NormalTok{,}\FloatTok{37.6}\NormalTok{,}
          \FloatTok{39.3}\NormalTok{,}\FloatTok{37.5}\NormalTok{,}\FloatTok{38.3}\NormalTok{,}\FloatTok{45.5}\NormalTok{,}\FloatTok{53.2}\NormalTok{,}\FloatTok{57.7}\NormalTok{,}\FloatTok{60.8}\NormalTok{,}\FloatTok{58.2}\NormalTok{,}\FloatTok{56.4}\NormalTok{,}\FloatTok{49.8}\NormalTok{,}\FloatTok{44.4}\NormalTok{,}\FloatTok{43.6}\NormalTok{,}\FloatTok{40.0}\NormalTok{,}\FloatTok{40.5}\NormalTok{,}\FloatTok{40.8}\NormalTok{,}\FloatTok{45.1}\NormalTok{,}
          \FloatTok{53.8}\NormalTok{,}\FloatTok{59.4}\NormalTok{,}\FloatTok{63.5}\NormalTok{,}\FloatTok{61.0}\NormalTok{,}\FloatTok{53.0}\NormalTok{,}\FloatTok{50.0}\NormalTok{,}\FloatTok{38.1}\NormalTok{,}\FloatTok{36.3}\NormalTok{,}\FloatTok{39.2}\NormalTok{,}\FloatTok{43.4}\NormalTok{,}\FloatTok{43.4}\NormalTok{,}\FloatTok{48.9}\NormalTok{,}\FloatTok{50.6}\NormalTok{,}\FloatTok{56.8}\NormalTok{,}\FloatTok{62.5}\NormalTok{,}\FloatTok{62.0}\NormalTok{,}
          \FloatTok{57.5}\NormalTok{,}\FloatTok{46.7}\NormalTok{,}\FloatTok{41.6}\NormalTok{,}\FloatTok{39.8}\NormalTok{,}\FloatTok{39.4}\NormalTok{,}\FloatTok{38.5}\NormalTok{,}\FloatTok{45.3}\NormalTok{,}\FloatTok{47.1}\NormalTok{,}\FloatTok{51.7}\NormalTok{,}\FloatTok{55.0}\NormalTok{,}\FloatTok{60.4}\NormalTok{,}\FloatTok{60.5}\NormalTok{,}\FloatTok{54.7}\NormalTok{,}\FloatTok{50.3}\NormalTok{,}\FloatTok{42.3}\NormalTok{,}\FloatTok{35.2}\NormalTok{,}
          \FloatTok{40.8}\NormalTok{,}\FloatTok{41.1}\NormalTok{,}\FloatTok{42.8}\NormalTok{,}\FloatTok{47.3}\NormalTok{,}\FloatTok{50.9}\NormalTok{,}\FloatTok{56.4}\NormalTok{,}\FloatTok{62.2}\NormalTok{,}\FloatTok{60.5}\NormalTok{,}\FloatTok{55.4}\NormalTok{,}\FloatTok{50.2}\NormalTok{,}\FloatTok{43.0}\NormalTok{,}\FloatTok{37.3}\NormalTok{,}\FloatTok{34.8}\NormalTok{,}\FloatTok{31.3}\NormalTok{,}\FloatTok{41.0}\NormalTok{,}\FloatTok{43.9}\NormalTok{,}
          \FloatTok{53.1}\NormalTok{,}\FloatTok{56.9}\NormalTok{,}\FloatTok{62.5}\NormalTok{,}\FloatTok{60.3}\NormalTok{,}\FloatTok{59.8}\NormalTok{,}\FloatTok{49.2}\NormalTok{,}\FloatTok{42.9}\NormalTok{,}\FloatTok{41.9}\NormalTok{,}\FloatTok{41.6}\NormalTok{,}\FloatTok{37.1}\NormalTok{,}\FloatTok{41.2}\NormalTok{,}\FloatTok{46.9}\NormalTok{,}\FloatTok{51.2}\NormalTok{,}\FloatTok{60.4}\NormalTok{,}\FloatTok{60.1}\NormalTok{,}\FloatTok{61.6}\NormalTok{,}
          \FloatTok{57.0}\NormalTok{,}\FloatTok{50.9}\NormalTok{,}\FloatTok{43.0}\NormalTok{,}\FloatTok{38.8}\NormalTok{,}\FloatTok{37.1}\NormalTok{,}\FloatTok{38.4}\NormalTok{,}\FloatTok{38.4}\NormalTok{,}\FloatTok{46.5}\NormalTok{,}\FloatTok{53.5}\NormalTok{,}\FloatTok{58.4}\NormalTok{,}\FloatTok{60.6}\NormalTok{,}\FloatTok{58.2}\NormalTok{,}\FloatTok{53.8}\NormalTok{,}\FloatTok{46.6}\NormalTok{,}\FloatTok{45.5}\NormalTok{,}\FloatTok{40.6}\NormalTok{,}
          \FloatTok{42.4}\NormalTok{,}\FloatTok{38.4}\NormalTok{,}\FloatTok{40.3}\NormalTok{,}\FloatTok{44.6}\NormalTok{,}\FloatTok{50.9}\NormalTok{,}\FloatTok{57.0}\NormalTok{,}\FloatTok{62.1}\NormalTok{,}\FloatTok{63.5}\NormalTok{,}\FloatTok{56.3}\NormalTok{,}\FloatTok{47.3}\NormalTok{,}\FloatTok{43.6}\NormalTok{,}\FloatTok{41.8}\NormalTok{,}\FloatTok{36.2}\NormalTok{,}\FloatTok{39.3}\NormalTok{,}\FloatTok{44.5}\NormalTok{,}\FloatTok{48.7}\NormalTok{,}
          \FloatTok{54.2}\NormalTok{,}\FloatTok{60.8}\NormalTok{,}\FloatTok{65.5}\NormalTok{,}\FloatTok{64.9}\NormalTok{,}\FloatTok{60.1}\NormalTok{,}\FloatTok{50.2}\NormalTok{,}\FloatTok{42.1}\NormalTok{,}\FloatTok{35.8}\NormalTok{,}\FloatTok{39.4}\NormalTok{,}\FloatTok{38.2}\NormalTok{,}\FloatTok{40.4}\NormalTok{,}\FloatTok{46.9}\NormalTok{,}\FloatTok{53.4}\NormalTok{,}\FloatTok{59.6}\NormalTok{,}\FloatTok{66.5}\NormalTok{,}\FloatTok{60.4}\NormalTok{,}
          \FloatTok{59.2}\NormalTok{,}\FloatTok{51.2}\NormalTok{,}\FloatTok{42.8}\NormalTok{,}\FloatTok{45.8}\NormalTok{,}\FloatTok{40.0}\NormalTok{,}\FloatTok{42.6}\NormalTok{,}\FloatTok{43.5}\NormalTok{,}\FloatTok{47.1}\NormalTok{,}\FloatTok{50.0}\NormalTok{,}\FloatTok{60.5}\NormalTok{,}\FloatTok{64.6}\NormalTok{,}\FloatTok{64.0}\NormalTok{,}\FloatTok{56.8}\NormalTok{,}\FloatTok{48.6}\NormalTok{,}\FloatTok{44.2}\NormalTok{,}\FloatTok{36.4}\NormalTok{,}
          \FloatTok{37.3}\NormalTok{,}\FloatTok{35.0}\NormalTok{,}\FloatTok{44.0}\NormalTok{,}\FloatTok{43.9}\NormalTok{,}\FloatTok{52.7}\NormalTok{,}\FloatTok{58.6}\NormalTok{,}\FloatTok{60.0}\NormalTok{,}\FloatTok{61.1}\NormalTok{,}\FloatTok{58.1}\NormalTok{,}\FloatTok{49.6}\NormalTok{,}\FloatTok{41.6}\NormalTok{,}\FloatTok{41.3}\NormalTok{,}\FloatTok{40.8}\NormalTok{,}\FloatTok{41.0}\NormalTok{,}\FloatTok{38.4}\NormalTok{,}\FloatTok{47.4}\NormalTok{,}
          \FloatTok{54.1}\NormalTok{,}\FloatTok{58.6}\NormalTok{,}\FloatTok{61.4}\NormalTok{,}\FloatTok{61.8}\NormalTok{,}\FloatTok{56.3}\NormalTok{,}\FloatTok{50.9}\NormalTok{,}\FloatTok{41.4}\NormalTok{,}\FloatTok{37.1}\NormalTok{,}\FloatTok{42.1}\NormalTok{,}\FloatTok{41.2}\NormalTok{,}\FloatTok{47.3}\NormalTok{,}\FloatTok{46.6}\NormalTok{,}\FloatTok{52.4}\NormalTok{,}\FloatTok{59.0}\NormalTok{,}\FloatTok{59.6}\NormalTok{,}\FloatTok{60.4}\NormalTok{,}
          \FloatTok{57.0}\NormalTok{,}\FloatTok{50.7}\NormalTok{,}\FloatTok{47.8}\NormalTok{,}\FloatTok{39.2}\NormalTok{,}\FloatTok{39.4}\NormalTok{,}\FloatTok{40.9}\NormalTok{,}\FloatTok{42.4}\NormalTok{,}\FloatTok{47.8}\NormalTok{,}\FloatTok{52.4}\NormalTok{,}\FloatTok{58.0}\NormalTok{,}\FloatTok{60.7}\NormalTok{,}\FloatTok{61.8}\NormalTok{,}\FloatTok{58.2}\NormalTok{,}\FloatTok{46.7}\NormalTok{,}\FloatTok{46.6}\NormalTok{,}\FloatTok{37.8}\NormalTok{)}
\NormalTok{nottem.ts }\OtherTok{=} \FunctionTok{ts}\NormalTok{(nottem, }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{1920}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Next, we plot the original series and the forecasted future values as well. Assuming forecasting horizon h = 5.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seasonal.naive }\OtherTok{=} \FunctionTok{snaive}\NormalTok{(nottem, }\AttributeTok{h =} \DecValTok{5}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(seasonal.naive)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-199-1.pdf}

The actual forecasted temperatures in the next 5 months and their predictive intervals are given by

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(seasonal.naive, }\AttributeTok{caption=}\StringTok{"Forecasted monthly average temperatures of Nottingham between Jan {-} May 1940"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-200}Forecasted monthly average temperatures of Nottingham between Jan - May 1940}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
  & Point Forecast & Lo 80 & Hi 80 & Lo 95 & Hi 95\\
\hline
241 & 37.8 & 31.08754 & 44.51246 & 27.53418 & 48.06582\\
\hline
242 & 37.8 & 28.30715 & 47.29285 & 23.28193 & 52.31807\\
\hline
243 & 37.8 & 26.17368 & 49.42632 & 20.01907 & 55.58093\\
\hline
244 & 37.8 & 24.37508 & 51.22492 & 17.26835 & 58.33165\\
\hline
245 & 37.8 & 22.79048 & 52.80952 & 14.84492 & 60.75508\\
\hline
\end{tabular}
\end{table}

\hypertarget{drift-method}{%
\subsection{Drift Method}\label{drift-method}}

The seasonal naive method allows non-constant predictive values by taking the same values from the time period in the previous season. The drift method is another variation of the naive method that allows the forecasts to increase or decrease over time. To be more specific, the next one-step forecast value is set to be the average change seen in the historical data. Thus the forecast for time \(T + h\) is given by \[\hat{y}_{T+h|T} = y_T +h(y_T-y_1)/(T-1)\]

where the amount of change over time \(y_T-y_1\) is called the drift. This is where the name of the method comes from.

\textbf{Example 4} New York City Monthly Birth Counts - the dataset contains the number of births per month in New York City, from January 1946 to December 1958. The data set can be downloaded from \url{https://raw.githubusercontent.com/pengdsci/sta321/main/datasets/w11-nycbirths.txt}. We will use the seasonal naive method to forecast future values using function \textbf{naive()} in library\{forecast\}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nycbirth }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/datasets/w11{-}nycbirths.txt"}\NormalTok{)}
\NormalTok{births }\OtherTok{\textless{}{-}} \FunctionTok{ts}\NormalTok{(nycbirth, }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{1946}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{drift.pred }\OtherTok{=} \FunctionTok{rwf}\NormalTok{(births, }\AttributeTok{h=} \DecValTok{5}\NormalTok{, }\AttributeTok{drift =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(drift.pred)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-201-1.pdf}

As usual, we can extract the predicted value from the above drift forecast procedure in the following

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(drift.pred, }\AttributeTok{caption =} \StringTok{"Forecast birth counts using the drift method"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-202}Forecast birth counts using the drift method}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
  & Point Forecast & Lo 80 & Hi 80 & Lo 95 & Hi 95\\
\hline
Jan 1960 & 27.90439 & 25.96383 & 29.84495 & 24.93656 & 30.87222\\
\hline
Feb 1960 & 27.91178 & 25.15926 & 30.66430 & 23.70216 & 32.12139\\
\hline
Mar 1960 & 27.91917 & 24.53807 & 31.30026 & 22.74823 & 33.09010\\
\hline
Apr 1960 & 27.92656 & 24.01094 & 31.84217 & 21.93814 & 33.91498\\
\hline
May 1960 & 27.93395 & 23.54337 & 32.32452 & 21.21914 & 34.64875\\
\hline
\end{tabular}
\end{table}

\hypertarget{accuracy-measures-in-time-series-forecasting}{%
\section{Accuracy Measures in Time Series Forecasting}\label{accuracy-measures-in-time-series-forecasting}}

\hypertarget{training-and-testing-data}{%
\subsection{Training and Testing Data}\label{training-and-testing-data}}

To evaluate forecast accuracy using genuine forecasts, we separate the available data into two portions, training and test data, where the training data is used to estimate any parameters of a forecasting method, and the test data is used to evaluate its accuracy. We have used this logic in logistic predictive modeling. The difference is that we cannot use the random split method as we did in the predictive modeling using logistic regression.

We have introduced several baseline forecasting methods in the previous section.

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-TrainingTesting} \end{center}

\hypertarget{forecasting-errors}{%
\subsection{Forecasting Errors}\label{forecasting-errors}}

In a given time series data with n observations, we hold up k observations for evaluating forecast errors and use the rest of the \(n-k\) values to build time series models. We set the forecast horizon h = k. We can find the difference between the predicted and actual hold-up values to define various error metrics in the following table.

\begin{center}\includegraphics[width=0.8\linewidth]{img11/w11-ErrorMetrics} \end{center}

The error term \(\hat{e}_i = \hat{y}_i - y_i\). We will use the above error measures to define the accuracy metrics in a case study.

\hypertarget{case-study-1}{%
\section{Case Study}\label{case-study-1}}

\textbf{Daily Female Births Dataset}: This dataset describes the number of daily female births in California in 1959. The units are a count and there are 365 observations

\textbf{A. Training and testing data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{female.births }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/datasets/w11{-}daily{-}total{-}female{-}births.csv"}\NormalTok{)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\DocumentationTok{\#\# training and testing data: hold{-}up last 6 months of data for calculating forecasting errors}
\NormalTok{training }\OtherTok{=}\NormalTok{ female.births[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{350}\NormalTok{]}
\NormalTok{testing }\OtherTok{=}\NormalTok{ female.births[}\DecValTok{351}\SpecialCharTok{:}\DecValTok{365}\NormalTok{]}
\DocumentationTok{\#\#}
\NormalTok{female.births.ts }\OtherTok{=} \FunctionTok{ts}\NormalTok{(training, }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{1959}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\DocumentationTok{\#\# shampoo.ts}
\end{Highlighting}
\end{Shaded}

\textbf{B. Building 4 Forecasting Models}

We next use the four baseline forecasting methods and the first 30 data values to forecast the next 6 month's data values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.mv }\OtherTok{=} \FunctionTok{meanf}\NormalTok{(female.births.ts, }\AttributeTok{h=}\DecValTok{15}\NormalTok{)}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{pred.naive }\OtherTok{=} \FunctionTok{naive}\NormalTok{(female.births.ts, }\AttributeTok{h=}\DecValTok{15}\NormalTok{)}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{pred.snaive }\OtherTok{=} \FunctionTok{snaive}\NormalTok{(female.births.ts, }\AttributeTok{h=}\DecValTok{15}\NormalTok{)}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{pred.rwf }\OtherTok{=} \FunctionTok{rwf}\NormalTok{(female.births.ts, }\AttributeTok{h=}\DecValTok{15}\NormalTok{, }\AttributeTok{drift =} \ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{$}\NormalTok{mean}
\DocumentationTok{\#\#\#}
\DocumentationTok{\#\#\#}
\NormalTok{pred.table }\OtherTok{=} \FunctionTok{cbind}\NormalTok{( }\AttributeTok{pred.mv =}\NormalTok{ pred.mv,}
                    \AttributeTok{pred.naive =}\NormalTok{ pred.naive,}
                    \AttributeTok{pred.snaive =}\NormalTok{ pred.snaive,}
                    \AttributeTok{pred.rwf =}\NormalTok{ pred.rwf)}
\FunctionTok{kable}\NormalTok{(pred.table, }\AttributeTok{caption =} \StringTok{"Forecasting Table"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-206}Forecasting Table}
\centering
\begin{tabular}[t]{r|r|r|r}
\hline
pred.mv & pred.naive & pred.snaive & pred.rwf\\
\hline
41.91429 & 52 & 34 & 52.04871\\
\hline
41.91429 & 52 & 33 & 52.09742\\
\hline
41.91429 & 52 & 36 & 52.14613\\
\hline
41.91429 & 52 & 49 & 52.19484\\
\hline
41.91429 & 52 & 43 & 52.24355\\
\hline
41.91429 & 52 & 43 & 52.29226\\
\hline
41.91429 & 52 & 34 & 52.34097\\
\hline
41.91429 & 52 & 39 & 52.38968\\
\hline
41.91429 & 52 & 35 & 52.43840\\
\hline
41.91429 & 52 & 52 & 52.48711\\
\hline
41.91429 & 52 & 47 & 52.53582\\
\hline
41.91429 & 52 & 52 & 52.58453\\
\hline
41.91429 & 52 & 34 & 52.63324\\
\hline
41.91429 & 52 & 33 & 52.68195\\
\hline
41.91429 & 52 & 36 & 52.73066\\
\hline
\end{tabular}
\end{table}

\textbf{C. Visualization}

We now make a time series plot and the predicted values. Note that, the forecast values were based on the model that uses 350 historical data in the time series. The following only show observations \#320 -\#365 and the 15 forecasted values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\DecValTok{340}\SpecialCharTok{:}\DecValTok{365}\NormalTok{, female.births[}\DecValTok{340}\SpecialCharTok{:}\DecValTok{365}\NormalTok{], }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{340}\NormalTok{,}\DecValTok{365}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{90}\NormalTok{),}
     \AttributeTok{xlab =} \StringTok{"observation sequence"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Female birth counts"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Monthly female counts and forecasting"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{340}\SpecialCharTok{:}\DecValTok{365}\NormalTok{, female.births[}\DecValTok{340}\SpecialCharTok{:}\DecValTok{365}\NormalTok{],}\AttributeTok{pch=}\DecValTok{20}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{points}\NormalTok{(}\DecValTok{351}\SpecialCharTok{:}\DecValTok{365}\NormalTok{, pred.mv, }\AttributeTok{pch=}\DecValTok{15}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{351}\SpecialCharTok{:}\DecValTok{365}\NormalTok{, pred.naive, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{351}\SpecialCharTok{:}\DecValTok{365}\NormalTok{, pred.rwf, }\AttributeTok{pch=}\DecValTok{18}\NormalTok{, }\AttributeTok{col =} \StringTok{"navy"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{351}\SpecialCharTok{:}\DecValTok{365}\NormalTok{, pred.snaive, }\AttributeTok{pch=}\DecValTok{17}\NormalTok{, }\AttributeTok{col =} \StringTok{"purple"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{lines}\NormalTok{(}\DecValTok{351}\SpecialCharTok{:}\DecValTok{365}\NormalTok{, pred.mv, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{351}\SpecialCharTok{:}\DecValTok{365}\NormalTok{, pred.snaive, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"purple"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{351}\SpecialCharTok{:}\DecValTok{365}\NormalTok{, pred.naive, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{351}\SpecialCharTok{:}\DecValTok{365}\NormalTok{, pred.rwf, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"navy"}\NormalTok{)}
\DocumentationTok{\#\# }
\FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"moving avergae"}\NormalTok{, }\StringTok{"naive"}\NormalTok{, }\StringTok{"drift"}\NormalTok{, }\StringTok{"seasonal naive"}\NormalTok{),}
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"navy"}\NormalTok{, }\StringTok{"purple"}\NormalTok{), }\AttributeTok{pch=}\DecValTok{15}\SpecialCharTok{:}\DecValTok{18}\NormalTok{, }\AttributeTok{lty=}\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{),}
       \AttributeTok{bty=}\StringTok{"n"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-207-1.pdf}

We can see that the \textbf{moving average} method worked pretty well. The performance of naive and drift methods in this seasonal time series are close to each other. But \textbf{naive}, \textbf{seasonal naive}, and \textbf{drift} methods worked poorly compared to the \textbf{moving average} method.

Intuitively, \textbf{moving average} should produce non-constant forecast values. What

\textbf{D. Accuracy Metrics}

We will use the mean absolute prediction error (MAPE) to compare the performance of the four forecasting methods.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{true.value }\OtherTok{=}\NormalTok{ female.births[}\DecValTok{351}\SpecialCharTok{:}\DecValTok{365}\NormalTok{]}
\NormalTok{PE.mv }\OtherTok{=}  \DecValTok{100}\SpecialCharTok{*}\NormalTok{(true.value }\SpecialCharTok{{-}}\NormalTok{ pred.mv)}\SpecialCharTok{/}\NormalTok{true.value}
\NormalTok{PE.naive }\OtherTok{=}  \DecValTok{100}\SpecialCharTok{*}\NormalTok{(true.value }\SpecialCharTok{{-}}\NormalTok{ pred.naive)}\SpecialCharTok{/}\NormalTok{true.value}
\NormalTok{PE.snaive }\OtherTok{=}  \DecValTok{100}\SpecialCharTok{*}\NormalTok{(true.value }\SpecialCharTok{{-}}\NormalTok{ pred.snaive)}\SpecialCharTok{/}\NormalTok{true.value}
\NormalTok{PE.rwf }\OtherTok{=}  \DecValTok{100}\SpecialCharTok{*}\NormalTok{(true.value }\SpecialCharTok{{-}}\NormalTok{ pred.rwf)}\SpecialCharTok{/}\NormalTok{true.value}
\DocumentationTok{\#\#}
\NormalTok{MAPE.mv }\OtherTok{=} \FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(PE.mv))}
\NormalTok{MAPE.naive }\OtherTok{=} \FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(PE.naive))}
\NormalTok{MAPE.snaive }\OtherTok{=} \FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(PE.snaive))}
\NormalTok{MAPE.rwf }\OtherTok{=} \FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(PE.rwf))}
\DocumentationTok{\#\#}
\NormalTok{MAPE }\OtherTok{=} \FunctionTok{c}\NormalTok{(MAPE.mv, MAPE.naive, MAPE.snaive, MAPE.rwf)}
\DocumentationTok{\#\# residual{-}based Error}
\NormalTok{e.mv }\OtherTok{=}\NormalTok{ true.value }\SpecialCharTok{{-}}\NormalTok{ pred.mv}
\NormalTok{e.naive }\OtherTok{=}\NormalTok{ true.value }\SpecialCharTok{{-}}\NormalTok{ pred.naive}
\NormalTok{e.snaive }\OtherTok{=}\NormalTok{ true.value }\SpecialCharTok{{-}}\NormalTok{ pred.snaive}
\NormalTok{e.rwf }\OtherTok{=}\NormalTok{ true.value }\SpecialCharTok{{-}}\NormalTok{ pred.rwf}
\DocumentationTok{\#\# MAD}
\NormalTok{MAD.mv }\OtherTok{=} \FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(e.mv))}
\NormalTok{MAD.naive }\OtherTok{=} \FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(e.naive))}
\NormalTok{MAD.snaive }\OtherTok{=} \FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(e.snaive))}
\NormalTok{MAD.rwf }\OtherTok{=} \FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(e.rwf))}
\NormalTok{MAD }\OtherTok{=} \FunctionTok{c}\NormalTok{(MAD.mv, MAD.naive, MAD.snaive, MAD.rwf)}
\DocumentationTok{\#\# MSE}
\NormalTok{MSE.mv }\OtherTok{=} \FunctionTok{mean}\NormalTok{((e.mv)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{MSE.naive }\OtherTok{=} \FunctionTok{mean}\NormalTok{((e.naive)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{MSE.snaive }\OtherTok{=} \FunctionTok{mean}\NormalTok{((e.snaive)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{MSE.rwf }\OtherTok{=} \FunctionTok{mean}\NormalTok{((e.rwf)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{MSE }\OtherTok{=} \FunctionTok{c}\NormalTok{(MSE.mv, MSE.naive, MSE.snaive, MSE.rwf)}
\DocumentationTok{\#\#}
\NormalTok{accuracy.table }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{MAPE =}\NormalTok{ MAPE, }\AttributeTok{MAD =}\NormalTok{ MAD, }\AttributeTok{MSE =}\NormalTok{ MSE)}
\FunctionTok{row.names}\NormalTok{(accuracy.table) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Moving Average"}\NormalTok{, }\StringTok{"Naive"}\NormalTok{, }\StringTok{"Seasonal Naive"}\NormalTok{, }\StringTok{"Drift"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(accuracy.table, }\AttributeTok{caption =}\StringTok{"Overall performance of the four forecasting methods"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-208}Overall performance of the four forecasting methods}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & MAPE & MAD & MSE\\
\hline
Moving Average & 11.29861 & 77.08571 & 41.93687\\
\hline
Naive & 22.83107 & 135.00000 & 111.00000\\
\hline
Seasonal Naive & 20.37455 & 133.00000 & 112.86667\\
\hline
Drift & 23.49040 & 138.99427 & 116.60627\\
\hline
\end{tabular}
\end{table}

In summary, the \textbf{moving average method} has the best performance. Note that the methods introduced in this module are baseline forecasting. They are all descriptive since we did not use any statistical assumptions. In the next module, we will introduce a few formal non-parametric forecasting methods - exponential forecasting methods. We will use the same accuracy measures to compare different forecasting methods.

\hfill\break

\hypertarget{analysis-assignment-5}{%
\section{Analysis Assignment}\label{analysis-assignment-5}}

Find a time series with at least 76 periods of values from the internet. Any type of time series (trend or non-trend, seasonal or non-seasonal, etc) will be fine for this assignment. The objective is to use the accuracy measures to compare the performance of the four baseline forecasting methods. To be more specific,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Make a time series plot based on the data.
\item
  Split the time series into training and testing sets. Please hold up the most recent 10 periods to calculate the forecasting error.
\item
  Define a time series object using the R function \textbf{ts()} using the training data.
\item
  Build the four baseline forecasting time series models.
\item
  Create a graph to visualize the forecasted values.
\item
  Calculate the accuracy measures for the four forecasting models and summarize the results.
\end{enumerate}

\hypertarget{time-series-decomposition}{%
\chapter{Time Series Decomposition}\label{time-series-decomposition}}

Time series decomposition is a process of splitting a time series into basic components: trend, seasonality random error. The method originated a century ago and new developments in the past few decades.

\textbf{Seasonal}: Patterns that repeat for a fixed period. For example, a website might receive more visits during weekends; this would produce data with seasonality of 7 days.

\textbf{Trend}: The underlying trend of the metrics. A website increasing in popularity should show a general trend that goes up.

\textbf{Random Error}: Also call ``noise'', ``residual'' or ``remainder''. These are the residuals of the original time series after the seasonal and trend series are removed.

The objective of time series decomposition is to model the trend and seasonality and estimate the overall time series as a combination of them. A seasonally adjusted value removes the seasonal effect from a value so that trends can be seen more clearly.

The following two working data sets were widely used in different textbooks. We will use them to illustrate some of the concepts.

\textbf{Australian Beer Production Data}

The following data gives quarterly beer production figures in Australia from 1956 through the 2nd quarter of 2010. The beer production figure is in megalitres.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ausbeer0}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{284}\NormalTok{, }\DecValTok{213}\NormalTok{, }\DecValTok{227}\NormalTok{, }\DecValTok{308}\NormalTok{, }\DecValTok{262}\NormalTok{, }\DecValTok{228}\NormalTok{, }\DecValTok{236}\NormalTok{, }\DecValTok{320}\NormalTok{, }\DecValTok{272}\NormalTok{, }\DecValTok{233}\NormalTok{, }\DecValTok{237}\NormalTok{, }\DecValTok{313}\NormalTok{, }\DecValTok{261}\NormalTok{, }\DecValTok{227}\NormalTok{, }\DecValTok{250}\NormalTok{, }\DecValTok{314}\NormalTok{, }
          \DecValTok{286}\NormalTok{, }\DecValTok{227}\NormalTok{, }\DecValTok{260}\NormalTok{, }\DecValTok{311}\NormalTok{, }\DecValTok{295}\NormalTok{, }\DecValTok{233}\NormalTok{, }\DecValTok{257}\NormalTok{, }\DecValTok{339}\NormalTok{, }\DecValTok{279}\NormalTok{, }\DecValTok{250}\NormalTok{, }\DecValTok{270}\NormalTok{, }\DecValTok{346}\NormalTok{, }\DecValTok{294}\NormalTok{, }\DecValTok{255}\NormalTok{, }\DecValTok{278}\NormalTok{, }\DecValTok{363}\NormalTok{, }
          \DecValTok{313}\NormalTok{, }\DecValTok{273}\NormalTok{, }\DecValTok{300}\NormalTok{, }\DecValTok{370}\NormalTok{, }\DecValTok{331}\NormalTok{, }\DecValTok{288}\NormalTok{, }\DecValTok{306}\NormalTok{, }\DecValTok{386}\NormalTok{, }\DecValTok{335}\NormalTok{, }\DecValTok{288}\NormalTok{, }\DecValTok{308}\NormalTok{, }\DecValTok{402}\NormalTok{, }\DecValTok{353}\NormalTok{, }\DecValTok{316}\NormalTok{, }\DecValTok{325}\NormalTok{, }\DecValTok{405}\NormalTok{, }
          \DecValTok{393}\NormalTok{, }\DecValTok{319}\NormalTok{, }\DecValTok{327}\NormalTok{, }\DecValTok{442}\NormalTok{, }\DecValTok{383}\NormalTok{, }\DecValTok{332}\NormalTok{, }\DecValTok{361}\NormalTok{, }\DecValTok{446}\NormalTok{, }\DecValTok{387}\NormalTok{, }\DecValTok{357}\NormalTok{, }\DecValTok{374}\NormalTok{, }\DecValTok{466}\NormalTok{, }\DecValTok{410}\NormalTok{, }\DecValTok{370}\NormalTok{, }\DecValTok{379}\NormalTok{, }\DecValTok{487}\NormalTok{, }
          \DecValTok{419}\NormalTok{, }\DecValTok{378}\NormalTok{, }\DecValTok{393}\NormalTok{, }\DecValTok{506}\NormalTok{, }\DecValTok{458}\NormalTok{, }\DecValTok{387}\NormalTok{, }\DecValTok{427}\NormalTok{, }\DecValTok{565}\NormalTok{, }\DecValTok{465}\NormalTok{, }\DecValTok{445}\NormalTok{, }\DecValTok{450}\NormalTok{, }\DecValTok{556}\NormalTok{, }\DecValTok{500}\NormalTok{, }\DecValTok{452}\NormalTok{, }\DecValTok{435}\NormalTok{, }\DecValTok{554}\NormalTok{, }
          \DecValTok{510}\NormalTok{, }\DecValTok{433}\NormalTok{, }\DecValTok{453}\NormalTok{, }\DecValTok{548}\NormalTok{, }\DecValTok{486}\NormalTok{, }\DecValTok{453}\NormalTok{, }\DecValTok{457}\NormalTok{, }\DecValTok{566}\NormalTok{, }\DecValTok{515}\NormalTok{, }\DecValTok{464}\NormalTok{, }\DecValTok{431}\NormalTok{, }\DecValTok{588}\NormalTok{, }\DecValTok{503}\NormalTok{, }\DecValTok{443}\NormalTok{, }\DecValTok{448}\NormalTok{, }\DecValTok{555}\NormalTok{, }
          \DecValTok{513}\NormalTok{, }\DecValTok{427}\NormalTok{, }\DecValTok{473}\NormalTok{, }\DecValTok{526}\NormalTok{, }\DecValTok{548}\NormalTok{, }\DecValTok{440}\NormalTok{, }\DecValTok{469}\NormalTok{, }\DecValTok{575}\NormalTok{, }\DecValTok{493}\NormalTok{, }\DecValTok{433}\NormalTok{, }\DecValTok{480}\NormalTok{, }\DecValTok{576}\NormalTok{, }\DecValTok{475}\NormalTok{, }\DecValTok{405}\NormalTok{, }\DecValTok{435}\NormalTok{, }\DecValTok{535}\NormalTok{, }
          \DecValTok{453}\NormalTok{, }\DecValTok{430}\NormalTok{, }\DecValTok{417}\NormalTok{, }\DecValTok{552}\NormalTok{, }\DecValTok{464}\NormalTok{, }\DecValTok{417}\NormalTok{, }\DecValTok{423}\NormalTok{, }\DecValTok{554}\NormalTok{, }\DecValTok{459}\NormalTok{, }\DecValTok{428}\NormalTok{, }\DecValTok{429}\NormalTok{, }\DecValTok{534}\NormalTok{, }\DecValTok{481}\NormalTok{, }\DecValTok{416}\NormalTok{, }\DecValTok{440}\NormalTok{, }\DecValTok{538}\NormalTok{, }
          \DecValTok{474}\NormalTok{, }\DecValTok{440}\NormalTok{, }\DecValTok{447}\NormalTok{, }\DecValTok{598}\NormalTok{, }\DecValTok{467}\NormalTok{, }\DecValTok{439}\NormalTok{, }\DecValTok{446}\NormalTok{, }\DecValTok{567}\NormalTok{, }\DecValTok{485}\NormalTok{, }\DecValTok{441}\NormalTok{, }\DecValTok{429}\NormalTok{, }\DecValTok{599}\NormalTok{, }\DecValTok{464}\NormalTok{, }\DecValTok{424}\NormalTok{, }\DecValTok{436}\NormalTok{, }\DecValTok{574}\NormalTok{, }
          \DecValTok{443}\NormalTok{, }\DecValTok{410}\NormalTok{, }\DecValTok{420}\NormalTok{, }\DecValTok{532}\NormalTok{, }\DecValTok{433}\NormalTok{, }\DecValTok{421}\NormalTok{, }\DecValTok{410}\NormalTok{, }\DecValTok{512}\NormalTok{, }\DecValTok{449}\NormalTok{, }\DecValTok{381}\NormalTok{, }\DecValTok{423}\NormalTok{, }\DecValTok{531}\NormalTok{, }\DecValTok{426}\NormalTok{, }\DecValTok{408}\NormalTok{, }\DecValTok{416}\NormalTok{, }\DecValTok{520}\NormalTok{, }
          \DecValTok{409}\NormalTok{, }\DecValTok{398}\NormalTok{, }\DecValTok{398}\NormalTok{, }\DecValTok{507}\NormalTok{, }\DecValTok{432}\NormalTok{, }\DecValTok{398}\NormalTok{, }\DecValTok{406}\NormalTok{, }\DecValTok{526}\NormalTok{, }\DecValTok{428}\NormalTok{, }\DecValTok{397}\NormalTok{, }\DecValTok{403}\NormalTok{, }\DecValTok{517}\NormalTok{, }\DecValTok{435}\NormalTok{, }\DecValTok{383}\NormalTok{, }\DecValTok{424}\NormalTok{, }\DecValTok{521}\NormalTok{, }
          \DecValTok{421}\NormalTok{, }\DecValTok{402}\NormalTok{, }\DecValTok{414}\NormalTok{, }\DecValTok{500}\NormalTok{, }\DecValTok{451}\NormalTok{, }\DecValTok{380}\NormalTok{, }\DecValTok{416}\NormalTok{, }\DecValTok{492}\NormalTok{, }\DecValTok{428}\NormalTok{, }\DecValTok{408}\NormalTok{, }\DecValTok{406}\NormalTok{, }\DecValTok{506}\NormalTok{, }\DecValTok{435}\NormalTok{, }\DecValTok{380}\NormalTok{, }\DecValTok{421}\NormalTok{, }\DecValTok{490}\NormalTok{, }
          \DecValTok{435}\NormalTok{, }\DecValTok{390}\NormalTok{, }\DecValTok{412}\NormalTok{, }\DecValTok{454}\NormalTok{, }\DecValTok{416}\NormalTok{, }\DecValTok{403}\NormalTok{, }\DecValTok{408}\NormalTok{, }\DecValTok{482}\NormalTok{, }\DecValTok{438}\NormalTok{, }\DecValTok{386}\NormalTok{, }\DecValTok{405}\NormalTok{, }\DecValTok{491}\NormalTok{, }\DecValTok{427}\NormalTok{, }\DecValTok{383}\NormalTok{, }\DecValTok{394}\NormalTok{, }\DecValTok{473}\NormalTok{, }
          \DecValTok{420}\NormalTok{, }\DecValTok{390}\NormalTok{, }\DecValTok{410}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Airline Passengers Data}
\end{itemize}

This data set records monthly totals of international airline passengers (1949-1960).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers0}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{112}\NormalTok{, }\DecValTok{118}\NormalTok{, }\DecValTok{132}\NormalTok{, }\DecValTok{129}\NormalTok{, }\DecValTok{121}\NormalTok{, }\DecValTok{135}\NormalTok{, }\DecValTok{148}\NormalTok{, }\DecValTok{148}\NormalTok{, }\DecValTok{136}\NormalTok{, }\DecValTok{119}\NormalTok{, }\DecValTok{104}\NormalTok{, }\DecValTok{118}\NormalTok{, }\DecValTok{115}\NormalTok{, }\DecValTok{126}\NormalTok{, }\DecValTok{141}\NormalTok{, }
                \DecValTok{135}\NormalTok{, }\DecValTok{125}\NormalTok{, }\DecValTok{149}\NormalTok{, }\DecValTok{170}\NormalTok{, }\DecValTok{170}\NormalTok{, }\DecValTok{158}\NormalTok{, }\DecValTok{133}\NormalTok{, }\DecValTok{114}\NormalTok{, }\DecValTok{140}\NormalTok{, }\DecValTok{145}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{178}\NormalTok{, }\DecValTok{163}\NormalTok{, }\DecValTok{172}\NormalTok{, }\DecValTok{178}\NormalTok{, }
                \DecValTok{199}\NormalTok{, }\DecValTok{199}\NormalTok{, }\DecValTok{184}\NormalTok{, }\DecValTok{162}\NormalTok{, }\DecValTok{146}\NormalTok{, }\DecValTok{166}\NormalTok{, }\DecValTok{171}\NormalTok{, }\DecValTok{180}\NormalTok{, }\DecValTok{193}\NormalTok{, }\DecValTok{181}\NormalTok{, }\DecValTok{183}\NormalTok{, }\DecValTok{218}\NormalTok{, }\DecValTok{230}\NormalTok{, }\DecValTok{242}\NormalTok{, }\DecValTok{209}\NormalTok{, }
                \DecValTok{191}\NormalTok{, }\DecValTok{172}\NormalTok{, }\DecValTok{194}\NormalTok{, }\DecValTok{196}\NormalTok{, }\DecValTok{196}\NormalTok{, }\DecValTok{236}\NormalTok{, }\DecValTok{235}\NormalTok{, }\DecValTok{229}\NormalTok{, }\DecValTok{243}\NormalTok{, }\DecValTok{264}\NormalTok{, }\DecValTok{272}\NormalTok{, }\DecValTok{237}\NormalTok{, }\DecValTok{211}\NormalTok{, }\DecValTok{180}\NormalTok{, }\DecValTok{201}\NormalTok{, }
                \DecValTok{204}\NormalTok{, }\DecValTok{188}\NormalTok{, }\DecValTok{235}\NormalTok{, }\DecValTok{227}\NormalTok{, }\DecValTok{234}\NormalTok{, }\DecValTok{264}\NormalTok{, }\DecValTok{302}\NormalTok{, }\DecValTok{293}\NormalTok{, }\DecValTok{259}\NormalTok{, }\DecValTok{229}\NormalTok{, }\DecValTok{203}\NormalTok{, }\DecValTok{229}\NormalTok{, }\DecValTok{242}\NormalTok{, }\DecValTok{233}\NormalTok{, }\DecValTok{267}\NormalTok{, }
                \DecValTok{269}\NormalTok{, }\DecValTok{270}\NormalTok{, }\DecValTok{315}\NormalTok{, }\DecValTok{364}\NormalTok{, }\DecValTok{347}\NormalTok{, }\DecValTok{312}\NormalTok{, }\DecValTok{274}\NormalTok{, }\DecValTok{237}\NormalTok{, }\DecValTok{278}\NormalTok{, }\DecValTok{284}\NormalTok{, }\DecValTok{277}\NormalTok{, }\DecValTok{317}\NormalTok{, }\DecValTok{313}\NormalTok{, }\DecValTok{318}\NormalTok{, }\DecValTok{374}\NormalTok{, }
                \DecValTok{413}\NormalTok{, }\DecValTok{405}\NormalTok{, }\DecValTok{355}\NormalTok{, }\DecValTok{306}\NormalTok{, }\DecValTok{271}\NormalTok{, }\DecValTok{306}\NormalTok{, }\DecValTok{315}\NormalTok{, }\DecValTok{301}\NormalTok{, }\DecValTok{356}\NormalTok{, }\DecValTok{348}\NormalTok{, }\DecValTok{355}\NormalTok{, }\DecValTok{422}\NormalTok{, }\DecValTok{465}\NormalTok{, }\DecValTok{467}\NormalTok{, }\DecValTok{404}\NormalTok{, }
                \DecValTok{347}\NormalTok{, }\DecValTok{305}\NormalTok{, }\DecValTok{336}\NormalTok{, }\DecValTok{340}\NormalTok{, }\DecValTok{318}\NormalTok{, }\DecValTok{362}\NormalTok{, }\DecValTok{348}\NormalTok{, }\DecValTok{363}\NormalTok{, }\DecValTok{435}\NormalTok{, }\DecValTok{491}\NormalTok{, }\DecValTok{505}\NormalTok{, }\DecValTok{404}\NormalTok{, }\DecValTok{359}\NormalTok{, }\DecValTok{310}\NormalTok{, }\DecValTok{337}\NormalTok{, }
                \DecValTok{360}\NormalTok{, }\DecValTok{342}\NormalTok{, }\DecValTok{406}\NormalTok{, }\DecValTok{396}\NormalTok{, }\DecValTok{420}\NormalTok{, }\DecValTok{472}\NormalTok{, }\DecValTok{548}\NormalTok{, }\DecValTok{559}\NormalTok{, }\DecValTok{463}\NormalTok{, }\DecValTok{407}\NormalTok{, }\DecValTok{362}\NormalTok{, }\DecValTok{405}\NormalTok{, }\DecValTok{417}\NormalTok{, }\DecValTok{391}\NormalTok{, }\DecValTok{419}\NormalTok{,}
                \DecValTok{461}\NormalTok{, }\DecValTok{472}\NormalTok{, }\DecValTok{535}\NormalTok{, }\DecValTok{622}\NormalTok{, }\DecValTok{606}\NormalTok{, }\DecValTok{508}\NormalTok{, }\DecValTok{461}\NormalTok{, }\DecValTok{390}\NormalTok{, }\DecValTok{432}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{classical-decompositions}{%
\section{Classical Decompositions}\label{classical-decompositions}}

Classical decomposition was developed about a century ago and is still widely used nowadays. Depending on the types of time series models, there are two basic methods of decomposition: additive and multiplicative.

The following two time series represent the above two basic types of times series models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ausbeer.ts }\OtherTok{=} \FunctionTok{ts}\NormalTok{(ausbeer0[}\DecValTok{9}\SpecialCharTok{:}\DecValTok{72}\NormalTok{], }\AttributeTok{frequency =} \DecValTok{4}\NormalTok{, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{1958}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{AirPassengers.ts }\OtherTok{=} \FunctionTok{ts}\NormalTok{(AirPassengers0, }\AttributeTok{frequency =} \DecValTok{4}\NormalTok{, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{1949}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ausbeer.ts, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }\AttributeTok{ylab=}\StringTok{""}\NormalTok{, }\AttributeTok{main =} \StringTok{"Additive Model"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(AirPassengers.ts, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }\AttributeTok{ylab=}\StringTok{""}\NormalTok{, }\AttributeTok{main =} \StringTok{"Multiplicative Model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-212-1} 

}

\caption{time series plots of additive and multiplicative series}\label{fig:unnamed-chunk-212}
\end{figure}

Denote \(T = trend\), \(S = seasonality\), and \(E = error\). With these notations, we can characterize the structure of \textbf{additive and multiplicative} time series.

In a \textbf{multiplicative time series}, the components multiply together to make the time series. As the time series increases in magnitude, the \textbf{seasonal variation} increases as well. The structure of a multiple time series has the following form.

\[
y_t = T_t \times S_t \times E_t.
\]

In an \textbf{additive time series}, the components add together to make the time series. If you have an increasing trend, you still see roughly the same size peaks and troughs throughout the time series. This is often seen in indexed time series where the absolute value is growing but changes stay relative. The structure of an additive time series has the following form

\[
y_t = T_t + S_t + E_t.
\]

For an \textbf{additive time series}, the detrended additive series has for \(D_t = y_t - T_t\). For the multiplicative time series, the detrended time series is calculated by \(D_t = y_t/T_t\)

\hypertarget{understanding-the-classical-decomposition-of-time-series}{%
\section{Understanding the Classical Decomposition of Time Series}\label{understanding-the-classical-decomposition-of-time-series}}

To understand the structure of additive and multiplication ties series, we decompose these time series by calculating the trend, seasonality, and errors \emph{manually} by writing a basic R script to gain a technical understanding of decomposing a time series. At the very end of this section, we introduce the R function \textbf{decompose()} to extract the three components of additive and multiplicative time series.

\hypertarget{detect-trend}{%
\subsection{Detect Trend}\label{detect-trend}}

To detect the underlying trend, we use a smoothing technique called \textbf{moving average} and it's variant \textbf{centered moving average}. For a seasonal time series, the width of the moving window must be the same as the seasonality. Therefore, to decompose a time series we need to know the seasonality period: weekly, monthly, etc.

\textbf{Example 1}: Australian beer production data has an annual seasonality. Since the data set is quarterly data, the moving average window should be 4.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trend.beer }\OtherTok{=} \FunctionTok{ma}\NormalTok{(ausbeer.ts, }\AttributeTok{order =} \DecValTok{4}\NormalTok{, }\AttributeTok{centre =}\NormalTok{ T)  }\CommentTok{\# centre = T =\textgreater{} centered moving average}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{as.ts}\NormalTok{(ausbeer.ts), }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }\AttributeTok{ylab=}\StringTok{""}\NormalTok{, }\AttributeTok{col=}\StringTok{"darkred"}\NormalTok{, }\AttributeTok{lwd =}\DecValTok{2}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main =} \StringTok{"Extract trend from Australia Beer Production"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(trend.beer, }\AttributeTok{lwd =}\DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"original series"}\NormalTok{, }\StringTok{"trend curve"}\NormalTok{), }\AttributeTok{lwd=}\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"darkred"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-213-1} 

}

\caption{Serires plot with trend curve}\label{fig:unnamed-chunk-213}
\end{figure}

\textbf{Example 2}: The airline passenger data were recorded monthly. It has an annual seasonal pattern. We choose a moving average window of 12 to extract the trend from this multiplicative time series.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trend.air }\OtherTok{=} \FunctionTok{ma}\NormalTok{(AirPassengers.ts, }\AttributeTok{order =} \DecValTok{12}\NormalTok{, }\AttributeTok{centre =}\NormalTok{ T)  }\CommentTok{\# centre = T =\textgreater{} centered moving average}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{as.ts}\NormalTok{(AirPassengers.ts), }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }\AttributeTok{ylab=}\StringTok{""}\NormalTok{, }\AttributeTok{col=}\StringTok{"darkred"}\NormalTok{, }\AttributeTok{lwd =}\DecValTok{2}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main =} \StringTok{"Extract trend from Airline Passengers Monthly Data"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(trend.air, }\AttributeTok{lwd =}\DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"original series"}\NormalTok{, }\StringTok{"trend curve"}\NormalTok{), }\AttributeTok{lwd=}\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"darkred"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-214-1} 

}

\caption{series plot with curve trend}\label{fig:unnamed-chunk-214}
\end{figure}

The \textbf{moving averages} of both time series are recorded in the above two code chunks and will be used to restore the original series.

The process of removing the trend from a time series is called \textbf{detrending} time series.

The way of detrending a time series is dependent on the types of the time series. The following code shows how to calculate the detrended time series.

\textbf{Example 3}: We calculate the detrended series using the Australian Beer data and the Airline Passengers data as an example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{detrend.beer }\OtherTok{=}\NormalTok{ ausbeer.ts }\SpecialCharTok{{-}}\NormalTok{ trend.beer}
\NormalTok{detrend.air }\OtherTok{=}\NormalTok{ AirPassengers.ts}\SpecialCharTok{/}\NormalTok{trend.air}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\CommentTok{\# plot(ausbeer.ts, xlab="", main = "Australia Beer", col="darkred")}
\CommentTok{\# plot(AirPassengers.ts, xlab="", main = "Air Passengers", col="blue")}
\FunctionTok{plot}\NormalTok{(detrend.beer, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }\AttributeTok{main =} \StringTok{"Australia Beer"}\NormalTok{, }\AttributeTok{col=}\StringTok{"darkred"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(detrend.air, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }\AttributeTok{main =} \StringTok{"Air Passengers"}\NormalTok{, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-215-1} 

}

\caption{Detrended series}\label{fig:unnamed-chunk-215}
\end{figure}

The technique we used in removing the trend from a time series model is a non-parametric smoothing procedure. There are different techniques in statistics to estimate a curve for a given set. The \textbf{moving average} is one of the simplest ones and is widely used in time series.

\hypertarget{extracting-the-seasonality}{%
\subsection{Extracting the Seasonality}\label{extracting-the-seasonality}}

Similar to the trend in a time series, the seasonality of a time series is also a non-random structural pattern. We can extract the seasonality from the detrended time series.

The idea is to redefine a \textbf{seasonal series} based on the detrended series by replacing all observations taken from the same seasonal period with the average of these observations. This process is called \textbf{averaging seasonality}. This idea is implemented in R. Here is how to do it in R.

\textbf{Example 4}: Use the \textbf{Australian Beer Production Data} and the \textbf{Airline Passenger Data} after their trends were removed. The following code illustrates how to calculate and graph the seasonality of both series.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\DocumentationTok{\#\# Australia Beer}
\NormalTok{mtrx.beer }\OtherTok{=} \FunctionTok{t}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(}\AttributeTok{data =}\NormalTok{ detrend.beer, }\AttributeTok{nrow =} \DecValTok{4}\NormalTok{))}
\NormalTok{seasonal.beer }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(mtrx.beer, }\AttributeTok{na.rm =}\NormalTok{ T)}
\NormalTok{seasonal.beer.ts }\OtherTok{=} \FunctionTok{as.ts}\NormalTok{(}\FunctionTok{rep}\NormalTok{(seasonal.beer,}\DecValTok{16}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(seasonal.beer.ts, }\AttributeTok{xlab =} \StringTok{""}\NormalTok{, }\AttributeTok{col=}\StringTok{"darkred"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Seasonal series of Australia beer"}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{mtrx.air }\OtherTok{=} \FunctionTok{t}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(}\AttributeTok{data =}\NormalTok{ detrend.air, }\AttributeTok{nrow =} \DecValTok{12}\NormalTok{))}
\NormalTok{seasonal.air }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(mtrx.air, }\AttributeTok{na.rm =}\NormalTok{ T)}
\NormalTok{seasonal.air.ts }\OtherTok{=} \FunctionTok{as.ts}\NormalTok{(}\FunctionTok{rep}\NormalTok{(seasonal.air,}\DecValTok{16}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(seasonal.air.ts, }\AttributeTok{xlab =} \StringTok{""}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Seasonal series of air passengers"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-216-1} 

}

\caption{Seasonal series}\label{fig:unnamed-chunk-216}
\end{figure}

\hypertarget{extracting-remainder-errors}{%
\subsection{Extracting Remainder Errors}\label{extracting-remainder-errors}}

The \textbf{error term} is the random component in the time series. We learned the of extracting the trend and seasonality from the original time series. How to extract the ``random'' noise from a given time series?

In the additive model, the random \textbf{error term} is given by \(E_t = y_t - T_t -S_t\). The \textbf{random error} for a multiplicative model is given by \(E_t = y_t/(T_t \times S_t)\).

\textbf{Example 5}: Use the above formulas to separate the random error components in additive and multiplicative models using Australian beer production and Airline passenger series data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{random.beer }\OtherTok{=}\NormalTok{ ausbeer.ts }\SpecialCharTok{{-}}\NormalTok{ trend.beer }\SpecialCharTok{{-}}\NormalTok{ seasonal.beer}
\NormalTok{random.air }\OtherTok{=}\NormalTok{ AirPassengers.ts }\SpecialCharTok{/}\NormalTok{ (trend.air }\SpecialCharTok{*}\NormalTok{ seasonal.air)}
\DocumentationTok{\#\#}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(random.beer , }\AttributeTok{xlab =} \StringTok{""}\NormalTok{, }\AttributeTok{col=}\StringTok{"darkred"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Random errors of Australia beer"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(random.air, }\AttributeTok{xlab =} \StringTok{""}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Random errors of air passengers"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-217-1} 

}

\caption{Random error components}\label{fig:unnamed-chunk-217}
\end{figure}

\hypertarget{reconstruct-the-original-series-compose-new-series}{%
\subsection{Reconstruct the Original Series / Compose New Series}\label{reconstruct-the-original-series-compose-new-series}}

The original series can be \textbf{reconstructed} by using the decomposed components. Since the \textbf{moving average technique} was used in the detrending series, the resulting \textbf{reconstructed} series with \(T_t\), \(S_t\), and \(E_t\) will generate a few missing values in the beginning and the end depending on the width of the \textbf{moving average window}.

\textbf{Example 6}: \textbf{Reconstruct} the \textbf{original} series of Australian beer data and the airline passenger data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{recomposed.beer }\OtherTok{=}\NormalTok{ trend.beer}\SpecialCharTok{+}\NormalTok{seasonal.beer}\SpecialCharTok{+}\NormalTok{random.beer}
\NormalTok{recomposed.air }\OtherTok{=}\NormalTok{ trend.air}\SpecialCharTok{*}\NormalTok{seasonal.air}\SpecialCharTok{*}\NormalTok{random.air}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ausbeer.ts, }\AttributeTok{col=}\StringTok{"darkred"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(recomposed.beer, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"original series"}\NormalTok{, }\StringTok{"reconstructed series"}\NormalTok{), }
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"darkred"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\AttributeTok{lty=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.8}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"Australian Beer"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{plot}\NormalTok{(AirPassengers.ts, }\AttributeTok{col=}\StringTok{"darkred"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(recomposed.air, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"original series"}\NormalTok{, }\StringTok{"reconstructed series"}\NormalTok{), }
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"darkred"}\NormalTok{,}\StringTok{"blue"}\NormalTok{), }\AttributeTok{lty=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.8}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"Airline Passengers"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-218-1} 

}

\caption{Adding trend series to the original series}\label{fig:unnamed-chunk-218}
\end{figure}

\hypertarget{decomposing-time-series-with-decompose}{%
\subsection{\texorpdfstring{Decomposing Time Series with \textbf{decompose()}}{Decomposing Time Series with decompose()}}\label{decomposing-time-series-with-decompose}}

The R library \textbf{forecast} was created by a team led by a leading expert in the discipline. We'll use the R function \textbf{decompose( )} in library\{forecast\} as a decomposition function to decompose a series into seasonal, trend, and random components. The Australian beer production (additive) and airline passenger numbers (multiplicative) will still be used to illustrate the steps.

\textbf{Example 6}: Plot the components of the Australian beer production data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decomp.beer }\OtherTok{=} \FunctionTok{decompose}\NormalTok{(ausbeer.ts, }\StringTok{"additive"}\NormalTok{)}
\DocumentationTok{\#\# plot the decomposed components}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{oma=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(decomp.beer, }\AttributeTok{col=}\StringTok{"darkred"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-219-1} \end{center}

\textbf{Example 7}: Plot the components of the Airline Passenger Data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decomp.air }\OtherTok{=} \FunctionTok{decompose}\NormalTok{(AirPassengers.ts, }\StringTok{"multiplicative"}\NormalTok{)}
\DocumentationTok{\#\# plot the decomposed components}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{oma=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(decomp.air, }\AttributeTok{col=} \StringTok{"navy"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA321EB_files/figure-latex/unnamed-chunk-220-1} \end{center}

Th R function \textbf{decompose()} can be used to extract the individual components from a given additive and multiplicative model using the following code.

\textbf{Example 8}: Decomposing Australian beer production data using \textbf{decompose()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decomp.beer }\OtherTok{=} \FunctionTok{decompose}\NormalTok{(ausbeer.ts, }\StringTok{"additive"}\NormalTok{)}
\CommentTok{\# the four components can be extracted by}
\NormalTok{seasonal.beer }\OtherTok{=}\NormalTok{ decomp.beer}\SpecialCharTok{$}\NormalTok{seasonal}
\NormalTok{trend.beer }\OtherTok{=}\NormalTok{ decomp.beer}\SpecialCharTok{$}\NormalTok{trend}
\NormalTok{error.beer }\OtherTok{=}\NormalTok{ decomp.beer}\SpecialCharTok{$}\NormalTok{random}
\end{Highlighting}
\end{Shaded}

\textbf{Example 9}: Decomposing airline passengers data using \textbf{decompose()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decomp.air }\OtherTok{=} \FunctionTok{decompose}\NormalTok{(AirPassengers.ts, }\StringTok{"multiplicative"}\NormalTok{)}
\CommentTok{\# the four components can be extracted by}
\NormalTok{seasonal.air }\OtherTok{=}\NormalTok{ decomp.air}\SpecialCharTok{$}\NormalTok{seasonal}
\NormalTok{trend.air }\OtherTok{=}\NormalTok{ decomp.air}\SpecialCharTok{$}\NormalTok{trend}
\NormalTok{error.air }\OtherTok{=}\NormalTok{ decomp.air}\SpecialCharTok{$}\NormalTok{random}
\end{Highlighting}
\end{Shaded}

\textbf{Concluding Remark}: There are other decomposition methods. among them, \textbf{X11} is commonly used in econometrics. The recently developed method \textbf{STL()} that used LOESS algorithm to estimate the trend can also be used to extract components from more general time series. We will outline this decomposition method to forecast future values.

\hypertarget{forecasting-with-decomposing}{%
\section{Forecasting with Decomposing}\label{forecasting-with-decomposing}}

Several benchmark forecasting methods were introduced in the previous module. Next, we use these benchmark methods to forecast the deseasonalized series. Since the seasonality of a time series is a \textbf{scalar}, we can forecast the deseasonalized series through decomposition and then adjust the forecasted values.

\hypertarget{forecasting-additive-models-with-decomposing}{%
\subsection{Forecasting Additive Models with Decomposing}\label{forecasting-additive-models-with-decomposing}}

Since the multiplicative models can be converted to an additive model by

\[
\log(y_t) = \log(S_t) + \log(T_t) + \log(E_t).
\]

So we only restrict our discussion in this module to additive models. Assuming an additive decomposition, the decomposed time series can be written as

\[y_t = \hat{S}_t + \hat{A}_t\]

where \(\hat{A}_t = \hat{T} + \hat{E}_t\) is the seasonally adjusted component. We can forecast the future values based on \(\hat{A}_t\).

\textbf{Example 10}: Forecasting based on the seasonally adjusted series with Australian beer production data. We introduced four benchmark forecasting methods in the previous module. For a time series with a trend, naive, seasonal naive, and drift method is more accurate than the moving average. The issue is that none of the benchmark methods forecast the trend. As an illustrative example, we use the naive method to forecast the deseasonalized series and then add the seasonal adjustment to the forecast values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decomp.beer }\OtherTok{=} \FunctionTok{decompose}\NormalTok{(ausbeer.ts, }\StringTok{"additive"}\NormalTok{)}
\CommentTok{\# the four components can be extracted by}
\NormalTok{seasonal.beer }\OtherTok{=}\NormalTok{ decomp.beer}\SpecialCharTok{$}\NormalTok{seasonal}
\NormalTok{trend.beer }\OtherTok{=}\NormalTok{ decomp.beer}\SpecialCharTok{$}\NormalTok{trend}
\NormalTok{error.beer }\OtherTok{=}\NormalTok{ decomp.beer}\SpecialCharTok{$}\NormalTok{random}
\NormalTok{seasonal.adj }\OtherTok{=}\NormalTok{ trend.beer }\SpecialCharTok{+}\NormalTok{ error.beer}
\NormalTok{deseasonalized.pred }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{rwf}\NormalTok{(}\FunctionTok{na.omit}\NormalTok{(seasonal.adj), }\AttributeTok{h =} \DecValTok{6}\NormalTok{)) }
\NormalTok{seasonality }\OtherTok{=}  \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(seasonal.beer[}\DecValTok{3}\SpecialCharTok{:}\DecValTok{8}\NormalTok{],}\DecValTok{5}\NormalTok{), }\AttributeTok{nco=}\DecValTok{5}\NormalTok{, }\AttributeTok{byrow=}\NormalTok{F)                      }
\NormalTok{seasonal.adj.pred }\OtherTok{\textless{}{-}}\NormalTok{ deseasonalized.pred  }\SpecialCharTok{+}\NormalTok{ seasonality}
\FunctionTok{kable}\NormalTok{(seasonal.adj.pred, }\AttributeTok{caption =} \StringTok{"Forecasting with decomposing {-} drift method"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-223}Forecasting with decomposing - drift method}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
  & Point Forecast & Lo 80 & Hi 80 & Lo 95 & Hi 95\\
\hline
1973 Q3 & 404.7167 & 386.3725 & 423.0608 & 376.6617 & 432.7717\\
\hline
1973 Q4 & 486.6167 & 460.6741 & 512.5593 & 446.9409 & 526.2924\\
\hline
1974 Q1 & 437.1500 & 405.3769 & 468.9231 & 388.5573 & 485.7427\\
\hline
1974 Q2 & 387.0000 & 350.3116 & 423.6884 & 330.8900 & 443.1100\\
\hline
1974 Q3 & 404.7167 & 363.6978 & 445.7355 & 341.9838 & 467.4496\\
\hline
1974 Q4 & 486.6167 & 441.6828 & 531.5505 & 417.8962 & 555.3371\\
\hline
\end{tabular}
\end{table}

Since the deseasonalized series has two missing values in the beginning and two in the end. we remove the missing values before using the drift methods to forecast the next 6 periods (qtr3, 1973 - qtr 4, 1874). The forecast values are given in the above table.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{62}\NormalTok{, }\FunctionTok{as.vector}\NormalTok{(ausbeer.ts)[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{63}\NormalTok{,}\DecValTok{64}\NormalTok{)], }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{70}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{200}\NormalTok{, }\DecValTok{570}\NormalTok{),}
     \AttributeTok{xlab=}\StringTok{""}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Beer Production"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Forecast with classical decomposing"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{62}\SpecialCharTok{:}\DecValTok{64}\NormalTok{,  }\FunctionTok{as.vector}\NormalTok{(ausbeer.ts)[}\FunctionTok{c}\NormalTok{(}\DecValTok{62}\NormalTok{,}\DecValTok{63}\NormalTok{,}\DecValTok{64}\NormalTok{)], }\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{62}\SpecialCharTok{:}\DecValTok{64}\NormalTok{,  }\FunctionTok{as.vector}\NormalTok{(ausbeer.ts)[}\FunctionTok{c}\NormalTok{(}\DecValTok{62}\NormalTok{,}\DecValTok{63}\NormalTok{,}\DecValTok{64}\NormalTok{)], }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{pch=}\DecValTok{21}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{63}\SpecialCharTok{:}\DecValTok{68}\NormalTok{,seasonal.adj.pred[,}\DecValTok{1}\NormalTok{], }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{63}\SpecialCharTok{:}\DecValTok{68}\NormalTok{,seasonal.adj.pred[,}\DecValTok{1}\NormalTok{], }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-224-1} 

}

\caption{forecasting with classical decomposing}\label{fig:unnamed-chunk-224}
\end{figure}

The plot of the forecast values and the original values. The red plot represented quarters 3-4 of 1973 and forecast values in quarters 3 - quarters 4, 1974, are plotted in \textbf{blue}. We can see that

\hypertarget{concepts-of-seasonal-and-trend-decomposition-using-loess-stl}{%
\subsection{Concepts of Seasonal and Trend Decomposition Using Loess (STL)}\label{concepts-of-seasonal-and-trend-decomposition-using-loess-stl}}

Seasonal and Trend decomposition using LOESS (STL) combines the classical time series decomposition and the \textbf{modern} locally estimated scatterplot smoothing (LOESS). The LOESS has developed about 40 years ago and is a modern computational algorithm. The seasonal trend in a time series is a \textbf{fixed} pattern. The real benefit of STL is to use the LOESS to estimate the nonlinear trend more accurately. We will not discuss the technical development of the STL. Instead, we will use its R implementation to decompose time series and use it to forecast future values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stl.beer }\OtherTok{=} \FunctionTok{stl}\NormalTok{(ausbeer.ts, }\StringTok{"periodic"}\NormalTok{)}
\NormalTok{seasonal.stl.beer   }\OtherTok{\textless{}{-}}\NormalTok{ stl.beer}\SpecialCharTok{$}\NormalTok{time.series[,}\DecValTok{1}\NormalTok{]}
\NormalTok{trend.stl.beer     }\OtherTok{\textless{}{-}}\NormalTok{ stl.beer}\SpecialCharTok{$}\NormalTok{time.series[,}\DecValTok{2}\NormalTok{]}
\NormalTok{random.stl.beer  }\OtherTok{\textless{}{-}}\NormalTok{ stl.beer}\SpecialCharTok{$}\NormalTok{time.series[,}\DecValTok{3}\NormalTok{]}
\DocumentationTok{\#\#\#}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ausbeer.ts)}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{as.ts}\NormalTok{(seasonal.stl.beer))}
\FunctionTok{plot}\NormalTok{(trend.stl.beer)}
\FunctionTok{plot}\NormalTok{(random.stl.beer)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-225-1} 

}

\caption{Decomposing with STL approach}\label{fig:unnamed-chunk-225}
\end{figure}

We can also plot the above-decomposed components in a single step as follows with the STL model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(stl.beer)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-226-1} 

}

\caption{plot component panel with STL object}\label{fig:unnamed-chunk-226}
\end{figure}

\hypertarget{forecast-with-stl-decomposing}{%
\subsection{Forecast with STL decomposing}\label{forecast-with-stl-decomposing}}

For the additive model, we can use the R function \textbf{stl()} to decompose the series into three components. It uses a more robust non-parametric smoothing method (LOESS) to estimate the nonlinear trend.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stl}\NormalTok{(ausbeer.ts,}\AttributeTok{s.window=}\StringTok{"periodic"}\NormalTok{)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{forecast}\NormalTok{(fit,}\AttributeTok{h=}\DecValTok{6}\NormalTok{, }\AttributeTok{method=}\StringTok{"rwdrift"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-227-1} 

}

\caption{Forecas with STL decomposing}\label{fig:unnamed-chunk-227}
\end{figure}

\hypertarget{length-of-time-series}{%
\subsection{Length of Time Series}\label{length-of-time-series}}

The length of the time series impacts the performance of the forecasting. In general, a very long time series (for example, more than 200 observations) usually does not work well for most of the existing models partly because the existing models were not built for \textbf{very long} series. Intuitively, future values are dependent on recent historical values. If including too old observations that have no predictive power in the model will bring bias and noise to the underlying model and, hence, negative impacts on the performance of the model.

There are a lot of discussions in literature and practice about the minimum size required for building a good time series model. It seems that 60 is the suggested minimum size. The actual minimum size depends on the situation and the level of accuracy.

In this class, we recommend the sample size be between 60 and 200. Therefore, in the assignment, if the original time series data has more than 200 observations, we can use only 150-200 most recent data values for analysis.

\hypertarget{case-study-2}{%
\section{Case Study}\label{case-study-2}}

\hypertarget{data-description-and}{%
\subsection{Data description and}\label{data-description-and}}

The time series used in this case study is chosen from \url{https://datahub.io/search}: 10-year nominal yields on US government bonds from the Federal Reserve. The 10-year government bond yield is considered a standard indicator of long-term interest rates. The data contains monthly rates. There are 808 months of data between April 1943 and July 2020. We only use look at monthly data between January 2008 and July 2020.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{us.bond}\OtherTok{=}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://datahub.io/core/bond{-}yields{-}us{-}10y/r/monthly.csv"}\NormalTok{)}
\NormalTok{n.row }\OtherTok{=} \FunctionTok{dim}\NormalTok{(us.bond)[}\DecValTok{1}\NormalTok{]}
\NormalTok{data.us.bond }\OtherTok{=}\NormalTok{ us.bond[(n.row}\DecValTok{{-}150}\NormalTok{)}\SpecialCharTok{:}\NormalTok{n.row, ]}
\end{Highlighting}
\end{Shaded}

\hypertarget{define-time-series-object}{%
\subsection{Define time series object}\label{define-time-series-object}}

Since this is monthly data, frequency =12 will be used the define the time series object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usbond.ts }\OtherTok{=} \FunctionTok{ts}\NormalTok{(data.us.bond[,}\DecValTok{2}\NormalTok{], }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{2008}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(usbond.ts, }\AttributeTok{main=}\StringTok{"US Bond Rates Between Jan, 2008 and July, 2020"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Monthly Rate"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-229-1} 

}

\caption{US bond monthly rates}\label{fig:unnamed-chunk-229}
\end{figure}

\hypertarget{forecasting-with-decomposing-1}{%
\subsection{Forecasting with Decomposing}\label{forecasting-with-decomposing-1}}

Notice that the classical decomposition method does not work as well as the STL method due to the robustness of the LOESS component. The following visual representations show the different behaviors of the two methods of decomposition.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cls.decomp }\OtherTok{=} \FunctionTok{decompose}\NormalTok{(usbond.ts)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(cls.decomp, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-230-1} 

}

\caption{Classical decomposition of additive time series}\label{fig:unnamed-chunk-230}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stl.decomp}\OtherTok{=}\FunctionTok{stl}\NormalTok{(usbond.ts, }\AttributeTok{s.window =} \DecValTok{12}\NormalTok{)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(stl.decomp)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-231-1} 

}

\caption{STL decomposition of additive time series}\label{fig:unnamed-chunk-231}
\end{figure}

\textbf{Training and Testing Data}

We hold up the \textbf{last 7 periods} of data for testing. The rest of the historical data will be used to train the forecast model.

To evaluate the effect of different sizes in training the time series, We define different training data sets with different sizes. Three training set sizes used in this example are 144, 109, 73, and 48. The same test set with size 7 will be used to calculate the prediction error.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ini.data }\OtherTok{=}\NormalTok{ data.us.bond[,}\DecValTok{2}\NormalTok{]}
\NormalTok{n0 }\OtherTok{=} \FunctionTok{length}\NormalTok{(ini.data)}
\DocumentationTok{\#\#}
\NormalTok{train.data01 }\OtherTok{=}\NormalTok{ data.us.bond[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n0}\DecValTok{{-}7}\NormalTok{), }\DecValTok{2}\NormalTok{]}
\NormalTok{train.data02 }\OtherTok{=}\NormalTok{ data.us.bond[}\DecValTok{37}\SpecialCharTok{:}\NormalTok{(n0}\DecValTok{{-}7}\NormalTok{), }\DecValTok{2}\NormalTok{]}
\NormalTok{train.data03 }\OtherTok{=}\NormalTok{ data.us.bond[}\DecValTok{73}\SpecialCharTok{:}\NormalTok{(n0}\DecValTok{{-}7}\NormalTok{), }\DecValTok{2}\NormalTok{]}
\NormalTok{train.data04 }\OtherTok{=}\NormalTok{ data.us.bond[}\DecValTok{97}\SpecialCharTok{:}\NormalTok{(n0}\DecValTok{{-}7}\NormalTok{), }\DecValTok{2}\NormalTok{]}
\DocumentationTok{\#\# last 7 observations}
\NormalTok{test.data }\OtherTok{=}\NormalTok{ data.us.bond[(n0}\DecValTok{{-}6}\NormalTok{)}\SpecialCharTok{:}\NormalTok{n0,}\DecValTok{2}\NormalTok{]}
\DocumentationTok{\#\#}
\NormalTok{train01.ts }\OtherTok{=} \FunctionTok{ts}\NormalTok{(train.data01, }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{2008}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{train02.ts }\OtherTok{=} \FunctionTok{ts}\NormalTok{(train.data02, }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{2011}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{train03.ts }\OtherTok{=} \FunctionTok{ts}\NormalTok{(train.data03, }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{2014}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{train04.ts }\OtherTok{=} \FunctionTok{ts}\NormalTok{(train.data04, }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{2016}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\DocumentationTok{\#\#}
\NormalTok{stl01 }\OtherTok{=} \FunctionTok{stl}\NormalTok{(train01.ts, }\AttributeTok{s.window =} \DecValTok{12}\NormalTok{)}
\NormalTok{stl02 }\OtherTok{=} \FunctionTok{stl}\NormalTok{(train02.ts, }\AttributeTok{s.window =} \DecValTok{12}\NormalTok{)}
\NormalTok{stl03 }\OtherTok{=} \FunctionTok{stl}\NormalTok{(train03.ts, }\AttributeTok{s.window =} \DecValTok{12}\NormalTok{)}
\NormalTok{stl04 }\OtherTok{=} \FunctionTok{stl}\NormalTok{(train04.ts, }\AttributeTok{s.window =} \DecValTok{12}\NormalTok{)}
\DocumentationTok{\#\# Forecast with decomposing}
\NormalTok{fcst01 }\OtherTok{=} \FunctionTok{forecast}\NormalTok{(stl01,}\AttributeTok{h=}\DecValTok{7}\NormalTok{, }\AttributeTok{method=}\StringTok{"naive"}\NormalTok{)}
\NormalTok{fcst02 }\OtherTok{=} \FunctionTok{forecast}\NormalTok{(stl02,}\AttributeTok{h=}\DecValTok{7}\NormalTok{, }\AttributeTok{method=}\StringTok{"naive"}\NormalTok{)}
\NormalTok{fcst03 }\OtherTok{=} \FunctionTok{forecast}\NormalTok{(stl03,}\AttributeTok{h=}\DecValTok{7}\NormalTok{, }\AttributeTok{method=}\StringTok{"naive"}\NormalTok{)}
\NormalTok{fcst04 }\OtherTok{=} \FunctionTok{forecast}\NormalTok{(stl04,}\AttributeTok{h=}\DecValTok{7}\NormalTok{, }\AttributeTok{method=}\StringTok{"naive"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We next perform error analysis.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# To compare different errors, we will not use the percentage for MAPE}
\NormalTok{PE01}\OtherTok{=}\NormalTok{(test.data}\SpecialCharTok{{-}}\NormalTok{fcst01}\SpecialCharTok{$}\NormalTok{mean)}\SpecialCharTok{/}\NormalTok{fcst01}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{PE02}\OtherTok{=}\NormalTok{(test.data}\SpecialCharTok{{-}}\NormalTok{fcst02}\SpecialCharTok{$}\NormalTok{mean)}\SpecialCharTok{/}\NormalTok{fcst02}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{PE03}\OtherTok{=}\NormalTok{(test.data}\SpecialCharTok{{-}}\NormalTok{fcst03}\SpecialCharTok{$}\NormalTok{mean)}\SpecialCharTok{/}\NormalTok{fcst03}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{PE04}\OtherTok{=}\NormalTok{(test.data}\SpecialCharTok{{-}}\NormalTok{fcst04}\SpecialCharTok{$}\NormalTok{mean)}\SpecialCharTok{/}\NormalTok{fcst04}\SpecialCharTok{$}\NormalTok{mean}
\DocumentationTok{\#\#\#}
\NormalTok{MAPE1 }\OtherTok{=} \FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(PE01))}
\NormalTok{MAPE2 }\OtherTok{=} \FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(PE02))}
\NormalTok{MAPE3 }\OtherTok{=} \FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(PE03))}
\NormalTok{MAPE4 }\OtherTok{=} \FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(PE04))}
\DocumentationTok{\#\#\#}
\NormalTok{E1}\OtherTok{=}\NormalTok{test.data}\SpecialCharTok{{-}}\NormalTok{fcst01}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{E2}\OtherTok{=}\NormalTok{test.data}\SpecialCharTok{{-}}\NormalTok{fcst02}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{E3}\OtherTok{=}\NormalTok{test.data}\SpecialCharTok{{-}}\NormalTok{fcst03}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{E4}\OtherTok{=}\NormalTok{test.data}\SpecialCharTok{{-}}\NormalTok{fcst04}\SpecialCharTok{$}\NormalTok{mean}
\DocumentationTok{\#\#}
\NormalTok{MSE1}\OtherTok{=}\FunctionTok{mean}\NormalTok{(E1}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{MSE2}\OtherTok{=}\FunctionTok{mean}\NormalTok{(E2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{MSE3}\OtherTok{=}\FunctionTok{mean}\NormalTok{(E3}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{MSE4}\OtherTok{=}\FunctionTok{mean}\NormalTok{(E4}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#\#}
\NormalTok{MSE}\OtherTok{=}\FunctionTok{c}\NormalTok{(MSE1, MSE2, MSE3, MSE4)}
\NormalTok{MAPE}\OtherTok{=}\FunctionTok{c}\NormalTok{(MAPE1, MAPE2, MAPE3, MAPE4)}
\NormalTok{accuracy}\OtherTok{=}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{MSE=}\NormalTok{MSE, }\AttributeTok{MAPE=}\NormalTok{MAPE)}
\FunctionTok{row.names}\NormalTok{(accuracy)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"n.144"}\NormalTok{, }\StringTok{"n.109"}\NormalTok{, }\StringTok{"n. 73"}\NormalTok{, }\StringTok{"n. 48"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(accuracy, }\AttributeTok{caption=}\StringTok{"Error comparison between forecast results with different sample sizes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-233}Error comparison between forecast results with different sample sizes}
\centering
\begin{tabular}[t]{l|r|r}
\hline
  & MSE & MAPE\\
\hline
n.144 & 0.7967685 & 0.4518108\\
\hline
n.109 & 0.7718715 & 0.4463052\\
\hline
n. 73 & 0.7665760 & 0.4449924\\
\hline
n. 48 & 0.8055530 & 0.4649921\\
\hline
\end{tabular}
\end{table}

\newpage

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, MSE, }\AttributeTok{type=}\StringTok{"b"}\NormalTok{, }\AttributeTok{col=}\StringTok{"darkred"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Error"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{,}
     \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.4}\NormalTok{,.}\DecValTok{85}\NormalTok{),}\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{,}\FloatTok{4.5}\NormalTok{), }\AttributeTok{main=}\StringTok{"Error Curves"}\NormalTok{, }\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{labs}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"n=144"}\NormalTok{, }\StringTok{"n=109"}\NormalTok{, }\StringTok{"n=73"}\NormalTok{, }\StringTok{"n=48"}\NormalTok{)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{at=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{label=}\NormalTok{labs, }\AttributeTok{pos=}\FloatTok{0.4}\NormalTok{)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, MAPE, }\AttributeTok{type=}\StringTok{"b"}\NormalTok{, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, MAPE}\FloatTok{+0.03}\NormalTok{, }\FunctionTok{as.character}\NormalTok{(}\FunctionTok{round}\NormalTok{(MAPE,}\DecValTok{4}\NormalTok{)), }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.7}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, MSE}\FloatTok{{-}0.03}\NormalTok{, }\FunctionTok{as.character}\NormalTok{(}\FunctionTok{round}\NormalTok{(MSE,}\DecValTok{4}\NormalTok{)), }\AttributeTok{col=}\StringTok{"darkred"}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.7}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\FloatTok{0.63}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"MSE"}\NormalTok{, }\StringTok{"MAPE"}\NormalTok{), }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"darkred"}\NormalTok{,}\StringTok{"blue"}\NormalTok{), }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-234-1} 

}

\caption{Comparing forecast errors}\label{fig:unnamed-chunk-234}
\end{figure}

We trained the same algorithm with different sample sizes and compared the resulting accuracy measures. Among four training sizes 144, 109, 73, and 48. training data size 73 yields the best performance.

As anticipated, forecasting with STL smoothing does not yield decent results. However, our case study still accomplishes the main learning goals. To be more specific, we have learned how to

\begin{itemize}
\item
  decompose a time series
\item
  distinguish the graphical patterns of additive and multiplicative time series models
\item
  use the non-parametric smoothing LOESS method in time series forecasting;
\item
  use the technique of machine learning to tune the training size to identify the optimal training size to achieve the best accuracy. In this case, the training size is considered a tuning parameter (hyper-parameter).
\end{itemize}

We will start building actual and practical forecasting models in the next module - exponential smoothing models.

\hypertarget{analysis-assignment-6}{%
\section{Analysis Assignment}\label{analysis-assignment-6}}

This assignment focuses on the conceptual understanding of decomposing time series and forecasting with decomposing. As mentioned in the closing paragraph of the case study in the class note, the goals of this assignment are (1) to enhance your conceptual understanding of methods of decomposition and forecasting; (2) to find the appropriate training size to produce the best performance.

The following websites contain some time series data. You can find one time series that has at least 150 observations. If your data set has more than 150, you choose 150-200 most recent observations to complete the assignment. The analysis and components in the analysis should be similar to that in the case study in the class note.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \url{https://www.census.gov/econ/currentdata/datasets/index}
\item
  \url{https://datahub.io/search}
\item
  \url{https://lionbridge.ai/datasets/17-best-finance-economic-datasets-for-machine-learning/}
\end{enumerate}

You can also find your data set from other sites for this assignment.

\hypertarget{exponential-smoothing-methods}{%
\chapter{Exponential Smoothing Methods}\label{exponential-smoothing-methods}}

Exponential smoothing methods are a family of algorithms that forecast future values by using exponentially decreasing weights of historical observations to forecast new values. Therefore as observations get older, the importance of these values diminishes exponentially. The more recent the observation the higher the associated weight. The exponentially decreasing weights are controlled by several smoothing coefficients based on the patterns of the underlying time series.

There are some obvious \textbf{advantages} of exponential smoothing methods:

\begin{itemize}
\item
  Exponential smoothing is very simple in concept and structure. It is also very easy to understand.
\item
  Exponential smoothing is very powerful because of its exponentially-decayed weighting process.
\item
  Exponential smoothing methods including Holt-Winters methods are appropriate for non-stationary data. In fact, they are only really appropriate if the data are non-stationary. Using an exponential smoothing method on stationary data is not wrong but is sub-optimal.
\item
  Because exponential smoothing relies on only two pieces of data: (1). the last period's actual value; (2). the forecast value for the same period. This minimizes the use of random access memory (RAM).
\item
  Exponential smoothing requires minimum intervention in terms of model maintenance, it is can be adapted to make large-scale forecasting.
\end{itemize}

There are also \textbf{limitations} of exponential smoothing methods:

\begin{itemize}
\item
  The method is useful for short-term forecasting only. It assumes that future patterns and trends will not change significantly from the current patterns and trends. This kind of assumption may sound reasonable in the short term. However, it creates problems for the long-term forecast.
\item
  Exponential smoothing will lag. In other words, the forecast will be behind, as the trend increases or decreases over time.
\item
  Exponential smoothing will fail to account for the dynamic changes at work in the real world, and the forecast will constantly require updating to respond to new information.
\end{itemize}

To avoid getting bogged down in too much technical detail for various smoothing methods, we only outline the basic components in exponential smoothing models built on several reliable smoothing algorithms under the ETS framework in the following sections.

\hypertarget{ets-framework}{%
\section{ETS Framework}\label{ets-framework}}

The general exponential smoothing methods combine \textbf{error (E)}, \textbf{trend (T)}, and \textbf{seasonal(S)} components in such a way that the resulting functional forms and relevant smoothing coefficients best fit the historical data. \textbf{Each term} can be combined in one of three different ways: additive (\textbf{A}), multiplicative (\textbf{M}), and None (\textbf{N}). These three terms (\textbf{E}rror, \textbf{T}rend, and \textbf{S}eason) together with the ways of combinations are referred to as the \textbf{ETS} framework. \textbf{ETS} is also called the abbreviation of \textbf{E}xponen\textbf{T}ial \textbf{S}moothing.

The following table summarizes the possible ways to construct smoothing algorithms and forecasting models within the ETS framework.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2791}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3837}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3372}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Error (E)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Trend (T)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Seasonality (S)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Additive: \textbf{A} & Additive: \textbf{A} & Additive: \textbf{A} \\
Multiplicative: \textbf{M} & Multiplicative: \textbf{M} & Multiplicative: \textbf{M} \\
& None: \textbf{N} & None: \textbf{N} \\
& Additive Damped \textbf{Ad} & \\
& Multiplicative damped: \textbf{Md} & \\
\end{longtable}

\hypertarget{exponential-smoothing-methods-1}{%
\subsection{Exponential Smoothing Methods}\label{exponential-smoothing-methods-1}}

The idea of exponential smoothing is to smooth out the random fluctuations to see a clearer signal (trend and cycle, and seasonality). Depending on the pattern of the underlying time series, there are 15 different smoothing methods.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img13/w13-SmoothingMethodsChart} 

}

\caption{ETS smoothing methods}\label{fig:unnamed-chunk-236}
\end{figure}

The patterns in different smoothing methods (except for the damped additive trend) are sketched in the following figure.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img13/w13-ETS-CommonSmoothingMethods} 

}

\caption{ETS smoothing charts}\label{fig:unnamed-chunk-237}
\end{figure}

These patterns can be used in selecting specific smoothing models from the complete list of all possible models outlined in the following section.

\hypertarget{exponential-smoothing-models}{%
\subsection{Exponential Smoothing Models}\label{exponential-smoothing-models}}

The smoothing methods can only produce a point forecast. We can attach random error to the smoothing methods to build statistical forecast models. We can attach the error term to the combined trend and seasonality in the form of addition or multiplication. There are 30 exponential smoothing models.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img13/w13-ETS-30-SmoothingModels} 

}

\caption{ETS smoothing models}\label{fig:unnamed-chunk-238}
\end{figure}

We can use the notation \textbf{ETS(error, trend, seasonality)} to represent different smoothing models. For example, (1) \textbf{ETS(A,N,N)} - simple exponential smoothing model with additive errors, (2) \textbf{ETS(A,A,N)} - additive trend with additive errors (so Holt's linear method with additive errors).

Since some of the ETS smoothing models are unstable, in this note, we only focus on a few commonly used smoothing models:

\begin{itemize}
\item
  \textbf{ETS(A,N,N)}: Simple exponential smoothing with additive errors.
\item
  \textbf{ETS(A,A,N)}: Holt's linear method with additive errors.
\item
  \textbf{ETS(A,A,A)}: Additive Holt-Winters' method with additive errors.
\item
  \textbf{ETS(M,A,M)}: Multiplicative Holt-Winters' method with multiplicative errors.
\item
  \textbf{ETS(A,\(A_d\),N)}: Damped trend method with additive errors.
\end{itemize}

\hypertarget{estimation-smoothing-parameters-in-smoothing-models}{%
\subsection{Estimation Smoothing Parameters in Smoothing Models}\label{estimation-smoothing-parameters-in-smoothing-models}}

There are different ways to estimate the coefficient of the smoothing parameter. One way is to find the smoothing parameters by minimizing the means square error (MSE). This is similar to the least square method. This is a distribution-free method and can be used to automate the exponential smoothing models.

The other method is the \textbf{likelihood} approach. After we include a random error with specific parametric distribution, we can estimate the coefficient parameters by using the likelihood method. We will not discuss estimation methods in detail.

Most of the smoothing models are implemented in the R library \textbf{forecast}. We will use this library to perform data analysis in this note.

\hypertarget{simple-exponential-smoothing}{%
\section{Simple Exponential Smoothing}\label{simple-exponential-smoothing}}

Exponential smoothing is a very popular scheme to produce a smoothed time series and assigns exponentially decreasing weights as the observation gets older. That is, more recent data points affect the forecast trend more heavily than older data points. This unequal weighting is accomplished by using one or more smoothing constants.

Assume we have historical data \(\{Y_1, Y_2, \cdots, Y_T \}\), the forecast value of the next period is

\[
Y_{(T+1)|T} = \alpha Y_T + \alpha(1-\alpha)Y_{T-1} + \alpha(1-\alpha)^2Y_{T-2}+\alpha(1-\alpha)^3Y_{T-3} + \cdots
\]

\(0 < \alpha < 1\) is called \textbf{smoothing coefficients}. The forms of the coefficients in the above expression

It provides a forecasting method that is most effective when the components like trend and seasonal factors of the time series may change over time. Several equivalent formulations of simple exponential smoothing:

\begin{itemize}
\item
  \(Y_{(t+1)|t} = \alpha Y_t + (1-\alpha)Y_{t|(t-1)}\).
\item
  Forecasting form: \(\hat{Y}_{t+1|t} = \mathfrak{l}_t\)
\item
  Smoothing equation: \(\mathfrak{l}_t = \alpha Y_t + (1-\alpha)\mathfrak{l}_{t-1}\)
\item
  Error correction form: \(\mathfrak{l}_t=\mathfrak{l}_{t-1} + \alpha (y_t-\mathfrak{l}_{t-1}) = \mathfrak{l}_{t-1} + \alpha e_{t}\), where \(e_t=y_t - \mathfrak{l}_{t-1} = y_t-\hat{y}_{t|t-1}\).
\end{itemize}

\textbf{Example} We use stock price data to build three models with different smoothing coefficients and then use the accuracy measures to choose the optimal smoothing model. The optimal smoothing coefficient will be identified by the built-in algorithm and will be reported in the output (if the \(\alpha\) is not specified in the model formula).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stock}\OtherTok{=}\FunctionTok{read.table}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww13/w13{-}stockprice.txt"}\NormalTok{)}
\NormalTok{price}\OtherTok{=}\NormalTok{stock}\SpecialCharTok{$}\NormalTok{V1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{35}\NormalTok{]}
\NormalTok{fit1 }\OtherTok{=} \FunctionTok{ses}\NormalTok{(price, }\AttributeTok{alpha=}\FloatTok{0.2}\NormalTok{, }\AttributeTok{initial=}\StringTok{"optimal"}\NormalTok{, }\AttributeTok{h=}\DecValTok{3}\NormalTok{)}
\NormalTok{fit2 }\OtherTok{=} \FunctionTok{ses}\NormalTok{(price, }\AttributeTok{alpha=}\FloatTok{0.6}\NormalTok{, }\AttributeTok{initial=}\StringTok{"simple"}\NormalTok{, }\AttributeTok{h=}\DecValTok{3}\NormalTok{)}
\NormalTok{fit3 }\OtherTok{=} \FunctionTok{ses}\NormalTok{(price, }\AttributeTok{h=}\DecValTok{3}\NormalTok{)  }\DocumentationTok{\#\# alpha is unspecified, it will be estimated}
\FunctionTok{plot}\NormalTok{(fit1,  }\AttributeTok{ylab=}\StringTok{"Stock Price"}\NormalTok{,}
  \AttributeTok{xlab=}\StringTok{"Time"}\NormalTok{, }\AttributeTok{main=}\StringTok{""}\NormalTok{, }\AttributeTok{fcol=}\StringTok{"white"}\NormalTok{, }\AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\StringTok{"Comparing SES Models: Different Smoothinh Coefficients"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit1), }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit2), }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit3), }\AttributeTok{col=}\StringTok{"darkgreen"}\NormalTok{, }\AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(fit1}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{) }\DocumentationTok{\#\# plot forecast values}
\FunctionTok{points}\NormalTok{(fit2}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{pch=}\DecValTok{18}\NormalTok{)}
\FunctionTok{points}\NormalTok{(fit3}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"darkgreen"}\NormalTok{, }\AttributeTok{pch=}\DecValTok{21}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomleft"}\NormalTok{,}\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{,}\StringTok{"darkgreen"}\NormalTok{),}
  \FunctionTok{c}\NormalTok{(}\StringTok{"data"}\NormalTok{, }\FunctionTok{expression}\NormalTok{(alpha }\SpecialCharTok{==} \FloatTok{0.2}\NormalTok{), }\FunctionTok{expression}\NormalTok{(alpha }\SpecialCharTok{==} \FloatTok{0.6}\NormalTok{),}
  \FunctionTok{expression}\NormalTok{(alpha }\SpecialCharTok{==}  \FloatTok{0.9332}\NormalTok{)),}\AttributeTok{pch=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-239-1} 

}

\caption{Comparing simple exponential models with various smoothing coefficients.}\label{fig:unnamed-chunk-239}
\end{figure}

The following table summarizes the accuracy measures based on various smoothing models with different smoothing coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accuracy.table }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(}\FunctionTok{accuracy}\NormalTok{(fit1), }\FunctionTok{accuracy}\NormalTok{(fit2), }\FunctionTok{accuracy}\NormalTok{(fit3)),}\DecValTok{4}\NormalTok{)}
\FunctionTok{row.names}\NormalTok{(accuracy.table)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"alpha=0.2"}\NormalTok{, }\StringTok{"alpha=0.6"}\NormalTok{, }\StringTok{"optimal alpha = 0.09332"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(accuracy.table, }\AttributeTok{caption =} \StringTok{"The accuracy measures of simple exponential }
\StringTok{      smoothing models with different smoothing coefficients."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-240}The accuracy measures of simple exponential 
      smoothing models with different smoothing coefficients.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r}
\hline
  & ME & RMSE & MAE & MPE & MAPE & MASE & ACF1\\
\hline
alpha=0.2 & -0.2278 & 1.3663 & 1.0369 & -1.1287 & 4.0286 & 1.9361 & 0.6838\\
\hline
alpha=0.6 & -0.2013 & 0.9974 & 0.5895 & -0.8835 & 2.2956 & 1.1006 & 0.3024\\
\hline
optimal alpha = 0.09332 & -0.1320 & 0.9421 & 0.5158 & -0.5833 & 1.9991 & 0.9630 & -0.0126\\
\hline
\end{tabular}
\end{table}

\textbf{Remark}: The above measures are based on the training data (i.e., based on the observed values and the fitted values). We can also hold up test data to calculate the actual accuracy measures.

\hypertarget{holts-smoothing-model-linear-additive-trend}{%
\section{Holt's Smoothing Model: Linear (additive) Trend}\label{holts-smoothing-model-linear-additive-trend}}

Holt generalized simple exponential smoothing by adding a trend parameter to allow the forecasting of data with a linear trend. The model components are given in the following:

\begin{itemize}
\item
  Forecast function: \(\hat{y}_{t+h|t} = \mathfrak{l}_t + h b_t\).
\item
  Level: \(\mathfrak{l}_t = \alpha y_t + (1-\alpha)(\mathfrak{l}_{t-1} + b_{t-1})\).
\item
  Trend: \(b_t=\beta^*(\mathfrak{l}_t-\mathfrak{l}_{t-1}) + (1-\beta^*)b_{t-1}\).
\item
  Error: \(e_t = y_t-{\mathfrak{l}_{t-1} + b_t} = y_t-\hat{y}_{t|t-1}\).
\end{itemize}

where \(\alpha\) is the smoothing coefficient for level and \(\beta^*\) the smoothing coefficient of trend. The smoothing coefficients are estimated by minimizing the sum of the squared error (SSE) of the model.

The R function \textbf{holt()} computes the smoothing coefficients and forecasts future values for a given \(h\) period of future values.

\textbf{Example 2}: We apply Holt's method to annual passenger numbers for Australian airlines from 1990 to 2016.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ausair}\OtherTok{=}\FunctionTok{read.table}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww13/w13{-}ausair.txt"}\NormalTok{)[,}\DecValTok{3}\NormalTok{]}
\NormalTok{air }\OtherTok{=} \FunctionTok{ts}\NormalTok{(ausair,}\AttributeTok{start=}\DecValTok{1990}\NormalTok{,}\AttributeTok{end=}\DecValTok{2016}\NormalTok{)}
\NormalTok{fit0 }\OtherTok{=} \FunctionTok{holt}\NormalTok{(air, }\AttributeTok{initial=}\StringTok{"simple"}\NormalTok{, }\AttributeTok{exponential=}\ConstantTok{TRUE}\NormalTok{,}\AttributeTok{h=}\DecValTok{5}\NormalTok{)  }\DocumentationTok{\#\#\# optimal alpha and beta}
\NormalTok{fit1 }\OtherTok{=} \FunctionTok{holt}\NormalTok{(air, }\AttributeTok{alpha=}\FloatTok{0.8}\NormalTok{, }\AttributeTok{beta=}\FloatTok{0.2}\NormalTok{, }\AttributeTok{exponential=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{initial=}\StringTok{"simple"}\NormalTok{, }\AttributeTok{h=}\DecValTok{5}\NormalTok{)}
\DocumentationTok{\#\#\#\#\#\# Plot the original data}
\FunctionTok{plot}\NormalTok{(fit0, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Air Passengers"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Time"}\NormalTok{,}
     \AttributeTok{fcol=}\StringTok{"white"}\NormalTok{, }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{110}\NormalTok{))}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit0), }\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit1), }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\CommentTok{\#points(fit0, col="black", pch=1)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit0), }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.6}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit1), }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{pch=}\DecValTok{22}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.6}\NormalTok{)}
\DocumentationTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\FunctionTok{lines}\NormalTok{(fit0}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{type=}\StringTok{"o"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit1}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{type=}\StringTok{"o"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{,}\StringTok{"black"}\NormalTok{,}\StringTok{"blue"}\NormalTok{),}\AttributeTok{pch=}\FunctionTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{22}\NormalTok{),}
   \FunctionTok{c}\NormalTok{(}\StringTok{"Holt\textquotesingle{}s Exp Trend(optimal)"}\NormalTok{,}\StringTok{"Data"}\NormalTok{,}\StringTok{"Holt\textquotesingle{}s Exp trend"}\NormalTok{), }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-241-1} 

}

\caption{ Comparing Holt's exponential trend models with various smoothing coefficients.}\label{fig:unnamed-chunk-241}
\end{figure}

As expected, we can see from the above chart that

\begin{itemize}
\item
  the smoothing model with exponential trend works equally well as the linear trend model for air passenger data;
\item
  the smoothing model with exponential trend works better than the linear trend model for the beverage data;
\end{itemize}

\hypertarget{damped-trend-methods}{%
\section{Damped Trend Methods}\label{damped-trend-methods}}

Empirical evidence indicates that these methods tend to over-forecast, especially for longer forecast horizons. Gardner's damped trend models are shown to be effective in improving accuracy for prediction.

To capture the damped trend (the trend component curve flattens over time instead of being linear), in addition to the two smoothing parameters \(\alpha\) and \(\beta^*\) in linear and exponential trend models, Gardener added a third parameter \(\phi\) (\(0< \phi < 1\)) that damps the trend as h gets bigger.

\textbf{For an additive (linear) trend model}, the additional parameter is added in the model component in the following form.

\begin{itemize}
\item
  Forecast Model: \(\hat{y}_{t+h|t} = \mathfrak{l}_t + (1 + \phi+\phi^2|\cdots+\phi^h)b_t\)
\item
  Level: \(\mathfrak{l}_t = \alpha y_t + (1-\alpha)(\mathfrak{l}_{t-1}+\phi b_{t-1})\)
\item
  Trend: \(b_t = \beta^* (\mathfrak{l}_t-\mathfrak{l}_{t-1}) + (1-\beta^*)\phi b_{t-1}\). where \(\phi\) is called the \textbf{damping parameter}.
\end{itemize}

Let error \(e_t = y_t-(\mathfrak{l}_{t-1}+b_{t-1})=y_t-\hat{y}_{t|t-1}\), then the level and trend can be re-expressed as

\begin{itemize}
\item
  Level: \(\mathfrak{l}_t = \mathfrak{l}_{t-1} + \phi b_{t-1} + \alpha e_t\).
\item
  Additive Trend: \(b_t=\phi b_{t-1} + \alpha \beta^* e_t\).
\end{itemize}

\textbf{For a multiplicative (exponential) trend mode}, the additional parameter is added in the model component in the following form.

\begin{itemize}
\item
  Forecast Model: \(\hat{y}_{t+h|t} = \mathfrak{l}_t b_{t}^{(1 + \phi+\phi^2|\cdots+\phi^h)}\).
\item
  Level: \(\mathfrak{l}_t = \alpha y_t + (1-\alpha)\mathfrak{l}_{t-1}b_{t-1}^\phi\)
\item
  Multiplicative Trend: \(b_t=\beta^*\mathfrak{l}_t/\mathfrak{l}_{t-1} + (1-\beta^*) b_{t-1}^\phi\).
\end{itemize}

Since this is a modification of the smoothing model with the exponential trend, \(b_t\) and \(h\) are interpreted in the same way as in the exponential trend smoothing model and \(\phi\) is between 0 and 1, exclusively.

Let error \(e_t = y_t - \hat{y}_{t-1|t} = y_t-\mathfrak{l}_{t-1}b_{t-1}\). The corresponding error correction form is given by

\begin{itemize}
\item
  Level: \(\mathfrak{l}_t = \alpha e_t + \mathfrak{l}_{t-1} b_{t-1}^\phi\).
\item
  Multiplicative Trend: \(b_t = b_{t-1}^\phi + \alpha\beta^* e_t/\mathfrak{l}_{t-1}\)
\end{itemize}

\textbf{Example 3}: We use Australian GDP data as an example to illustrate the application of the R function holt() for the damped trend models (both additive and multiplicative models).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ausgdp }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww13/w13{-}ausgdp.txt"}\NormalTok{)}
\NormalTok{ausgdp0}\OtherTok{=}\FunctionTok{ts}\NormalTok{(ausgdp}\SpecialCharTok{$}\NormalTok{V1,}\AttributeTok{frequency=}\DecValTok{4}\NormalTok{,}\AttributeTok{start=}\DecValTok{1971}\SpecialCharTok{+}\DecValTok{2}\SpecialCharTok{/}\DecValTok{4}\NormalTok{)}
\NormalTok{ausgdp1}\OtherTok{=}\NormalTok{ausgdp}\SpecialCharTok{$}\NormalTok{V1}
\NormalTok{fit1 }\OtherTok{=} \FunctionTok{ses}\NormalTok{(ausgdp1)}
\NormalTok{fit2 }\OtherTok{=} \FunctionTok{holt}\NormalTok{(ausgdp1)}
\NormalTok{fit3 }\OtherTok{=} \FunctionTok{holt}\NormalTok{(ausgdp1,}\AttributeTok{exponential=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{fit4 }\OtherTok{=} \FunctionTok{holt}\NormalTok{(ausgdp1,}\AttributeTok{damped=}\ConstantTok{TRUE}\NormalTok{)      }\DocumentationTok{\#\# additive damping}
\NormalTok{fit5 }\OtherTok{=} \FunctionTok{holt}\NormalTok{(ausgdp1,}\AttributeTok{exponential=}\ConstantTok{TRUE}\NormalTok{,}\AttributeTok{damped=}\ConstantTok{TRUE}\NormalTok{)  }\DocumentationTok{\#\# multiplicative dampling}
\DocumentationTok{\#\#\#}
\FunctionTok{plot}\NormalTok{(fit3, }\AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Australia GDP"}\NormalTok{,}\AttributeTok{flwd=}\DecValTok{1}\NormalTok{, }
\AttributeTok{main=}\StringTok{"Comparison of various smoothing models"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit1),}\AttributeTok{col=}\DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit2),}\AttributeTok{col=}\DecValTok{3}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit4),}\AttributeTok{col=}\DecValTok{5}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit5),}\AttributeTok{col=}\DecValTok{6}\NormalTok{)}
\DocumentationTok{\#\#\#\#\#\#}
\FunctionTok{lines}\NormalTok{(fit1}\SpecialCharTok{$}\NormalTok{mean,}\AttributeTok{col=}\DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit2}\SpecialCharTok{$}\NormalTok{mean,}\AttributeTok{col=}\DecValTok{3}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit4}\SpecialCharTok{$}\NormalTok{mean,}\AttributeTok{col=}\DecValTok{5}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit5}\SpecialCharTok{$}\NormalTok{mean,}\AttributeTok{col=}\DecValTok{6}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{pch=}\DecValTok{1}\NormalTok{, }\AttributeTok{col=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{,}
    \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{,}\StringTok{"SES"}\NormalTok{,}\StringTok{"Holt\textquotesingle{}s"}\NormalTok{,}\StringTok{"Exponential"}\NormalTok{,}
      \StringTok{"Additive Damped"}\NormalTok{,}\StringTok{"Multiplicative Damped"}\NormalTok{), }\AttributeTok{cex=}\FloatTok{0.8}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-242-1} 

}

\caption{Comparing Holt's exponential damped trend models.}\label{fig:unnamed-chunk-242}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accuracy.table }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(}\FunctionTok{accuracy}\NormalTok{(fit1), }\FunctionTok{accuracy}\NormalTok{(fit2), }\FunctionTok{accuracy}\NormalTok{(fit3), }
                             \FunctionTok{accuracy}\NormalTok{(fit4), }\FunctionTok{accuracy}\NormalTok{(fit5)),}\DecValTok{4}\NormalTok{)}
\FunctionTok{row.names}\NormalTok{(accuracy.table)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"SES"}\NormalTok{,}\StringTok{"Holt\textquotesingle{}s"}\NormalTok{,}\StringTok{"Exponential"}\NormalTok{,}
                    \StringTok{"Additive Damped"}\NormalTok{,}\StringTok{"Multiplicative Damped"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(accuracy.table, }\AttributeTok{caption =} \StringTok{"The accuracy measures of various exponential }
\StringTok{      smoothing models"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-243}The accuracy measures of various exponential 
      smoothing models}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r}
\hline
  & ME & RMSE & MAE & MPE & MAPE & MASE & ACF1\\
\hline
SES & 22.8967 & 38.5522 & 32.1076 & 0.4518 & 0.6392 & 0.9738 & 0.5930\\
\hline
Holt's & -1.1647 & 31.0537 & 26.0678 & -0.0248 & 0.5227 & 0.7906 & 0.6132\\
\hline
Exponential & -0.6055 & 31.2007 & 26.2099 & -0.0140 & 0.5247 & 0.7949 & 0.6100\\
\hline
Additive Damped & -0.8253 & 31.4965 & 25.5226 & -0.0232 & 0.5113 & 0.7740 & 0.6218\\
\hline
Multiplicative Damped & 0.6962 & 31.3254 & 25.7591 & 0.0084 & 0.5160 & 0.7812 & 0.6189\\
\hline
\end{tabular}
\end{table}

From the above accuracy table, we can see the additive and multiplicative damped trend models are better than other models. The additive damped model is marginally better than the multiplicative damped model.

\hypertarget{holt-winters-model-for-trend-and-seasonal-data}{%
\section{Holt-Winters Model for Trend and Seasonal Data}\label{holt-winters-model-for-trend-and-seasonal-data}}

Holt and Winter's exponential smoothing method is used to deal with time series containing both trend and seasonal variation. There are two Holt-Winter (HW) models: Additive and Multiplicative models.

\textbf{The additive method} is preferred when the seasonal variations are roughly constant through the series, while \textbf{the multiplicative method} is preferred when the seasonal variations are changing proportionally to the level of the series.

The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations --- one for the level \(\mathfrak{l}_t\), one for trend \(b_t\), and one for the seasonal component denoted by \(s_t\), with smoothing parameters \(\alpha\), \(\beta^*\) and \(\gamma\).

\textbf{Additive Holt-Winters Model} Components of additive HW model:

\begin{itemize}
\item
  Level: \(\mathfrak{l}_t = \alpha(y_t-s_{t-s})+(1-\alpha)(\mathfrak{l}_{t-1} + b_{t-1})\).
\item
  Trend: \(b_t=\beta^*(\mathfrak{l}_t-\mathfrak{l}_{t-1})+(1-\beta^*)b_{t-1}\).
\item
  Seasonality: \(s_t=\gamma(y_t-\mathfrak{l}_{t-1}) +(1-\gamma)s_{t-s}\).
\item
  Forecast Model: \(\hat{y}_{t+h|t} = \mathfrak{l}_t + hb_t + s_{t-s+h}\).
\end{itemize}

where \(s\) is the length of the seasonal cycle, for \(0 \le \alpha \le 1\) , \(0 \le \beta^* \le 1\), and \(0 \le \gamma \le 1\). \(h\) is the number of periods to be predicted.

Let \(e_t=\hat{y}_t - \hat{y}_{t|t-1} = y_t-(\mathfrak{l}_{t-1} + b_{t-1} + s_{t-1})\) be the one-step training error, we can re-express level, trend and seasonality equation in the following form:

\begin{itemize}
\item
  Level: \(\mathfrak{l}_t= \mathfrak{l}_{t-1} + b_{t-1} + \alpha e_t\).
\item
  Trend: \(b_t = b_{t-1} + \alpha \beta^* e_t\).
\item
  Seasonality: \(s_t = s_{t-2} + \gamma e_t\).
\end{itemize}

Interpretations of individual components:

\textbf{Level}: the current level is the weighted average of the difference between the current observation and previous seasonality and the sum of the previous level and trend.

\textbf{Trend}: the current trend is the weighted average of the previous trend and the difference between the current level and the previous level.

\textbf{Seasonality}: the current seasonality is the weighted average of the previous seasonality and the difference between the current observation and the current level.

\textbf{Multiplicative Holt-Winters Model}: An alternative Holt-Winter's model multiplies the forecast by a seasonal factor. Its equations are:

\begin{itemize}
\item
  Level: \(\mathfrak{l}_t=\alpha y_t/s_{t-s} + (1-\alpha)(\mathfrak{l}_{t-1}+b_{t01})\).
\item
  Trend: \(b_t = \beta^*(\mathfrak{l}_t - \mathfrak{l}_{t-1}) + (1-\beta^*)b_{t-1}\).
\item
  Seasonality: \(s_t = \gamma y_t/(\mathfrak{l}_{t-1}+b_{t-1}) + (1-\gamma)s_{t-s}\).
\item
  Forecast Model: \(\hat{y}_{t+h|t} = (\mathfrak{l}_t + h b_t)s_{t-s-h}\).
\end{itemize}

where \(s\) is the length of the seasonal cycle, for \(0 \le \alpha \le 1\) , \(0 \le \beta^* \le 1\), and \(0 \le \gamma \le 1\). \(h\) is the number of periods to be predicted.

Similarly, we denote the one-step error for the multiplicative model to be \(e_t=\hat{y}_t-\hat{y}_{t|t-1} = y_t -(\mathfrak{l}_{t-1}+b_{t-1} + s_{s-t})\), the error correction representation of the model can be written as

\begin{itemize}
\item
  Level: \(\mathfrak{l}_t = \mathfrak{l}_{t-1} + b_{t-1} + \alpha e_t/s_{t-s}\).
\item
  Trend: \(b_t = b_{t-1} + \alpha\beta^* e_t / s_{t-s}\).
\item
  Seasonality: \(s_t = s_{t-s} + \gamma e_t /(\mathfrak{l}_{t-1}+b_{t-1})\).
\end{itemize}

\textbf{Holt-Winters Model with a Damped Trend and Multiplicative Seasonality}. This is a simple modification of the HW multiplicative model with a damped trend. The model formulation is given below

\begin{itemize}
\item
  Level: \(\mathfrak{l}_t = \alpha y_t/s_{t-s} + (1-\alpha)(\mathfrak{l}_{t-1}+\phi b_{t-1})\).
\item
  Trend: \(b_t = \beta^*(\mathfrak{l}_t - \mathfrak{l}_{t-1}) + (1-\beta^*)\phi b_{t-1}\).
\item
  Seasonality: \(s_t = \gamma y_t/(\mathfrak{l}_{t-1}+\phi b_{t-1}) + (1-\gamma)s_{t-s}\).
\item
  Forecast Model: \(\hat{y}_{t+h|t}=\left[\mathfrak{l}_t +(1 + \phi +\phi^2+ \cdots + \phi^h)b_t \right]s_{t-s+h}\).
\end{itemize}

where \(0< \phi < 1\).

Three R functions can be used to fit Holt-Winters model: \textbf{hw()},\textbf{ets()}, and \textbf{HoltWinters()} in package \textbf{\{forecast\}}.

\begin{itemize}
\item
  The estimation of parameters used in function \textbf{HoltWinters()} uses \textbf{optim()} which requires the initial values for the parameters \(\alpha\), \(\beta^*\) and \(\gamma\). The default initial values of these parameters are 0.3, 0.1, and 0.1. You can provide your own more accurate values. \textbf{HoltWinters()} is using heuristic values for the initial states and then estimating the smoothing parameters by optimizing the MSE.
\item
  \textbf{ets()} also use \textbf{optim()} to estimate the parameters and the initial states by optimizing the likelihood function (which is only equivalent to optimizing the MSE for the linear additive models).
\item
  \textbf{hw()} has the option to fit a HW model with a damped trend. \textbf{HoltWinters()} does not have the option.
\end{itemize}

\textbf{Example 4}: We use a simulated time series to illustrate how to R functions to fit various HW models.

\textbf{HW models}: additive, multiplicative and damped HW models with R function \textbf{hw()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat01}\OtherTok{=}\FunctionTok{read.table}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww13/w13{-}ts3.txt"}\NormalTok{)}
\NormalTok{datset}\OtherTok{=}\FunctionTok{ts}\NormalTok{(dat01}\SpecialCharTok{$}\NormalTok{V1}\SpecialCharTok{+}\DecValTok{55}\NormalTok{, }\AttributeTok{start=}\DecValTok{2000}\NormalTok{, }\AttributeTok{frequency=}\DecValTok{12}\NormalTok{)}
\DocumentationTok{\#\# Model building}
\NormalTok{fit1 }\OtherTok{=} \FunctionTok{hw}\NormalTok{(datset,}\AttributeTok{h=}\DecValTok{12}\NormalTok{, }\AttributeTok{seasonal=}\StringTok{"additive"}\NormalTok{)   }\CommentTok{\# default h = 10}
\NormalTok{fit2 }\OtherTok{=} \FunctionTok{hw}\NormalTok{(datset,}\AttributeTok{h=}\DecValTok{12}\NormalTok{, }\AttributeTok{seasonal=}\StringTok{"multiplicative"}\NormalTok{)}
\NormalTok{fit3 }\OtherTok{=} \FunctionTok{hw}\NormalTok{(datset,}\AttributeTok{h=}\DecValTok{12}\NormalTok{, }\AttributeTok{seasonal=}\StringTok{"additive"}\NormalTok{,}\AttributeTok{damped=}\ConstantTok{TRUE}\NormalTok{)   }
\DocumentationTok{\#\#\# plots}
\FunctionTok{plot}\NormalTok{(fit2,}\AttributeTok{ylab=}\StringTok{"Artificial Series"}\NormalTok{,}\AttributeTok{main=}\StringTok{"Various Holt{-}Winters Models"}\NormalTok{,}
     \AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{fcol=}\StringTok{"white"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Year"}\NormalTok{, }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{300}\NormalTok{), }\AttributeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit1), }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit2), }\AttributeTok{col=}\StringTok{"green"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit3), }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{lines}\NormalTok{(fit1}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit2}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{col=}\StringTok{"green"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit3}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\DocumentationTok{\#\#\#}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{pch=}\DecValTok{1}\NormalTok{, }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{),}
  \FunctionTok{c}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"Holt Winters\textquotesingle{} Additive"}\NormalTok{,}\StringTok{"Holt Winters\textquotesingle{} Multiplicative"}\NormalTok{,}
    \StringTok{"HW Additive Damped Trend"}\NormalTok{), }\AttributeTok{cex=}\FloatTok{0.7}\NormalTok{, }\AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-244-1} 

}

\caption{ Comparing Holt-Winter's (damped) trend and seasonal models.}\label{fig:unnamed-chunk-244}
\end{figure}

The accuracy measures of the three models are summarized in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accuracy.table }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(}\FunctionTok{accuracy}\NormalTok{(fit1), }\FunctionTok{accuracy}\NormalTok{(fit2), }\FunctionTok{accuracy}\NormalTok{(fit3)),}\DecValTok{4}\NormalTok{)}
\FunctionTok{row.names}\NormalTok{(accuracy.table)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"Holt Winters\textquotesingle{} Additive"}\NormalTok{,}\StringTok{"Holt Winters\textquotesingle{} Multiplicative"}\NormalTok{,}
    \StringTok{"HW Additive Damped Trend"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(accuracy.table, }\AttributeTok{caption =} \StringTok{"The accuracy measures of various Holt{-}Winter\textquotesingle{}s }
\StringTok{        exponential smoothing models"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-245}The accuracy measures of various Holt-Winter's 
        exponential smoothing models}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r}
\hline
  & ME & RMSE & MAE & MPE & MAPE & MASE & ACF1\\
\hline
Holt Winters' Additive & 0.2052 & 12.5660 & 9.9395 & -4.4789 & 13.3916 & 0.4483 & 0.3564\\
\hline
Holt Winters' Multiplicative & 0.1987 & 14.6071 & 11.8053 & -4.5487 & 16.6051 & 0.5325 & 0.4842\\
\hline
HW Additive Damped Trend & 1.2919 & 12.6380 & 10.0841 & -3.1055 & 13.2677 & 0.4549 & 0.3544\\
\hline
\end{tabular}
\end{table}

We can see from the above table that Holt-Winter's additive model outperformed the other two models.

\textbf{HW models: additive, multiplicative, and damped HW models with R function HoltWinters()}. Since \textbf{HoltWinters()} does not have the option to make predict values directly. We need to use the R function \textbf{predict.HoltWinters()} to make the prediction.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat01}\OtherTok{=}\FunctionTok{read.table}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww13/w13{-}ts3.txt"}\NormalTok{)}
\NormalTok{datset}\OtherTok{=}\FunctionTok{ts}\NormalTok{(dat01}\SpecialCharTok{$}\NormalTok{V1}\SpecialCharTok{+}\DecValTok{55}\NormalTok{, }\AttributeTok{start=}\DecValTok{2000}\NormalTok{, }\AttributeTok{frequency=}\DecValTok{12}\NormalTok{)}
\DocumentationTok{\#\# Model building}
\NormalTok{fit1 }\OtherTok{=} \FunctionTok{HoltWinters}\NormalTok{(datset, }\AttributeTok{seasonal=}\StringTok{"additive"}\NormalTok{)   }\CommentTok{\# default h = 10}
\NormalTok{fit2 }\OtherTok{=} \FunctionTok{HoltWinters}\NormalTok{(datset, }\AttributeTok{seasonal=}\StringTok{"multiplicative"}\NormalTok{)}
\DocumentationTok{\#\#\# plots}
\FunctionTok{plot}\NormalTok{(fit1,}\AttributeTok{ylab=}\StringTok{"Artificial Series"}\NormalTok{,}\AttributeTok{main=}\StringTok{"Various Holt{-}Winters Models"}\NormalTok{, }
     \AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Year"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\CommentTok{\#lines(fitted(fit1)[,1], col="red", lty=2)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit2)[,}\DecValTok{1}\NormalTok{], }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\DocumentationTok{\#\#\#}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{pch=}\DecValTok{1}\NormalTok{, }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{),}
  \FunctionTok{c}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"Holt Winters\textquotesingle{} Additive"}\NormalTok{,}\StringTok{"Holt Winters\textquotesingle{} Multiplicative"}\NormalTok{),}
  \AttributeTok{cex =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-246-1} 

}

\caption{Comparing Holt's exponential trend and seasonal models.}\label{fig:unnamed-chunk-246}
\end{figure}

The accuracy measures of the three models are summarized in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(fit1, }\DecValTok{50}\NormalTok{, }\AttributeTok{prediction.interval =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{pred2 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(fit2, }\DecValTok{50}\NormalTok{, }\AttributeTok{prediction.interval =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(fit1, pred, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{main=}\StringTok{"HW Models with R Function HoltWinters()"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred[,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{1}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred[,}\DecValTok{3}\NormalTok{], }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{1}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(fit2)[,}\DecValTok{1}\NormalTok{], }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred2[,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred2[,}\DecValTok{1}\NormalTok{], }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred2[,}\DecValTok{3}\NormalTok{], }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{),}
  \FunctionTok{c}\NormalTok{(}\StringTok{"data"}\NormalTok{,}\StringTok{"Holt Winters\textquotesingle{} Additive"}\NormalTok{,}\StringTok{"Holt Winters\textquotesingle{} Multiplicative"}\NormalTok{),}
  \AttributeTok{cex =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-247-1} 

}

\caption{Exponential damped trend models with Holt-Winters Filtering.}\label{fig:unnamed-chunk-247}
\end{figure}

The above figure shows that the original series is a typical exponential trend with a seasonal pattern. As expected, the Holt-Winters additive model performed poorly (a very wide prediction band).

\hypertarget{case-study-3}{%
\section{Case study}\label{case-study-3}}

In this case study, we use the beverage consumption data to compare various models introduced in this note.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beverage }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww13/w13{-}beverage01.txt"}\NormalTok{)}
\NormalTok{test.bev }\OtherTok{=}\NormalTok{ beverage}\SpecialCharTok{$}\NormalTok{V1[}\DecValTok{169}\SpecialCharTok{:}\DecValTok{180}\NormalTok{]}
\NormalTok{train.bev }\OtherTok{=}\NormalTok{ beverage}\SpecialCharTok{$}\NormalTok{V1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{168}\NormalTok{]}
\NormalTok{bev}\OtherTok{=}\FunctionTok{ts}\NormalTok{(beverage}\SpecialCharTok{$}\NormalTok{V1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{168}\NormalTok{], }\AttributeTok{start=}\DecValTok{2000}\NormalTok{, }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{)}
\NormalTok{fit1 }\OtherTok{=} \FunctionTok{ses}\NormalTok{(bev, }\AttributeTok{h=}\DecValTok{12}\NormalTok{)}
\NormalTok{fit2 }\OtherTok{=} \FunctionTok{holt}\NormalTok{(bev, }\AttributeTok{initial=}\StringTok{"optimal"}\NormalTok{, }\AttributeTok{h=}\DecValTok{12}\NormalTok{)             }\DocumentationTok{\#\# optimal alpha and beta}
\NormalTok{fit3 }\OtherTok{=} \FunctionTok{holt}\NormalTok{(bev,}\AttributeTok{damped=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{h=}\DecValTok{12}\NormalTok{ )                   }\DocumentationTok{\#\# additive damping}
\NormalTok{fit4 }\OtherTok{=} \FunctionTok{holt}\NormalTok{(bev,}\AttributeTok{exponential=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{damped=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{h =}\DecValTok{12}\NormalTok{) }\DocumentationTok{\#\# multiplicative damp}
\NormalTok{fit5 }\OtherTok{=} \FunctionTok{hw}\NormalTok{(bev,}\AttributeTok{h=}\DecValTok{12}\NormalTok{, }\AttributeTok{seasonal=}\StringTok{"additive"}\NormalTok{)              }\DocumentationTok{\#\# default h = 10}
\NormalTok{fit6 }\OtherTok{=} \FunctionTok{hw}\NormalTok{(bev,}\AttributeTok{h=}\DecValTok{12}\NormalTok{, }\AttributeTok{seasonal=}\StringTok{"multiplicative"}\NormalTok{)}
\NormalTok{fit7 }\OtherTok{=} \FunctionTok{hw}\NormalTok{(bev,}\AttributeTok{h=}\DecValTok{12}\NormalTok{, }\AttributeTok{seasonal=}\StringTok{"additive"}\NormalTok{,}\AttributeTok{damped=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{fit8 }\OtherTok{=} \FunctionTok{hw}\NormalTok{(bev,}\AttributeTok{h=}\DecValTok{12}\NormalTok{, }\AttributeTok{seasonal=}\StringTok{"multiplicative"}\NormalTok{,}\AttributeTok{damped=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accuracy.table }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(}\FunctionTok{accuracy}\NormalTok{(fit1), }\FunctionTok{accuracy}\NormalTok{(fit2), }\FunctionTok{accuracy}\NormalTok{(fit3), }\FunctionTok{accuracy}\NormalTok{(fit4),}
                             \FunctionTok{accuracy}\NormalTok{(fit5), }\FunctionTok{accuracy}\NormalTok{(fit6), }\FunctionTok{accuracy}\NormalTok{(fit7), }\FunctionTok{accuracy}\NormalTok{(fit8)),}\DecValTok{4}\NormalTok{)}
\FunctionTok{row.names}\NormalTok{(accuracy.table)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"SES"}\NormalTok{,}\StringTok{"Holt Linear"}\NormalTok{,}\StringTok{"Holt Add. Damped"}\NormalTok{, }\StringTok{"Holt Exp. Damped"}\NormalTok{,}
                            \StringTok{"HW Add."}\NormalTok{,}\StringTok{"HW Exp."}\NormalTok{,}\StringTok{"HW Add. Damp"}\NormalTok{, }\StringTok{"HW Exp. Damp"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(accuracy.table, }\AttributeTok{caption =} \StringTok{"The accuracy measures of various exponential smoothing models }
\StringTok{      based on the training data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-249}The accuracy measures of various exponential smoothing models 
      based on the training data}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r}
\hline
  & ME & RMSE & MAE & MPE & MAPE & MASE & ACF1\\
\hline
SES & 16.5017 & 309.4093 & 257.6429 & 0.1583 & 5.0742 & 1.3090 & 0.0043\\
\hline
Holt Linear & -2.5111 & 312.4926 & 262.2489 & -0.2495 & 5.2023 & 1.3324 & -0.0007\\
\hline
Holt Add. Damped & 10.8546 & 311.8373 & 260.8383 & 0.0082 & 5.1661 & 1.3252 & -0.0016\\
\hline
Holt Exp. Damped & 11.6379 & 312.2482 & 261.0031 & 0.0305 & 5.1699 & 1.3261 & -0.0029\\
\hline
HW Add. & -8.3236 & 117.5757 & 91.9541 & -0.2135 & 1.7920 & 0.4672 & -0.0215\\
\hline
HW Exp. & 2.3822 & 121.7157 & 95.9449 & -0.0051 & 1.8727 & 0.4875 & 0.1052\\
\hline
HW Add. Damp & 11.7728 & 118.5088 & 93.8735 & 0.1786 & 1.8225 & 0.4769 & -0.0376\\
\hline
HW Exp. Damp & 11.9044 & 118.6752 & 92.4620 & 0.1635 & 1.7994 & 0.4698 & -0.0900\\
\hline
\end{tabular}
\end{table}

The above table shows that the HW additive seems to be the most appropriate.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\DocumentationTok{\#\#\#\#\#\# plot the original data}
\NormalTok{pred.id }\OtherTok{=} \DecValTok{169}\SpecialCharTok{:}\DecValTok{180}
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{168}\NormalTok{, train.bev, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Beverage"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }
     \AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{180}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{2500}\NormalTok{, }\DecValTok{7500}\NormalTok{), }\AttributeTok{cex=}\FloatTok{0.3}\NormalTok{,}
     \AttributeTok{main=}\StringTok{"Non{-}seasonal Smoothing Models"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred.id, fit1}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred.id, fit2}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred.id, fit3}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"purple"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred.id, fit4}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"navy"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{points}\NormalTok{(pred.id, fit1}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(pred.id, fit2}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{pch=}\DecValTok{17}\NormalTok{, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(pred.id, fit3}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{col=}\StringTok{"purple"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(pred.id, fit4}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{pch=}\DecValTok{21}\NormalTok{, }\AttributeTok{col=}\StringTok{"navy"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\CommentTok{\#points(fit0, col="black", pch=1)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}\StringTok{"purple"}\NormalTok{, }\StringTok{"navy"}\NormalTok{),}\AttributeTok{pch=}\FunctionTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{19}\NormalTok{,}\DecValTok{21}\NormalTok{),}
   \FunctionTok{c}\NormalTok{(}\StringTok{"SES"}\NormalTok{,}\StringTok{"Holt Linear"}\NormalTok{,}\StringTok{"Holt Linear Damped"}\NormalTok{, }\StringTok{"Holt Multiplicative Damped"}\NormalTok{), }
   \AttributeTok{cex =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\DocumentationTok{\#\#\#\#\#\#\#\#\#\#\#}
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{168}\NormalTok{, train.bev, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{type=}\StringTok{"o"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Beverage"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{""}\NormalTok{, }
     \AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{180}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{2500}\NormalTok{, }\DecValTok{7500}\NormalTok{), }\AttributeTok{cex=}\FloatTok{0.3}\NormalTok{,}
     \AttributeTok{main=}\StringTok{"Holt{-}Winterd Teend and Seasonal Smoothing Models"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred.id, fit5}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred.id, fit6}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred.id, fit7}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"purple"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pred.id, fit8}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{col=}\StringTok{"navy"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{points}\NormalTok{(pred.id, fit5}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(pred.id, fit6}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{pch=}\DecValTok{17}\NormalTok{, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(pred.id, fit7}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{col=}\StringTok{"purple"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{points}\NormalTok{(pred.id, fit8}\SpecialCharTok{$}\NormalTok{mean, }\AttributeTok{pch=}\DecValTok{21}\NormalTok{, }\AttributeTok{col=}\StringTok{"navy"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\DocumentationTok{\#\#\#}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}\StringTok{"purple"}\NormalTok{, }\StringTok{"navy"}\NormalTok{),}\AttributeTok{pch=}\FunctionTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{19}\NormalTok{,}\DecValTok{21}\NormalTok{),}
   \FunctionTok{c}\NormalTok{(}\StringTok{"HW Additive"}\NormalTok{,}\StringTok{"HW Multiplicative"}\NormalTok{,}\StringTok{"HW Additive Damped"}\NormalTok{, }\StringTok{"HW Multiplicative Damped"}\NormalTok{), }
   \AttributeTok{cex =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA321EB_files/figure-latex/unnamed-chunk-250-1} 

}

\caption{Case study: Comparing various exponential smoothing models.}\label{fig:unnamed-chunk-250}
\end{figure}

We can see from the above accuracy table that HW's linear trend with an additive seasonal model is the best of the eight smoothing models. This is consistent with the patterns in the original serial plot.

Since we train the model with the training data and identify the best model using both training and testing data. Both methods yield the same results. To use the model for real-forecast, we need to refit the model using the entire data to update the smoothing parameters in the final working model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acc.fun }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(test.data, mod.obj)\{}
\NormalTok{  PE}\OtherTok{=}\DecValTok{100}\SpecialCharTok{*}\NormalTok{(test.data}\SpecialCharTok{{-}}\NormalTok{mod.obj}\SpecialCharTok{$}\NormalTok{mean)}\SpecialCharTok{/}\NormalTok{mod.obj}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{  MAPE }\OtherTok{=} \FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(PE))}
  \DocumentationTok{\#\#\#}
\NormalTok{  E}\OtherTok{=}\NormalTok{test.data}\SpecialCharTok{{-}}\NormalTok{mod.obj}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{  MSE}\OtherTok{=}\FunctionTok{mean}\NormalTok{(E}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
  \DocumentationTok{\#\#\#}
\NormalTok{  accuracy.metric}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\AttributeTok{MSE=}\NormalTok{MSE, }\AttributeTok{MAPE=}\NormalTok{MAPE)}
\NormalTok{  accuracy.metric}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.accuracy }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(}\AttributeTok{SES =}\FunctionTok{acc.fun}\NormalTok{(}\AttributeTok{test.data=}\NormalTok{test.bev, }\AttributeTok{mod.obj=}\NormalTok{fit1),}
                      \AttributeTok{Holt.Add =}\FunctionTok{acc.fun}\NormalTok{(}\AttributeTok{test.data=}\NormalTok{test.bev, }\AttributeTok{mod.obj=}\NormalTok{fit2),}
                      \AttributeTok{Holt.Add.Damp =}\FunctionTok{acc.fun}\NormalTok{(}\AttributeTok{test.data=}\NormalTok{test.bev, }\AttributeTok{mod.obj=}\NormalTok{fit3),}
                      \AttributeTok{Holt.Exp =}\FunctionTok{acc.fun}\NormalTok{(}\AttributeTok{test.data=}\NormalTok{test.bev, }\AttributeTok{mod.obj=}\NormalTok{fit4),}
                      \AttributeTok{HW.Add =}\FunctionTok{acc.fun}\NormalTok{(}\AttributeTok{test.data=}\NormalTok{test.bev, }\AttributeTok{mod.obj=}\NormalTok{fit5),}
                      \AttributeTok{HW.Exp =}\FunctionTok{acc.fun}\NormalTok{(}\AttributeTok{test.data=}\NormalTok{test.bev, }\AttributeTok{mod.obj=}\NormalTok{fit6),}
                      \AttributeTok{HW.Add.Damp =}\FunctionTok{acc.fun}\NormalTok{(}\AttributeTok{test.data=}\NormalTok{test.bev, }\AttributeTok{mod.obj=}\NormalTok{fit7),}
                      \AttributeTok{HW.Exp.Damp =}\FunctionTok{acc.fun}\NormalTok{(}\AttributeTok{test.data=}\NormalTok{test.bev, }\AttributeTok{mod.obj=}\NormalTok{fit8))}
\FunctionTok{kable}\NormalTok{(pred.accuracy, }\AttributeTok{caption=}\StringTok{"The accuracy measures of various exponential smoothing models }
\StringTok{      based on the testing data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-252}The accuracy measures of various exponential smoothing models 
      based on the testing data}
\centering
\begin{tabular}[t]{l|r|r}
\hline
  & MSE & MAPE\\
\hline
SES & 268795.08 & 6.325197\\
\hline
Holt.Add & 221347.08 & 5.411444\\
\hline
Holt.Add.Damp & 268829.31 & 6.326006\\
\hline
Holt.Exp & 267988.88 & 6.306752\\
\hline
HW.Add & 49429.46 & 3.055574\\
\hline
HW.Exp & 87301.79 & 3.607590\\
\hline
HW.Add.Damp & 51295.11 & 3.091598\\
\hline
HW.Exp.Damp & 72440.01 & 3.422931\\
\hline
\end{tabular}
\end{table}

We can see from the above accuracy table that HW's linear trend with an additive seasonal model is the best of the eight smoothing models. This is consistent with the patterns in the original serial plot.

In the previous analysis, we train the model with the training data and identify the best model using both training and testing data. \textbf{In real-forecast, we need to refit the model at the very end using the entire data to update the smoothing parameters in the final working model}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beverage }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/sta321/main/ww13/w13{-}beverage01.txt"}\NormalTok{)}
\NormalTok{bev}\OtherTok{=}\FunctionTok{ts}\NormalTok{(beverage}\SpecialCharTok{$}\NormalTok{V1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{180}\NormalTok{], }\AttributeTok{start=}\DecValTok{2000}\NormalTok{, }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{)}
\NormalTok{final.model }\OtherTok{=} \FunctionTok{hw}\NormalTok{(bev,}\AttributeTok{h=}\DecValTok{12}\NormalTok{, }\AttributeTok{seasonal=}\StringTok{"additive"}\NormalTok{) }
\NormalTok{smoothing.parameter }\OtherTok{=}\NormalTok{ final.model}\SpecialCharTok{$}\NormalTok{model}\SpecialCharTok{$}\NormalTok{par[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]}
\FunctionTok{kable}\NormalTok{(smoothing.parameter, }\AttributeTok{caption=}\StringTok{"Estimated values of the smoothing parameters in}
\StringTok{      Holt{-}Winters linear trend with additive seasonality"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-253}Estimated values of the smoothing parameters in
      Holt-Winters linear trend with additive seasonality}
\centering
\begin{tabular}[t]{l|r}
\hline
  & x\\
\hline
alpha & 0.3443447\\
\hline
beta & 0.0001004\\
\hline
gamma & 0.0003443\\
\hline
\end{tabular}
\end{table}

In summary, the updated values of the three smoothing parameters in the Holt-Winters linear trend and with additive seasonality using the entire data are given in the above table.

\hfill\break

\hfill\break

  \bibliography{book.bib,packages.bib}

\end{document}
